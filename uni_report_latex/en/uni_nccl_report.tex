\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{float}

% Sequence diagrams
\usepackage{tikz}
\usepackage{pgf-umlsd}
\usepgflibrary{arrows.meta}

\lstset{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  breaklines=true,
  frame=single,
  framerule=0.3pt,
  rulecolor=\color{black!30},
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{black!60},
  stringstyle=\color{red!60!black}
}

% For ASCII/plaintext diagrams
\lstdefinestyle{ascii}{
  basicstyle=\ttfamily\footnotesize,
  columns=fixed,
  breaklines=true,
  frame=single,
  framerule=0.3pt
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.75\baselineskip}

\title{NCCL Profiler Plugin API -- A Feasibility Study}
\author{}
\date{}

\begin{document}

\maketitle
\tableofcontents

% ---------------------------------------------------------------------------
\section{TODO / Structure (from Markdown)}
% ---------------------------------------------------------------------------

\subsection{Table of Contents}
\begin{itemize}
  \item Abstract -- motivate GPU communication profiling/tracing
  \item Introduction -- assumed background as needed (e.g.\ MPI, NCCL, SLURM, Score-P)
  \item The Profiler API
  \begin{itemize}
    \item How NCCL detects the profiler plugin (first draft; TODO final draft)
    \item The Profiler API definition (with codeflow/swimlane diagrams showing where NCCL calls the profiler api)
    \item Code examples illustrating the codeflow/behaviour (simple example, multi-node, ncclGroup)
    \item What is possible with the Profiler Plugin API? Considerations and Pitfalls for (logging, running metrics, CUPTI, \ldots) - section title WIP
    \item Performance and scalability of the Profiler Plugin API (experiments, accuracy)  
  \end{itemize}
  \item Potential integration with Score-P
  \item Conclusion
\end{itemize}

\subsection{Main content chunks / concepts}
\begin{itemize}
  \item Simple code example walkthroughs
  \item Swim-lane diagrams (User API $\to$ init/finalize; start/stop/recordEventState)
  \item Benchmarking, measurements
  \item Conclusion
\end{itemize}

% ---------------------------------------------------------------------------
\section{Abstract}
% ---------------------------------------------------------------------------

Artificial intelligence (AI) has established itself as a primary use case in high-performance computing (HPC) environments due to its compute-intensive and resource-intensive workloads. 
Analyzing and optimizing application performance is therefore essential to maximize efficiency and reduce costs. 
Many AI workloads involve communication between GPUs, often distributed across numerous GPUs in multi-node systems. 
The NVIDIA Collective Communication Library (NCCL) serves as the core library for implementing optimized communication primitives on NVIDIA GPUs. 
To provide detailed performance insights, NCCL offers a flexible profiler plugin API. 
This allows developers to directly integrate custom profiling tools into the library to extract detailed performance data on communication operations. 
This feasibility study explores the capabilities and integration mechanisms of the API.

TODO finally, mention the structure of this document?

% ---------------------------------------------------------------------------
\section{About NCCL}
% ---------------------------------------------------------------------------
NCCL (pronounced "Nickel") was first introduced by NVIDIA in 2015 at the Supercomputing Conference (SC15), with an accompanying presentation highlighting its optimized collectives for multi-GPU systems. 
Although code was made available on GitHub, the release of NCCL 2.0 in 2017, which brought support for NVLink, was initially only available as pre-built binaries.
With the release of NCCL 2.3 in 2018, it returned to being fully open source.
The NCCL Profiler Plugin API was even later introduced with NCCL 2.23 in early 2025.

Before taking a closer look at the Profiler Plugin API, it is helpful to have some rudimentary understanding on certain designs in NCCL.

\subsection{Comparison to MPI}
Although NCCL is inspired by the Message Passing Interface (MPI) in terms of API design and usage patterns, there are notable differences due to their respective focuses:
\begin{itemize}
\item \textbf{MPI}: Communication is CPU-based. A rank corresponds to a single CPU process, and within a communicator, each process is assigned exactly one unique rank.
\item \textbf{NCCL}: Communication is GPU-based, with CPU threads handling orchestration. A rank corresponds to a GPU device, and a single CPU thread can manage multiple ranks (i.e., multiple devices) using functions such as \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}.
\end{itemize}

\subsection{Relevant NCCL internals}
It helps to understand what NCCL does internally when an application calls the NCCL User API.


A typical NCCL application follows this basic structure:

\begin{lstlisting}[language=C]
// create nccl communicators
createNcclComm();

// allocate memory for computation and communication
prepareDeviceForWork();

// do computation and communication
callNcclCollectives();
// ...

// finalize and clean up nccl communicators
cleanupNccl();
\end{lstlisting}

During NCCL communicator creation, NCCL internally spawns a thread called \texttt{ProxyService}. This thread lazily starts another thread called \texttt{ProxyProgress}, which handles network requests for GPU communication during collective and P2P operations.
See Figure~\ref{fig:thread-creation}.
\iffalse
\begin{lstlisting}[style=ascii]
Thread Creation Flow
-------------------------------------------------------------------------------
User API                          Internal Flow
-------------------------------------------------------------------------------
ncclCommInitRank()         -+
ncclCommInitAll()           |
ncclCommInitRankConfig()    +---> ncclCommInitRankDev()  --+
ncclCommInitRankScalable() -+                              |
ncclCommSplit()            -+                              |
ncclCommShrink()            +---> ncclCommInitChildComm() -+
ncclCommGrow()             -+     +------------------------+
                            |     v
                            +---> ncclCommInitRankFunc()
                                  |
                                  v
                                  initTransportsRank()
                                  |
                                  +-> ncclProxyCreate(comm)
                                      |
                                      v
                                      if (proxyState->refCount == 1)
                                          pthread_create(&comm->proxyState->thread, NULL, ncclProxyService, ...)
                                          |
                                          v
                                          [New Thread] ncclProxyService()
                                          |
                                          +-> proxyProgressAsync(...)
                                          |   |
                                          |   +-> proxyConnInit(...)
                                          |       |
                                          |       +-> proxyProgressInit(proxyState)
                                          |           |
                                          |           +-> ncclProxyProgressCreate(proxyState)
                                          |               |
                                          |               +-> if (!state->thread)
                                          |                       pthread_create(&state->thread, NULL, ncclProxyProgress, ...)
                                          |                       |
                                          |                       v
                                          |                       [New Thread] ncclProxyProgress()
                                          |
                                          +---> proxyServiceInitOp(...)
                                                |
                                                +---> proxyProgressAsync(...)  (same path as above)
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
The ASCII diagram shows how user API calls (e.g.\ \texttt{ncclCommInitRank}, \texttt{ncclCommSplit}) funnel into two internal entry points (\texttt{ncclCommInitRankDev} or \texttt{ncclCommInitChildComm}), then into \texttt{ncclCommInitRankFunc()}, \texttt{initTransportsRank()}, and \texttt{ncclProxyCreate(comm)}. From there, NCCL conditionally spawns the \texttt{ProxyService} thread; that thread later lazily starts the \texttt{ProxyProgress} thread via \texttt{proxyProgressAsync} $\to$ \texttt{ncclProxyProgressCreate} $\to$ \texttt{pthread\_create(ncclProxyProgress)}.

\paragraph{Representation as a sequence diagram.}
We model four participants: the \emph{User} (application), \emph{NCCL} (internal call chain), and the two spawned threads \emph{ProxyService} and \emph{ProxyProgress}. The User invokes NCCL; NCCL performs internal calls (self-calls) and then creates the ProxyService thread; ProxyService creates the ProxyProgress thread. This makes the creation order and the two-level thread spawn explicit.
\fi

\begin{figure}[htbp]
\centering
\begin{sequencediagram}
\newthread{user}{User}
\newinst[2]{nccl}{NCCL init.cc}
\newinst[2]{proxySvc}{ProxyService}
\newinst[2]{proxyProg}{ProxyProgress}
\begin{call}{user}{initRank / split / \ldots}{nccl}{}
  \begin{callself}{nccl}{ncclCommInitRankFunc}{}
    \begin{callself}{nccl}{initTransportsRank}{}
      
      \begin{callself}{nccl}{ncclProfilerPluginInit()}{}
      \end{callself}

      \begin{callself}{nccl}{ncclProxyCreate}{}
        
        \begin{sdblock}{\texttt{if (proxyState->refCount == 1)}}{}
          \begin{call}{nccl}{create}{proxySvc}{}
            
            % \begin{callself}{proxySvc}{proxyProgressAsync}{}
            % \begin{callself}{proxySvc}{ncclProxyProgressCreate}{}
            \begin{sdblock}{\texttt{if (!state->thread)}}{}
              \begin{call}{proxySvc}{create}{proxyProg}{}
              \end{call}
            \end{sdblock}
            % \end{callself}
            % \end{callself}
            
          \end{call}
        \end{sdblock}
      \end{callself}
    \end{callself}
  \end{callself}
\end{call}
\end{sequencediagram}
\caption{Thread creation: User API $\to$ NCCL internal init $\to$ create ProxyService $\to$ create ProxyProgress.}
\label{fig:thread-creation}
\end{figure}


The guards \texttt{if (proxyState->refCount == 1)} 
and \texttt{if (!state->thread)}
ensure that these threads are created once per shared resource (struct \texttt{ncclSharedResources}). 
The SharedResource has a ProxyState field. The fields in ProxyState are used to ensure only one instance of each thread exists:

\textbf{/src/include/comm.h}
\begin{lstlisting}[language=C]
struct ncclSharedResources {
  struct ncclComm* owner; /* communicator which creates this shared res. */
  struct ncclProxyState* proxyState;
  // other fields
}
\end{lstlisting}

\textbf{/src/include/proxy.h}
\begin{lstlisting}[language=C]
struct ncclProxyState {
  int refCount;
  pthread_t thread;
  ncclProxyProgressState progressState;
  // other fields
}

struct ncclProxyProgressState {
  struct ncclProxyOpsPool* opsPool;
  // other fields
}

struct ncclProxyOpsPool {
  struct ncclProxyOp ops[MAX_OPS_PER_PEER*NCCL_MAX_LOCAL_RANKS];
  // other fields
}
  
struct ncclProxyOps {
  // other fields
}
\end{lstlisting}

By default every NCCL communicator has its own shared resource. When the application calls \texttt{ncclCommSplit()} or \texttt{ncclCommShrink()} where the original communicator was initialized with a \texttt{ncclConfig\_t} with fields \texttt{splitShare} or \texttt{shrinkShare} set to \texttt{1}, the newly created communicator shares the shared resource (and the proxy threads) with the parent communicator.

\small
\begin{quote}
\texttt{/* proxyState is shared among parent comm and split comms}\\
\texttt{comm->proxyState->thread is pthread\_join()'d by commFree() in init.cc }\\
\texttt{when the refCount reduces down to 0. */}
\end{quote}
(Quoted from \textbf{/src/proxy.cc})
\normalsize


Later, whenever the application calls the NCCL User API, 
NCCL internally decides what network operations to perform and calls \texttt{ncclProxyPost()} to post them to a proxyOpsPool
(See Figure~\ref{fig:proxy-post}).

\iffalse
\begin{lstlisting}[style=ascii]
Flow from User API Calls to ncclProxyPost()
-------------------------------------------------------------------------------
User API
---------------------------------------------------------------------------------
ncclCommInitAll()          -+    ncclAllGather()      -+
ncclCommInitRankConfig()    |    ncclAlltoAll()        |
ncclCommInitRankScalable()  |    ncclAllReduce()       |
ncclCommFinalize()          |    ncclBroadcast()       |
ncclCommDestroy()           |    ncclGather()          |
ncclCommRevoke()            |    ncclReduce()          |
ncclCommAbort()             |    ncclReduceScatter()   |
ncclCommSplit()             |    ncclScatter()         |
ncclCommShrink()            |    ncclSend()            |
ncclCommGrow()              |    ncclRecv()           -+
ncclDevCommCreate()         |                          |
ncclCommWindowRegister()    |                          |
ncclGroupSimulateEnd()     -+                          |
-------------------------------------------------------------------------------
Internal Flow
-------------------------------------------------------------------------------
                            |                          |
                            |                          v
                            |                   ncclEnqueueCheck()
                            +--------------------------+
                            v
                            ncclGroupEndInternal()
                            +----+
                            |    v
                            |    groupLaunchNonBlocking()
                            +----+
                            v
                            groupLaunch()
                            |
                            v
                            doLaunches()
                            +-> ncclLaunchPrepare() ------------+
                            +-> ncclLaunchKernelAfter_NoCuda()  v
                                |                               ncclLaunchPrepare()
                                |                               |
                                |                               v
                                |                               cudaLaunchHostFunc()
                                |                               |
                                |                               v
                                |                               hostStreamPlanCallback()
                                +-------------------------------+
                                v
                                hostStreamPlanTask()
                                +-> uploadProxyOps() -+
                                +-> ncclProxyStart()  v
                                    |                 ncclProxySaveOp()
                                    |                 |
                                    |                 v
                                    |                 SaveProxy()
                                    |                 |
                                    |                 v
                                    |                 ncclLocalOpAppend()
                                    +-----------------+
                                    v
                                    ncclProxyPost() (proxy.cc)
                                    +-> [Posts Ops to pool]
                                    +-> [Signals Proxy Progress Thread]
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
Two families of user API calls lead to \texttt{ncclProxyPost()}: (1)~communicator lifecycle and group operations (\texttt{ncclCommInitAll}, \texttt{ncclCommSplit}, \texttt{ncclGroupSimulateEnd}, etc.) and (2)~collectives and P2P (\texttt{ncclAllReduce}, \texttt{ncclSend}, etc.). Both converge at \texttt{ncclGroupEndInternal()}, then \texttt{groupLaunch()} $\to$ \texttt{doLaunches()}. From there, kernel launch and host-callback paths lead to \texttt{hostStreamPlanTask()}, which calls \texttt{uploadProxyOps()} / \texttt{ncclProxyStart()} $\to$ \texttt{SaveProxy()} $\to$ \texttt{ncclProxyPost()}, where ops are posted to the pool and the ProxyProgress thread is signalled.

\paragraph{Representation as a sequence diagram.}
We show the \emph{User} invoking \emph{NCCL} with either a collective/P2P call or a comm/group call; NCCL's internal flow is collapsed into a chain of self-calls that merge at \texttt{ncclGroupEndInternal()} and end at \texttt{ncclProxyPost()}. The diagram emphasizes the single internal path after the merge and the role of \texttt{ncclProxyPost()} as the point where ops are posted and the proxy thread is woken.
\fi

\begin{figure}[htbp]
\centering
\begin{sequencediagram}
\newthread{user}{User}
\newinst[2]{nccl}{NCCL}
% \newinst[2]{cuda}{CUDA host stream}
\newinst[2]{proxy}{ProxyProgress}
\newinst[2]{opspool}{ncclProxyOpsPool}
\begin{call}{user}{User API (collective / comm / group)}{nccl}{}

  \begin{callself}{nccl}{ncclGroupEndInternal}{}
    \begin{callself}{nccl}{groupLaunch}{}
      \begin{callself}{nccl}{doLaunches}{}

        % \begin{callself}{nccl}{ncclLaunchPrepare}{}
        %   \begin{call}{nccl}{cudaLaunchHostFunc(hostStream, hostStreamPlanCallback)}{cuda}{}
        %   \end{call}
        % \end{callself}

        \begin{callself}{nccl}{hostStreamPlanTask}{}
          \begin{call}{nccl}{ncclProxyPost}{opspool}{}
          \end{call}
          \begin{call}{nccl}{signal}{proxy}{}
          \end{call}
        \end{callself}

      \end{callself}
    \end{callself}
  \end{callself}

\end{call}

% \begin{callself}{cuda}{hostStreamPlanCallback}{}
%   \begin{callself}{cuda}{hostStreamPlanTask}{}
%     \begin{call}{cuda}{ncclProxyPost}{opspool}{}
%     \end{call}
%     \begin{call}{cuda}{signal}{proxy}{}
%     \end{call}
%   \end{callself}
% \end{callself}

\end{sequencediagram}%
\caption{Flow from User API to \texttt{ncclProxyPost()}}
\label{fig:proxy-post}
\end{figure}

The ProxyProgress thread reads from this pool when calling \texttt{ncclProxyGetPostedOps()} and progresses the ops.
See Figure~\ref{fig:proxy-progress-loop}.

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
ncclProxyProgress() progressing loop
-------------------------------------------------------------------------------
ncclProxyProgress(proxyState)
|
+-> do {
        +-> progressOps(proxyState, ...)
        |   |
        |   +-> while (op) {
        |           op->progress(proxyState, op);
        |           op = op->next;
        |       }
        |
        +-> ncclProxyGetPostedOps()
            |
            +-> [reads Ops or thread will wait]
    } while (...)
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
The ProxyProgress thread runs a loop: in each iteration it first calls \texttt{progressOps()}, which walks the list of ops and invokes \texttt{op->progress(proxyState, op)} for each; then it calls \texttt{ncclProxyGetPostedOps()}, which either reads newly posted ops from the pool or blocks until the main path signals it. The loop thus alternates between making progress on existing ops and acquiring new work.

\paragraph{Representation as a sequence diagram.}
We use a single participant (the ProxyProgress thread) and a loop block. Inside the loop we show two self-calls: \texttt{progressOps()} (with an inner loop over ops) and \texttt{ncclProxyGetPostedOps()}. This makes the recurring cycle and the two phases (progress vs.\ get posted ops) explicit.
\fi

\begin{figure}[htbp]
\centering
\begin{sequencediagram}
\newthread{proxy}{ProxyProgress}
\newinst[5]{opspool}{ncclProxyOpsPool}
\begin{sdblock}{loop: do \ldots while}{}
  
  \begin{callself}{proxy}{progressOps(proxyState)}{}
  \end{callself}
  
  \begin{call}{proxy}{ncclProxyGetPostedOps [waits if no ops]}{opspool}{proxyState}
  \end{call}

\end{sdblock}
\end{sequencediagram}%
\caption{\textbf{/src/proxy.cc} \texttt{ncclProxyProgress()} progressing loop: progress ops, then get posted ops (or wait). }
\label{fig:proxy-progress-loop}
\end{figure}

In the next section, understanding the behaviour of NCCL for network-related activity is helpful in relation to the behavior of Profiler Plugin API.

% ---------------------------------------------------------------------------
\section{The Profiler API}
% ---------------------------------------------------------------------------

\subsection{How NCCL detects the profiler plugin}

When a NCCL communicator is created, NCCL looks for a shared library that represents the profiler plugin by checking an environment variable: \\
\texttt{profilerName = ncclGetEnv("NCCL\_PROFILER\_PLUGIN");} \\
It then calls \\
\texttt{handle* = dlopen(name, RTLD\_NOW | RTLD\_LOCAL);} \\
and \\
\texttt{ncclProfiler\_v5 = (ncclProfiler\_v5\_t*)dlsym(handle, "ncclProfiler\_v5");} \\
to load the library immediately with local symbol visibility. See Figure~\ref{fig:profiler-init}.

\begin{quote}
\begin{itemize}
  \item If \texttt{NCCL\_PROFILER\_PLUGIN} is set: attempt to load the library with the specified name; if that fails, attempt \texttt{libnccl-profiler-<NCCL\_PROFILER\_PLUGIN>.so}.
  \item If \texttt{NCCL\_PROFILER\_PLUGIN} is not set: attempt \texttt{libnccl-profiler.so}.
  \item If no plugin was found: profiling is disabled.
  \item If \texttt{NCCL\_PROFILER\_PLUGIN} is set to \texttt{STATIC\_PLUGIN}, the plugin symbols are searched in the program binary.
\end{itemize}
\end{quote}
\begin{flushleft}
\small
(Source: \url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html\#nccl-profiler-plugin})
\normalsize
\end{flushleft}


The profiler plugin is loaded when creating a communicator (before proxy thread creation). 
The plugin loading mechanism expects the struct variable name to follow the naming convention\\ 
\texttt{ncclProfiler\_v\{versionNum\}}, which also indicates the API version.

The profiler API has changed multiple times with newer NCCL releases; 
NCCL features a fallback mechanism to load older struct versions.
However from personal experience, backwards compatibility with plugins from API version 2 may be limited.
One instance is known, where a profiler plugin being developed against the NCCL release 2.25.1 with Profiler API version 2, was unable to run with the latest NCCL release.
NCCL developers may be required to actively maintain older API versions, to ensure they safely work when old behaviour is getting deprecated 
and do not unexpectedly get handed new features from new API versions, 
of which the Profiler Plugin developer is not aware of (when faithfully implementing old API versions).

The exact implementation is in \textbf{/src/plugin/plugin\_open.cc} and \textbf{/src/plugin/profiler.cc}.


\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
User API                          Internal Flow
-------------------------------------------------------------------------------
ncclCommInitRank()         --+
ncclCommInitAll()            |
ncclCommInitRankConfig()     +--> ncclCommInitRankDev()  --+
ncclCommInitRankScalable() --+                             |
                                                           |
ncclCommSplit()            --+                             |
ncclCommShrink()           --+--> ncclCommInitChildComm() -+
                                                           |
ncclCommGrow()             --+    +------------------------+
                             |    v
                             +--> ncclCommInitRankFunc()
                                  |
                                  v                        
                                  initTransportsRank()
                                  +-> ncclProfilerPluginInit(comm)-+
                                  +-> ncclProxyCreate(comm)        |
                                      |                            v
                                      v                            ncclProfilerPluginInit(comm)
                                      ...                          +-> ncclProfilerPluginLoad()
                                                                   |   +-> ncclGetEnv("NCCL_PROFILER_PLUGIN")
                                                                   |   +-> ncclOpenProfilerPluginLib()
                                                                   |   |   +-> dlopen(name, RTLD_NOW | RTLD_LOCAL)
                                                                   |   +-> getNcclProfiler_v5()
                                                                   |   |   +-> (ncclProfiler_v5_t*)dlsym(lib, "ncclProfiler_v5");
                                                                   |   +-> [try fallback to older api version]
                                                                   +-> profiler->init()

\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
User API calls for communicator creation (\texttt{ncclCommInitRank}, \texttt{ncclCommSplit}, etc.) enter NCCL via \texttt{ncclCommInitRankDev()} or \texttt{ncclCommInitChildComm()}, then \texttt{ncclCommInitRankFunc()} $\to$ \texttt{initTransportsRank()}. There, \texttt{ncclProfilerPluginInit(comm)} runs in parallel with \texttt{ncclProxyCreate(comm)}. The profiler path calls \texttt{ncclProfilerPluginLoad()} (get env, \texttt{dlopen}, \texttt{dlsym} for \texttt{ncclProfiler\_v5}), then \texttt{profiler->init()}.

\paragraph{Representation as a sequence diagram.}
We show three participants: \emph{User}, \emph{NCCL}, and \emph{Profiler plugin}. The User invokes NCCL; NCCL performs internal init and then loads and initializes the plugin (load library, get symbol, call \texttt{init}). The diagram makes it clear that the plugin is loaded and \texttt{init()} is called during communicator creation, before proxy threads are created.
\fi

\begin{figure}[H]
\centering
\begin{sequencediagram}
\newthread{user}{User}
\newinst[1.7]{nccl}{NCCL init.cc}
% \newinst[1]{profilercc}{NCCL profiler.cc}
\newinst[2.7]{proxy}{ProxyProgress}
\newinst[1]{plug}{Profiler Plugin}
\begin{call}{user}{initRank / split / \ldots}{nccl}{}
  \begin{callself}{nccl}{ncclCommInitRankFunc}{}
    \begin{callself}{nccl}{initTransportsRank}{}
      
      \begin{call}{nccl}{ncclProfilerPluginInit}{nccl}{}
     
        \begin{callself}{nccl}{ncclProfilerPluginLoad}{}
          \begin{callself}{nccl}{\texttt{ncclGetEnv}}{}
          \end{callself}
          \begin{callself}{nccl}{ncclOpenProfilerPluginLib}{}
            \begin{callself}{nccl}{\texttt{dlopen}}{}
            \end{callself}
          \end{callself}
          \begin{callself}{nccl}{getNcclProfiler\_v5}{}
            \begin{callself}{nccl}{\texttt{dlsym}}{}
            \end{callself}
          \end{callself}

        \end{callself}
        
        \begin{call}{nccl}{\texttt{ncclProfiler->init(commId, \ldots)}}{plug}{}
        \end{call}
     
      \end{call}
      
      \begin{call}{nccl}{ncclProxyCreate}{proxy}{}
      \end{call}
      
    \end{callself}
  \end{callself}
\end{call}
\end{sequencediagram}%
\caption{User API $\to$ NCCL init $\to$ load profiler plugin and call \texttt{profiler->init()}.}
\label{fig:profiler-init}
\end{figure}

\subsection{The profiler API definition}

The plugin must implement a profiler API specified by NCCL by exposing a struct. 
This struct should contain pointers to all functions required by the API. 
A plugin may expose multiple versioned structs for backwards compatibility with older NCCL versions.

\begin{lstlisting}[language=C]
ncclProfiler_v5_t ncclProfiler_v5 = {
  const char* name;
  ncclResult_t (*init)(...);              // NCCL calls this right after loading
  ncclResult_t (*startEvent)(...);        // at start of operations/activities
  ncclResult_t (*stopEvent)(...);         // at end of these operations/activities
  ncclResult_t (*recordEventState)(...);  // to record state of certain operations
  ncclResult_t (*finalize)(...);          // before unloading the plugin
};
\end{lstlisting}

The full profiler API is under \textbf{/src/include/plugin/profiler/profiler\_v\{versionNum\}.cc}. As of NCCL v2.29.1, version 6 is the latest; five functions must be implemented. 
Internally NCCL wraps calls to the profiler API in custom functions (found in \textbf{/src/include/profiler.h}).

NCCL invokes the profiler API at different levels to capture start/stop of NCCL groups, collectives, P2P, proxy, kernel and network activity. 
As the API function names suggest, this will allow the profiler to track these operations and activities as events.

The API functions and where NCCL invokes them are explained in the following sections.

\subsubsection{init}

\texttt{init} initializes the profiler plugin. NCCL passes follwing arguments:

\begin{lstlisting}[language=C]
ncclResult_t init(
  void** context,          // out param - opaque profiler context
  uint64_t commId,         // communicator id
  int* eActivationMask,    // out param - bitmask for which events are tracked
  const char* commName,    // user assigned communicator name
  int nNodes,              // number of nodes in communicator
  int nranks,              // number of ranks in communicator
  int rank,                // rank identifier in communicator
  ncclDebugLogger_t logfn  // logger function
);
\end{lstlisting}

Every time a communicator is created, \texttt{init()} is called immediately upon successful plugin load in \texttt{ncclProfilerPluginLoad()} 
(see Figure~\ref{fig:profiler-init}). If the profiler plugin \texttt{init} function does not return \texttt{ncclSuccess}, NCCL disables the plugin.


\begin{quote}
As soon as NCCL finds the plugin and the correct ncclProfiler symbol, it calls its init function. This allows the plugin to initialize its internal context used during profiling of NCCL events.
\end{quote}
(Source: \textbf{/ext-profiler/README.md})

\texttt{void** context} is an opaque handle that the plugin developer may point to any custom context object; this pointer is passed again in \texttt{startEvent} and \texttt{finalize}. This context object is separate per communicator.

The plugin developer should set \texttt{int* eActivationMask} to a bitmask indicating which event types the profiler wants to track. The mapping is in \textbf{/src/include/plugin/nccl\_profiler.h}; internally the mask defaults to 0 (no events). Setting it to 4095 will track all events.

\texttt{ncclDebugLogger\_t logfn} is a function pointer to NCCL's internal debug logger (\texttt{ncclDebugLog}). 
NCCL passes this so the plugin can emit log lines through the same channel and filtering as NCCL: 
the plugin may store the callback and call it with \texttt{(level, flags, file, line, fmt, ...)} when it wants to log. 
Messages then appear in NCCL's debug output (e.g.\ stderr or \texttt{NCCL\_DEBUG\_FILE}) and respect the user's \texttt{NCCL\_DEBUG} level and subsystem mask. 
Using \texttt{logfn} keeps profiler output consistent with NCCL's own logs.

\subsubsection{startEvent}

\texttt{startEvent} is called when NCCL begins certain operations:

\begin{lstlisting}[language=C]
ncclResult_t startEvent(
  void* context,                       // opaque profiler context object
  void** eHandle,                      // out param - event handle
  ncclProfilerEventDescr_v5_t* eDescr  // pointer to event descriptor
);
\end{lstlisting}

As of release v2.29.1 NCCL does not care about the return value. 
\texttt{void** eHandle} may point to a custom event object; this pointer is passed again in \texttt{stopEvent} and \texttt{recordEventState}. 
\texttt{eDescr} describes the started event. Its exact details can be found in \textbf{/src/include/plugin/profiler/}. 

The field \texttt{void* parentObj} in the event descriptor is the \texttt{eHandle} of a parent event (or null).
The use of this field can be explained as following:

All User API calls to Collective or P2P operations will start a Group API event.
When networking is required, ProxyCtrl Events may be emitted. 
Depending on the `eActivationMask` bitmask returned in the `init()` function, 
further (child) events will be emitted in deeper sections of the nccl code base. 
It can be thought of as an event hierarchy with several depth levels:


\begin{quote}
\begin{lstlisting}[style=ascii]
  Group API event
  |
  +- Collective API event
  |  |
  |  +- Collective event
  |     |
  |     +- ProxyOp event
  |     |  |
  |     |  +- ProxyStep event
  |     |     |
  |     |     +- NetPlugin event
  |     |
  |     +- KernelCh event
  |
  +- Point-to-point API event
  |  |
  |  +- Point-to-point event
  |     |
  |     +- ProxyOp event
  |     |  |
  |     |  +- ProxyStep event
  |     |     |
  |     |     +- NetPlugin event
  |     |
  |     +- KernelCh event
  |
  +- Kernel Launch event

ProxyCtrl event
\end{lstlisting}
\end{quote}
(Source: \textbf{/ext-profiler/README.md})

If the profiler enables tracking for event types lower in the hierarchy, NCCL also tracks their parent event types.
The \texttt{parentObj} inside \texttt{eDescr} will be a reference to the \texttt{eHandle} of the respective parent event for the current event according to this hierarchy. 

\subsubsection{stopEvent}

\begin{lstlisting}[language=C]
ncclResult_t stopEvent(void* eHandle);  // handle to event object
\end{lstlisting}
\texttt{stopEvent} tells the plugin that the event has stopped. \texttt{stopEvent} for collectives simply indicates to the profiler that the collective has been enqueued and not that the collective has been completed.

NCCL ignores the return value. \texttt{stopEvent} is called in the same functions that call \texttt{startEvent}, except for the GroupApi event (see diagram).

Figure~\ref{fig:profiler-events} shows when NCCL emits \texttt{startEvent} and \texttt{stopEvent} after a User API call.
The Proxy\-Progress thread also emits \texttt{startEvent} and \texttt{stopEvent} while progressing ops (see Figure~\ref{fig:proxy-event-emission}).

\iffalse
\begin{lstlisting}[style=ascii]
Flow from NCCL API Calls to profiler events
---------------------------------------------------------------------------------
User API
---------------------------------------------------------------------------------
ncclCommInitAll()          -+    ncclAllGather()      -+      
ncclCommInitRankConfig()    |    ncclAlltoAll()        |      
ncclCommInitRankScalable()  |    ncclAllReduce()       |      
ncclCommFinalize()          |    ncclBroadcast()       |      
ncclCommDestroy()           |    ncclGather()          |      
ncclCommRevoke()            |    ncclReduce()          |      
ncclCommAbort()             |    ncclReduceScatter()   |      
ncclCommSplit()             |    ncclScatter()         |      
ncclCommShrink()            |    ncclSend()            |      
ncclCommGrow()              |    ncclRecv()           -+      
ncclDevCommCreate()         |                          |      
ncclCommWindowRegister()    |                          |
ncclGroupSimulateEnd()     -+                          |
                            |                          | 
---------------------------------------------------------------------------------
Internal Flow
---------------------------------------------------------------------------------
                            |                          | 
                            |                          v      
                            |                          ncclEnqueueCheck()
                            |                          +-> taskAppend()
                            +--------------------------+   +-> collTaskAppend()
                            |                              |   +-> Emits: GroupApi Event (start), CollApi Event
                            |                              +-> p2pTaskAppend()
                            v                                  +-> Emits: GroupApi Event (start), P2pApi Event
                            ncclGroupEndInternal()
                            +-> Emits: GroupApi Event (stop, at end of function call if eHandle exists)
                            +-------+
                            |       v
                            |       groupLaunchNonBlocking()
                            +-------+
                            v
                            groupLaunch()
                            |
                            v
                            doLaunches()
                            +-> ncclLaunchPrepare() ------------+
                            +-> ncclLaunchKernel()              |
                            |   +-> Emits: KernelLaunch Event   |
                            +-> ncclLaunchKernelAfter_NoCuda()  v
                                |                               ncclLaunchPrepare()
                                |                               |
                                |                               v
                                |                               cudaLaunchHostFunc()
                                |                               |
                                |                               v
                                |                               hostStreamPlanCallback()
                                +-------------------------------+
                                v
                                hostStreamPlanTask()
                                +-> Emits: Group Event, Coll Event, P2p Event
                                +-> uploadProxyOps() -+
                                +-> ncclProxyStart()  v
                                    |                 ...
                                    v
                                    ...
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
The same two families of user API calls (comm/group vs.\ collectives/P2P) lead to profiler events. For collectives/P2P, \texttt{ncclEnqueueCheck()} $\to$ \texttt{taskAppend()} emits GroupApi (start) and CollApi or P2pApi (start). At \texttt{ncclGroupEndInternal()} NCCL emits GroupApi (stop). In \texttt{doLaunches()}, \texttt{ncclLaunchKernel()} emits KernelLaunch; \texttt{hostStreamPlanTask()} emits Group, Coll, P2p events; then \texttt{uploadProxyOps()} / \texttt{ncclProxyStart()} continue the path. The diagram shows where \texttt{startEvent} and \texttt{stopEvent} are invoked relative to the internal call flow.

\paragraph{Representation as a sequence diagram.}
We show \emph{User}, \emph{NCCL}, and \emph{Profiler plugin}. The User calls NCCL; NCCL performs internal steps and at specific points calls the plugin (\texttt{startEvent} / \texttt{stopEvent}). The sequence diagram makes the order of API calls and the injection points of profiler callbacks explicit.
\fi

\begin{figure}[H]
\centering
\resizebox{!}{20cm}{%
\begin{sequencediagram}
\newthread{user}{User}
\newinst[2]{nccl}{NCCL}
% \newinst[1.7]{cuda}{CUDA host stream}
\newinst[1]{proxy}{ProxyProgress}
\newinst[1]{opspool}{ncclProxyOpsPool}
\newinst[1.7]{plug}{Profiler plugin}

\begin{call}{user}{User collectives API}{nccl}{}
  
  \begin{callself}{nccl}{ncclEnqueueCheck}{}

    \begin{callself}{nccl}{taskAppend}{}
      \begin{call}{nccl}{startEvent (GroupApi)}{plug}{}
      \end{call}
      \begin{call}{nccl}{startEvent / stopEvent (CollApi / P2pApi)}{plug}{}
      \end{call}
    \end{callself}
    
    \begin{call}{nccl}{ncclGroupEndInternal}{nccl}{}
      
      \begin{callself}{nccl}{groupLaunch}{}
        \begin{callself}{nccl}{doLaunches}{}

          % \begin{callself}{nccl}{ncclLaunchPrepare}{}
          %   \begin{call}{nccl}{hostStreamPlanCallback}{nccl}{}
          %   \end{call}
          % \end{callself}

          \begin{callself}{nccl}{ncclLaunchKernel}{}
            \begin{call}{nccl}{startEvent / stopEvent (KernelLaunch)}{plug}{}
            \end{call}
          \end{callself}

          % \begin{callself}{nccl}{hostStreamPlanCallback}{}
          \begin{callself}{nccl}{hostStreamPlanTask}{}
            \begin{call}{nccl}{startEvent (Group)}{plug}{}
            \end{call}
            \begin{call}{nccl}{startEvent (Coll / P2p)}{plug}{}
            \end{call}
            
            \begin{call}{nccl}{ncclProxyPost}{opspool}{}
            \end{call}
            \begin{call}{nccl}{signal}{proxy}{}
            \end{call}
    
            \begin{call}{nccl}{StopEvent (Coll / P2p)}{plug}{}
            \end{call}
            \begin{call}{nccl}{stopEvent (Group)}{plug}{}
            \end{call}
          \end{callself}
          % \end{callself}
        
        \end{callself}
      \end{callself}

      \begin{call}{nccl}{stopEvent (GroupApi)}{plug}{}
      \end{call}    

    \end{call}
  
  \end{callself}
  
\end{call}
\end{sequencediagram}%
}
\caption{Flow from NCCL API calls to profiler events. In case of \texttt{ncclGroupStart / ncclGroupEnd}. multiple events of everything (except GroupApi) are called. internally, some Collectives (e.g. ncclAlltoAll) are implemented as many p2p ops, triggering many P2pApi and P2p events. Implementation: \textbf{/src/init.cc}, \textbf{/src/plugin/profiler.cc}.}
\label{fig:profiler-events}
\end{figure}


\iffalse
NCCL only uses the Cuda callback, if necessary for proxy args. 
Otherwise \texttt{hostStreamPlanTask} is called inside 
\texttt{doLaunches} wrapped inside \texttt{ncclLaunchKernelAfter\_NoCuda}:

\begin{quote}
\textbf{/src/enqueue.cc}
\begin{lstlisting}[language=C]
// We have to launch host tasks to push proxy args. We are careful to only
// do this if necessary since host tasks impose a high performance cost in CUDA.
if (plan->hasProxyOps) {
  // ...
  CUDACHECKGOTO(cudaLaunchHostFunc(hostStream, hostStreamPlanCallback, plan), result, failure);
}

// ...

ncclResult_t ncclLaunchKernelAfter_NoCuda(struct ncclComm* comm, struct ncclKernelPlan* plan) {
  if (!plan->isHostCbEnq) {
    // we are not using the host stream for proxy ops and reclaimation submission, call
    // hostStreamPlanTask directly
    NCCLCHECK(hostStreamPlanTask(comm, plan));
  }
  return ncclSuccess;
}
\end{lstlisting}
\end{quote}
\fi

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
ncclProxyProgress() Event Emission Flow
---------------------------------------------------------------------------------
ncclProxyProgress(proxyState) [Proxy Progress Thread Loop]
|
+-> do {
         |
         +-> progressOps(proxyState, opStart=state->active, ...)
         |   |
         |   +-> while (op) {
         |            op->progress(proxyState, op);
         |            |
         |            +-> [Transport-specific progress function]
         |                (net.cc, coll_net.cc, p2p.cc, shm.cc)
         |                | 
         |                +-> Emits: ProxyStep events
         |                +-> Emits: KernelCh events
         |                +-> Emits: Network plugin specific events
         |            op = op->next;
         |       }
         |
         +-> [Thread Idle/Active State Transitions]
         |   +-> Emits: ProxyCtrl events
         |
         +-> ncclProxyGetPostedOps(proxyState)
             +-> [Thread Sleep/Wakeup State Transitions]
             |     +-> [Thread sleeps, waits for signal]
             |     +-> Emits: ProxyCtrl events
             +-> [Update Op pool] 
             +-> Emits: ProxyCtrl events
             +-> ProxyAppend()
                 +-> ncclProxyOpToArgs()
                     +-> Emits: ProxyOp events
    } while (...)
\end{lstlisting}
\fi
\iffalse
\paragraph{What the diagram conveys.}
Inside the ProxyProgress thread loop, \texttt{progressOps()} invokes transport-specific progress (\texttt{op->progress}); those functions emit ProxyStep, KernelCh, and NetPlugin events. Thread idle/active and sleep/wakeup transitions emit ProxyCtrl events. When \texttt{ncclProxyGetPostedOps()} runs, it may emit ProxyCtrl events; \texttt{ProxyAppend()} / \texttt{ncclProxyOpToArgs()} emit ProxyOp events.

\paragraph{Representation as a sequence diagram.}
We use one participant (ProxyProgress thread) and a loop block. Inside the loop we show: \texttt{progressOps()} (with transport progress and event emission to the plugin), thread state transitions (ProxyCtrl), \texttt{ncclProxyGetPostedOps()} (ProxyCtrl), and \texttt{ProxyAppend()} (ProxyOp). The plugin is shown as a second participant that receives the emitted events.
\fi

\begin{figure}[H]
\centering
\resizebox{!}{13.5cm}{%
\begin{sequencediagram}
\newthread{proxy}{ProxyProgress}
\newinst[4]{opspool}{ncclProxyOpsPool}
\newinst[2]{plug}{Profiler plugin}
\begin{sdblock}{loop: do \ldots while}{}

  \begin{callself}{proxy}{progressOps}{}
    \begin{callself}{proxy}{\texttt{op->progress()} [transport specific progress]}{}
      \begin{call}{proxy}{startEvent / stopEvent (ProxyStep, KernelCh, NetPlugin)}{plug}{}
      \end{call}
    \end{callself}
  \end{callself}
  \begin{call}{proxy}{startEvent / stopEvent (ProxyCtrl)}{plug}{}
  \end{call}
  \begin{callself}{proxy}{ncclProxyGetPostedOps}{}
    \begin{call}{proxy}{startEvent / StopEvent (ProxyCtrl)}{plug}{}
    \end{call}

    \begin{call}{proxy}{get next ops / wait if no ops}{opspool}{}
    \end{call}

    \begin{callself}{proxy}{ProxyAppend}{}
      \begin{callself}{proxy}{ncclProxyOpToArgs}{}
        \begin{call}{proxy}{startEvent / stopEvent (ProxyOp)}{plug}{}
        \end{call}
      \end{callself}
    \end{callself}
  \end{callself}

\end{sdblock}
\end{sequencediagram}%
}
\caption{\texttt{ncclProxyProgress}: progressOps emits ProxyStep/KernelCh/NetPlugin events. getPostedOps emits ProxyOp events. Several events ProxyCtrl are also emitted}
\label{fig:proxy-event-emission}
\end{figure}

\texttt{op->progress()} progresses transport specific ops. 
this is implemented as a function pointer type (defined in \textbf{/src/include/proxy.h}). 
Confusingly the variable is called `op`, although its type is \texttt{ncclProxyArgs} and NOT \texttt{ncclProxyOp}.

\begin{lstlisting}[language=C]
typedef ncclResult_t (*proxyProgressFunc_t)(struct ncclProxyState*, struct ncclProxyArgs*);

struct ncclProxyArgs {
  proxyProgressFunc_t progress;
  struct ncclProxyArgs* next;
  /* other fields */
}
\end{lstlisting}

Which specific function this calls depends on the Op. 
This also decides which profiler events (ProxyStep, KernelCh or Network plugin specific) are started and stopped. 
The transport-specific progress functions are in \textbf{/src/transport/net.cc}, \textbf{coll\_net.cc}, \textbf{p2p.cc}, \textbf{shm.cc}.

\subsubsection{recordEventState}

\begin{lstlisting}[language=C]
  ncclResult_t recordEventState(
    void* eHandle,
    ncclProfilerEventState_v5_t eState,
    ncclProfilerEventStateArgs_v5_t* eStateArgs
  );
\end{lstlisting}
Some event types can be updated by NCCL through \texttt{recordEventState} (state and attributes). 
Supported states can be found under \textbf{/src/include/plugin/profiler/profiler\_v\{versionNum\}.h}.

Called at the same sites as \texttt{startEvent}.

\subsubsection{finalize}

\begin{lstlisting}[language=C]
  ncclResult_t finalize(void* context);
\end{lstlisting}
After a user API call to free resources associated with a communicator, \texttt{finalize()} is called.
Afterwards, a reference counter tracks how many communicators are still being tracked by the profiler plugin. 
If it reaches 0, the plugin will be closed via \texttt{dlclose(handle)}. Figure~\ref{fig:profiler-finalize} depicts the flow from user API call to \texttt{finalize()}.
See implementation at \textbf{/src/init.cc}, \textbf{/src/plugin/profiler.cc}, \textbf{/src/plugin/plugin\_open.cc}.

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
User API                          Internal Flow
-------------------------------------------------------------------------------
ncclCommAbort()    --+
ncclCommDestroy()  --+----------> commReclaim()
                                  |
                                  +-> ncclProfilerPluginFinalize()
                                      |
                                      +-> ncclProfiler->finalize()
                                      +-> ncclProfilerPluginUnload()
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
User API calls \texttt{ncclCommAbort()} or \texttt{ncclCommDestroy()} trigger \texttt{commReclaim()}, which calls \texttt{ncclProfilerPluginFinalize()} $\to$ \texttt{profiler->finalize()}, then \texttt{ncclProfilerPluginUnload()} (e.g.\ \texttt{dlclose}).

\paragraph{Representation as a sequence diagram.}
We show three participants: \emph{User}, \emph{NCCL}, and \emph{Profiler plugin}. The User calls NCCL to destroy/abort the communicator; NCCL runs \texttt{commReclaim()} and then calls the plugin's \texttt{finalize()} and unloads the plugin. The sequence diagram makes the teardown order explicit.
\fi

\begin{figure}[H]
\centering
\resizebox{!}{10cm}{%
\begin{sequencediagram}
\newthread{user}{User}
\newinst[4]{nccl}{NCCL}
\newinst[5]{plug}{Profiler plugin}
\begin{call}{user}{ncclCommAbort / ncclCommDestroy}{nccl}{}
  \begin{callself}{nccl}{commReclaim}{}
  \end{callself}
  \begin{callself}{nccl}{ncclProfilerPluginFinalize}{}
    \begin{call}{nccl}{finalize(context)}{plug}{}
    \end{call}
  \end{callself}
  \begin{callself}{nccl}{ncclProfilerPluginUnload}{}

    \begin{sdblock}{\texttt{if 0 == (---profilerPluginRefCount)}}{}
      \begin{callself}{nccl}{\texttt{dlclose}}{}
      \end{callself}
    \end{sdblock}
    
  \end{callself}
\end{call}
\end{sequencediagram}%
}
\caption{User API $\to$ \texttt{commReclaim()} $\to$ \texttt{finalize()} $\to$ plugin unload.}
\label{fig:profiler-finalize}
\end{figure}

\subsubsection{name}

The profiler plugin struct also has a \texttt{name} field.
The name field should point to a character string with the name of the profiler plugin. 
It will be used for all logging, especially when \texttt{NCCL\_DEBUG=INFO} is set.

\subsection{Code Examples}

The following examples illustrate the profiling behavior under different environment settings and user application.

\begin{itemize}
  \item single process multiple devices
  \item mpi multiple processes with multiple device per process (multiple threads per process)
  \item mpi multiple processes with multiple device per process (single thread per process, utilizing ncclGroup)
  \item behavior when utilizing ncclGroup for collective operations
\end{itemize}

TODO add exampless and pictures (profiler that just does logging + application examples)

An example implementation of a profiler plugin may just log all call information to a file.
A simplified view of the plugin code used throughout the examples is as follows:

\begin{lstlisting}[language=C]
struct MyContext { /* custom context struct */ };
struct MyEvent { /* custom event struct */ };

MyEvent* allocEvent(args) { /* handles event allocation */ }
uint64_t getTime() { /* gets time */ }
void writeJsonl() { /* writes call details to log file as structured jsonl */ }

ncclResult_t myInit( /* args - *context, *eActivationMask, ... */ ) {
  *context = malloc(sizeof(struct MyContext));
  *eActivationMask = 4095; /* enable ALL event types */
  
  writeJsonl(getTime(), args);
  return ncclSuccess;
}

ncclResult_t myStartEvent( /* args - *eHandle, ... */ ) {
  *event = allocEvent(args);
  *eHandle = event;

  writeJsonl(getTime(), args);
  return ncclSuccess;
}

ncclResult_t myStopEvent(void* eHandle) {
  writeJsonl(getTime(), args);
  return ncclSuccess;
}

ncclResult_t myRecordEventState( /* args - ... */ ) {
  writeJsonl(getTime(), args);
  return ncclSuccess;
}

ncclResult_t myFinalize(void* context) {
  writeJsonl(getTime(), args);

  free(context);
  return ncclSuccess;
}

ncclProfiler_v5_t ncclProfiler_v5 = {
  "MyProfilerPlugin",
  myInit,
  myStartEvent,
  myStopEvent,
  myRecordEventState,
  myFinalize,
};
\end{lstlisting}

\subsubsection{Single Process Multiple Devices}

In this example, there is a single task running on one node with multiple GPUs.
The example application for this can be found under \textbf{/examples/03\_collectives/01\_allreduce/}. In essence, it does the following:
\begin{itemize}
  \item Detect Available GPUs
  \item Allocate Memory for Communicators, Streams, and Data Buffers
  \item Initialize NCCL Communicators for All Devices
  \item Create CUDA Streams and Allocate Device Memory
  \item Perform AllReduce Sum Operation
  \item Cleanup Resources and Report Results
\end{itemize}

The logs written by the profiler plugin can be visualized, creating following output:

TODO run this example (or a modified version with one additional warmup allreduce) on HPC
2761641

TODO organize and save outputs locally 

TODO visualize output

TODO add screenshot here
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/03_communicators_01_allreduce/finalize.png"}
    \caption{Single-GPU single-process-multiple-devices with ncclGroup trace: This visualization shows a view on an AllReduce collective, where all ranks are managed by a single thread on the same process.}
  \label{fig:single-gpu-single-process-ncclgroup-finalize}
\end{figure}

TODO caption
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/03_communicators_01_allreduce/zoomed in.png"}
    \caption{Single-GPU single-process-multiple-devices with ncclGroup trace: Zoomed in TODO}
  \label{fig:single-gpu-single-process-ncclgroup-zoom}
\end{figure}

TODO caption
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/03_communicators_01_allreduce/zoomed in 2 combined.png"}
    \caption{Single-GPU single-process-multiple-devices with ncclGroup trace: Zoomed in TODO}
  \label{fig:single-gpu-single-process-ncclgroup-zoom2}
\end{figure}


TODO explain what we see

\subsubsection{Multiple Processes, Multiple Devices per Process, Multiple Threads per Process}

TODO explain "@multi gpu single thread" code

TODO run this code (or a modified version with one additional warmup allreduce) on HPC

TODO organize and save outputs locally 

TODO visualize output


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/multi_gpu_single_thread/start to end - allreduce - 4 different threads + 4 proxy threads.png"}
    \caption{Multi-GPU single-thread-per-device trace: This visualization shows the AllReduce collective being called on 4 different ranks. a single thread corresponds to each rank. First a small bar (row above the green bar), corresponds with the timestamp when the profiler \texttt{init()} function was called for each rank. since init is called during NCCL's internal communicator creation, this corresponds to roughly when the application called \texttt{ncclCommInitRank}. Afterwards the application called an \texttt{ncclAllReduce}. This is visible in the profiler output as when the green bar (\texttt{groupApi} event) starts. Just below is another tiny bar, which represents the start and stop of the \texttt{collApi} event. The yellow bar represent the timing where NCCL enqueued the kernel for launch on the GPU (\texttt{KernelLaunch} event). Two small bars below represnt the \texttt{group} and \texttt{coll} events. Besides these 4 threads, NCCL spawned a proxy progress thread for each rank as well. The red \texttt{ProxyCtrl} event in the row below until that point was indicating that the proxy progress thread was asleep. the new ProxyCtrl event right after is the time it took for the Proxy Progress thread to append proxy ops. Next, multiple proxy ops are starting to be progressed (\texttt{ProxyOps} events), which in \texttt{op->progress()} leads to starting \texttt{KernelCh} network activity. At some point the KernelCh acitvity is completed, the AllReduce collective is finished. The ProxyCtrl event thereafter indicates the proxy progress thread went back to sleep. It is possible to visualize which \texttt{coll} events across ranks belong to the same collective operation. This is indicated by the red line connecting the events, enabled by the equal valued \texttt{seqNum} field (provided in \texttt{eDescr} arg in profiler API \texttt{init()} function call) across ranks.}
  \label{fig:multi-gpu-single-thread-allreduce}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/multi_gpu_single_thread/show proxystep.png"}
  \caption{Multi-GPU single-thread-per-device trace: additionally showing proxy step events.}
  \label{fig:multi-gpu-single-thread-show-proxystep}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/multi_gpu_single_thread/no_arrows - no_same_lane - show proxystep combined.png"}
\caption{Multi-GPU single-thread-per-device trace: proxy step events, but always on new lane chronologically, no arrows.}
  \label{fig:multi-gpu-single-thread-proxystep2}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{"images/multi_gpu_single_thread/finalize.png"}
\caption{Multi-GPU single-thread-per-device trace: from \texttt{init} call until \texttt{finalize} call (a small bar to the very right in the slightly darker top row).}
\label{fig:multi-gpu-single-thread-finalize}
\end{figure}

TODO explain what we see

\subsubsection{Multiple Processes, Multiple Devices per Process, Single Threads per Process \& using ncclGroup}

skip this 

TODO probably just need one of the multi process multi gpu per process examples.
which one is better depends on whether 03\_collectives/01\_allreduce actually is single thread or multi threaded
commInitAll is single threaded using ncclGroup

\subsubsection{behavior when utilizing ncclGroup for collective operations}

TODO explain node environment setup

TODO add selective screenshots of interesting things,

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclGroup/grouped broadcast allreduce across different comms (all processes).png"}
  \caption{Multi-GPU multiple communicators per process with grouped collectives: here 6 different communicator cliques across 4 processes are shown. their collective operations are grouped together with \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}. Only a single \texttt{GroupApi} event happens per process.}
  \label{fig:multi-gpu-multi-comm-per-process-ncclgroup}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclGroup/grouped broadcast allreduce across different comms same process.png"}
  \caption{Multi-GPU multiple communicators per process with grouped collectives: View of just a single process. The \texttt{CollApi} events at the start show that they have the same GroupApi event as \texttt{parentObj}. However each rank still emits their own \texttt{KernelLaunch} event.}
  \label{fig:multi-gpu-multi-comm-per-process-ncclgroup-1-process}
\end{figure}

like some ncclGroup stuff,
multi level split
intereweaved stuff

TODO explain what we see

TODO go back up and explain the important "@stress test" code. and show snippets and/or summarize (like for 1st simple example)

TODO if not run on HPC in useful format yet, run this code on HPC

TODO organize and save outputs locally 

\subsection{What is possible with the Profiler Plugin API? Considerations and Pitfalls for (logging, running metrics, CUPTI, \ldots) when writing the plugin - section title WIP }

TODO
Logging. is this even worth talking about?
\begin{itemize}
  \item Logging function from \texttt{init} (TODO)
  \item Code snippet: custom logging infrastructure, timestamping
\end{itemize}

TODO
Tracking \& running metrics
\begin{quote}
  Due to the asynchronous nature of NCCL operations, events associated with collectives and point-to-point are not easy to delimit precisely. \texttt{stopEvent} for collectives simply indicates to the profiler that the collective has been enqueued. Without proxy and/or kernel activity the plugin cannot determine when a collective ends. With proxy/kernel events enabled, the plugin can estimate when it ends.
\end{quote}
(slightly rephrased from \textbf{/ext-profiler/README.md})
  

void* parentObj

if the plugin developer wants utilize this field, they should ensure that potential address reuse does not create ambiguity to what the parentObj was originally pointing to. Custom memory management is advised.
This field is useful when trying to understand which user API call triggered which events of lower level operations or activity such as network activity.

TODO add picture


seqNumber

When profiling is enabled, NCCL counts the number of calls for each type of collective function per communicator.

/src/include/comm.h
\begin{lstlisting}[language=C]
struct ncclComm {
  uint64_t seqNumber[NCCL_NUM_FUNCTIONS];
  /* other fields */
}
\end{lstlisting}

/src/plugin/profiler.cc
\begin{lstlisting}[language=C]
ncclResult_t ncclProfilerStartTaskEvents(struct ncclKernelPlan* plan) {
  /* other code */
  __atomic_fetch_add(&plan->comm->seqNumber[ct->func], 1, __ATOMIC_RELAXED);
  /* other code */
}
\end{lstlisting}

This value is present in the eDescr for collective events and can be used to identify which collectives operations belong together across processes.

TODO add picture of collectives seqNum


PXN

Unless Setting the environment variable \texttt{NCCL\_PXN\_DISABLE}=0 (default 1), due to PXN (PCIe x NVLink) some proxy ops may be progressed in a proxy thread from another process, different to the one that originally generated the operation 
Then \texttt{parentObj} in \texttt{eDescr} is not safe to dereference; the \texttt{eDescr} for \texttt{ProxyOp} events includes the originator's PID, which the profiler can match against the local PID. 
The \texttt{eDescr} for \texttt{ProxyStep} does not provide this field. However a workaround is possible:

The passed \texttt{context} object in \texttt{startEvent()} is also unsafe to dereference due to PXN. the profiler plugin developer may internally track initialized contexts and whether the passed \texttt{context} belongs to the local process. This is also indicative of PXN.

TODO: fact check if context is actually unsafe to deref? from code it looks like it is (context is read from pool - which iirc is posted to by potentially the originating process, not the progressing proxy thread process?)
looking at inspector plugin implementation, it casts the passed context in startEvent(), but doesnt dereference it. so it doesnt crash.
looking at example plugin implementation, it dereferences it in startEvent(), which is why it crashes.

TODO verify this behavior one more time: slurm 2732796

TODO
\begin{itemize}
  \item Code snippet: example CRUD of custom context object
  \item Code snippet: example CRUD of custom event object
\end{itemize}

TODO
Kernel tracing with CUPTI
\begin{itemize}
  \item CUPTI extension ID mechanism briefly explained
  \item Code snippet: where to CUPTI init/cleanup and usage
\end{itemize}

TODO Changing profiling behaviour at runtime (TODO: check example\_profiler, inspector)

TODO commId hierarchical behavior.
not so important.
for split and shrink with ncclCommInitRankConfig

\subsection{Performance and scalability of the Profiler Plugin API}
 
Following Experiments were run to assess the performance and scalability of profiler plugins.

TODO
\begin{itemize}
  \item synthetic performance measuring applications
  \item real application
\end{itemize}


TODO check out potential use of

\begin{lstlisting}[language=C]
@profiler (line 120 - 182)
#define ENABLE_TIMER 0
#include "timer.h"

#if ENABLE_TIMER
// These counters are used to measure profiler overheads for different part of the code
// These counters are only useful/meaningful in controlled test environments where there
// is only one thread updating each set of counters, i.e., every communicator has its
// own proxy thread and the network uses only one thread to make progress (this is true
// for net_ib plugin but might not be true for net_socket plugin).
...
\end{lstlisting}

setup

empty profiler: initializes dummy context struct, returns NULL for event handles, tracks all events 4095 (including those for network activity)
\begin{lstlisting}[language=C]
// an 'empty' NCCL Profiler Plugin

struct MyContext {
  char dummy;
};

ncclResult_t myInit(void** context, uint64_t commId, int* eActivationMask, const char* commName, int nNodes, int nranks, int rank, ncclDebugLogger_t logfn) {
  *context = malloc(sizeof(struct MyContext));
  *eActivationMask = 4095; /* enable ALL event types */
  return ncclSuccess;
}

ncclResult_t myStartEvent(void* context, void** eHandle, ncclProfilerEventDescr_v5_t* eDescr) {
  *eHandle = NULL;
  return ncclSuccess;
}

ncclResult_t myStopEvent(void* eHandle) {
  return ncclSuccess;
}

ncclResult_t myRecordEventState(void* eHandle, ncclProfilerEventState_v5_t eState, ncclProfilerEventStateArgs_v5_t* eStateArgs) {
  return ncclSuccess;
}

ncclResult_t myFinalize(void* context) {
  free(context);
  return ncclSuccess;
}

ncclProfiler_v5_t ncclProfiler_v5 = {
  "EmptyProfiler",
  myInit,
  myStartEvent,
  myStopEvent,
  myRecordEventState,
  myFinalize,
};
\end{lstlisting}


application:

TODO run mainly nccl-tests for synthetic. easily accessible so easier to reproduce

synthetic - TODO add application code in an appendix section or something, if there are some custom apps that are worthy but missing in nccl-tests

realistic - TODO maxtext zum laufen bringen

singlenode
\begin{itemize}
  \item coll singlenode (1mil iters, 100 warmup iters):
  \begin{itemize}
    \item sbatch run 1: 7.64 s overhead. averaged 26.40 s per iter w/o. 34.04 s w/ profiler 
  \end{itemize}

  \item p2p singlenode (1mil iters 100 warmup iters):
  \begin{itemize}
    \item sbatch run 1: 7.26 s overhead. averaged 26.66 s per iter w/o. 33.92 s w/ profiler 
  \end{itemize}

  \item commops singlenode (100 iters, 10 warmup iters):
  \begin{itemize}
    \item sbatch run 1: check slurm ...
  \end{itemize}
\end{itemize}

multinode
\begin{itemize}
  \item coll multinode (1mil iters, 100 warmup iters):
  \begin{itemize}
    \item sbatch run 1: 0.33 s overhead. averaged 22.12 s per iter w/o. 22.45 s w/ profiler
  \end{itemize}

  \item p2p multinode (1mil iters 100 warmup iters):
  \begin{itemize}
    \item sbatch run 1: not measurable. averaged 21.17 s per iter w/o. 20.82 s w/ profiler 
  \end{itemize}

  \item commops multinode (100 iters, 10 warmup iters):
  \begin{itemize}
    \item sbatch run 1: "init, destroy" experiment
    \begin{itemize}
      \item 1790.05 s overhead. averaged 577455.37 s per iter w/o. 579245.42 s w/ profiler
    \end{itemize}
    \item sbatch run 1: "init, split, destroy" experiment
    \begin{itemize}
      \item 1546.16 s overhead. averaged 1079679.93 s per iter w/o. 1078133.77 s w/ profiler
    \end{itemize}
  \end{itemize}
\end{itemize}


TODO run experiments multiple times?


TODO
accuracy and reliability/consistency of the the timings of profiler api calls from nccl

Using the profiler plugin when scaled to many gpus across multiple nodes is effortless and did not require any changes for the ran examples and experiments.

% ---------------------------------------------------------------------------
\section{Potential Integration with Score-P}
% ---------------------------------------------------------------------------

TODO
Substrate Plugin Route

TODO
Metric Plugin Route?

% ---------------------------------------------------------------------------
\section{Conclusion - What i have shown. Why would you use it? pros \& cons}
% ---------------------------------------------------------------------------

\begin{itemize}
  \item Customizable
  \item May require maintenance / active development since NCCL is actively developed
  \item Low overhead: NVIDIA advertises their \texttt{inspector} implementation as efficient enough for ``always-on'' in production
\end{itemize}

\subsection{NCCL\_DEBUG}

\url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-debug}

NCCL already comes with debug logging at various levels of granularity:
\begin{itemize}
  \item INFO -- debug information
  \item TRACE -- replayable trace information on every call
  \item Further options (v2.2.12 \texttt{NCCL\_DEBUG\_FILE}, v2.3.4 \texttt{NCCL\_DEBUG\_SUBSYS}, v2.26 timestamp format/levels)
  \item other profiling and tracing tools exists that are maintained by NVIDIA: nsight systems, nsight compute
\end{itemize}

\subsection{Known limitations}

\begin{quote}
Kernel event instrumentation uses counters exposed by the kernel to the host and the proxy progress thread. Thus the proxy progress thread infrastructure is shared between network and profiler. If the proxy is serving network requests, reading kernel profiling data can be delayed, causing loss of accuracy. Similarly, under heavy CPU load and delayed scheduling of the proxy progress thread, accuracy can be lost.

From profiler version 4, NCCL uses a per-channel ring buffer of 64 elements. Each counter is complemented by two timestamps (ptimers) supplied by the NCCL kernel (start and stop of the operation in the kernel). NCCL propagates these timestamps to the profiler plugin so it can convert them to the CPU time domain.
\end{quote}
Source: \url{https://github.com/NVIDIA/nccl/tree/master/ext-profiler/README.md}

\subsection{Summarize What i have shown TODO}
TODO

% ---------------------------------------------------------------------------
\section{TODO}
% ---------------------------------------------------------------------------

\begin{itemize}
  \item custom profiler code cleaning
\end{itemize}

\end{document}
