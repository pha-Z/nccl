\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{float}

% Sequence diagrams
\usepackage{tikz}
\usepackage{pgf-umlsd}
\usepgflibrary{arrows.meta}

\lstset{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  breaklines=true,
  frame=single,
  framerule=0.3pt,
  rulecolor=\color{black!30},
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{black!60},
  stringstyle=\color{red!60!black}
}

% For ASCII/plaintext diagrams
\lstdefinestyle{ascii}{
  basicstyle=\ttfamily\footnotesize,
  columns=fixed,
  breaklines=true,
  frame=single,
  framerule=0.3pt
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.75\baselineskip}

\title{NCCL Profiler Plugin API -- A Feasibility Study}
\author{}
\date{}

\begin{document}

\maketitle
\tableofcontents

% ---------------------------------------------------------------------------
\section{Abstract}
% ---------------------------------------------------------------------------

Artificial intelligence (AI) has established itself as a primary use case in high-performance computing (HPC) environments due to its compute-intensive and resource-intensive workloads. 
Analyzing and optimizing application performance is therefore essential to maximize efficiency and reduce costs. 
Many AI workloads involve communication between GPUs, often distributed across numerous GPUs in multi-node systems. 
The NVIDIA Collective Communication Library (NCCL) serves as the core library for implementing optimized communication primitives on NVIDIA GPUs. 
To provide detailed performance insights, NCCL offers a flexible profiler plugin API. 
This allows developers to directly integrate custom profiling tools into the library to extract detailed performance data on communication operations. 
This feasibility study explores the capabilities and integration mechanisms of the API.

First, this study provides background information on NCCL, 
followed by an explanation of the Profiler API is explained accompanied with code examples and visualizations.
Next, considerations for developers of the Profiler API and its potential integration with Score-P is discussed. 
Finally, the study concludes with a summary of the findings.

% ---------------------------------------------------------------------------
\section{About NCCL}
% ---------------------------------------------------------------------------
NCCL (pronounced "Nickel") was first introduced by NVIDIA in 2015 at the Supercomputing Conference (SC15), with an accompanying presentation highlighting its optimized collectives for multi-GPU systems. 
Although code was made available on GitHub, the release of NCCL 2.0 in 2017, which brought support for NVLink, was initially only available as pre-built binaries.
With the release of NCCL 2.3 in 2018, it returned to being fully open source.
The NCCL Profiler Plugin API was even later introduced with NCCL 2.23 in early 2025.

Before taking a closer look at the Profiler Plugin API, it is helpful to have some rudimentary understanding on certain designs in NCCL.

\subsection{Comparison to MPI}
Although NCCL is inspired by the Message Passing Interface (MPI) in terms of API design and usage patterns, there are notable differences due to their respective focuses:
\begin{itemize}
\item \textbf{MPI}: Communication is CPU-based. A rank corresponds to a single CPU process within a communicator.
\item \textbf{NCCL}: Communication is GPU-based, with CPU threads handling orchestration. A rank corresponds to a GPU device within a communicator; the mapping from ranks to devices is surjective. A single CPU thread can manage multiple ranks (i.e., multiple devices) in a communicator using the functions \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}. A CPU thread can also manage multiple ranks from different communicators (i.e same device alloted by multiple ranks from different communicators) through communicator creation with \texttt{ncclCommSplit} or \texttt{ncclCommShrink}. This means the mapping from ranks to threads is also surjective.
\end{itemize}

\subsection{Relevant NCCL internals}
It helps to understand what NCCL does internally when an application calls the NCCL User API.


A typical NCCL application follows this basic structure:

\begin{lstlisting}[language=C]
// create nccl communicators
createNcclComm();

// allocate memory for computation and communication
prepareDeviceForWork();

// do computation and communication
callNcclCollectives();
// ...

// finalize and clean up nccl communicators
cleanupNccl();
\end{lstlisting}

During NCCL communicator creation, NCCL internally spawns a thread called \texttt{ProxyService}. This thread lazily starts another thread called \texttt{ProxyProgress}, which handles network requests for GPU communication during collective and P2P operations.
See Figure~\ref{fig:thread-creation}.
\iffalse
\begin{lstlisting}[style=ascii]
Thread Creation Flow
-------------------------------------------------------------------------------
User API                          Internal Flow
-------------------------------------------------------------------------------
ncclCommInitRank()         -+
ncclCommInitAll()           |
ncclCommInitRankConfig()    +---> ncclCommInitRankDev()  --+
ncclCommInitRankScalable() -+                              |
ncclCommSplit()            -+                              |
ncclCommShrink()            +---> ncclCommInitChildComm() -+
ncclCommGrow()             -+     +------------------------+
                            |     v
                            +---> ncclCommInitRankFunc()
                                  |
                                  v
                                  initTransportsRank()
                                  |
                                  +-> ncclProxyCreate(comm)
                                      |
                                      v
                                      if (proxyState->refCount == 1)
                                          pthread_create(&comm->proxyState->thread, NULL, ncclProxyService, ...)
                                          |
                                          v
                                          [New Thread] ncclProxyService()
                                          |
                                          +-> proxyProgressAsync(...)
                                          |   |
                                          |   +-> proxyConnInit(...)
                                          |       |
                                          |       +-> proxyProgressInit(proxyState)
                                          |           |
                                          |           +-> ncclProxyProgressCreate(proxyState)
                                          |               |
                                          |               +-> if (!state->thread)
                                          |                       pthread_create(&state->thread, NULL, ncclProxyProgress, ...)
                                          |                       |
                                          |                       v
                                          |                       [New Thread] ncclProxyProgress()
                                          |
                                          +---> proxyServiceInitOp(...)
                                                |
                                                +---> proxyProgressAsync(...)  (same path as above)
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
The ASCII diagram shows how user API calls (e.g.\ \texttt{ncclCommInitRank}, \texttt{ncclCommSplit}) funnel into two internal entry points (\texttt{ncclCommInitRankDev} or \texttt{ncclCommInitChildComm}), then into \texttt{ncclCommInitRankFunc()}, \texttt{initTransportsRank()}, and \texttt{ncclProxyCreate(comm)}. From there, NCCL conditionally spawns the \texttt{ProxyService} thread; that thread later lazily starts the \texttt{ProxyProgress} thread via \texttt{proxyProgressAsync} $\to$ \texttt{ncclProxyProgressCreate} $\to$ \texttt{pthread\_create(ncclProxyProgress)}.

\paragraph{Representation as a sequence diagram.}
We model four participants: the \emph{User} (application), \emph{NCCL} (internal call chain), and the two spawned threads \emph{ProxyService} and \emph{ProxyProgress}. The User invokes NCCL; NCCL performs internal calls (self-calls) and then creates the ProxyService thread; ProxyService creates the ProxyProgress thread. This makes the creation order and the two-level thread spawn explicit.
\fi

\begin{figure}[htbp]
\centering
\begin{sequencediagram}
\newthread{user}{User}
\newinst[2]{nccl}{NCCL init.cc}
\newinst[2]{proxySvc}{ProxyService}
\newinst[2]{proxyProg}{ProxyProgress}
\begin{call}{user}{initRank / split / \ldots}{nccl}{}
  \begin{callself}{nccl}{ncclCommInitRankFunc}{}
    \begin{callself}{nccl}{initTransportsRank}{}
      
      \begin{callself}{nccl}{ncclProfilerPluginInit()}{}
      \end{callself}

      \begin{callself}{nccl}{ncclProxyCreate}{}
        
        \begin{sdblock}{\texttt{if (proxyState->refCount == 1)}}{}
          \begin{call}{nccl}{create}{proxySvc}{}
            
            % \begin{callself}{proxySvc}{proxyProgressAsync}{}
            % \begin{callself}{proxySvc}{ncclProxyProgressCreate}{}
            \begin{sdblock}{\texttt{if (!state->thread)}}{}
              \begin{call}{proxySvc}{create}{proxyProg}{}
              \end{call}
            \end{sdblock}
            % \end{callself}
            % \end{callself}
            
          \end{call}
        \end{sdblock}
      \end{callself}
    \end{callself}
  \end{callself}
\end{call}
\end{sequencediagram}
\caption{Thread creation: User API $\to$ NCCL internal init $\to$ create ProxyService $\to$ create ProxyProgress.}
\label{fig:thread-creation}
\end{figure}


The guards \texttt{if (proxyState->refCount == 1)} 
and \texttt{if (!state->thread)}
ensure that these threads are created once per shared resource (struct \texttt{ncclSharedResources}). 
The SharedResource has a ProxyState field. The fields in ProxyState are used to ensure only one instance of each thread exists:

\textbf{/src/include/comm.h}
\begin{lstlisting}[language=C]
struct ncclSharedResources {
  struct ncclComm* owner; /* communicator which creates this shared res. */
  struct ncclProxyState* proxyState;
  // other fields
}
\end{lstlisting}

\textbf{/src/include/proxy.h}
\begin{lstlisting}[language=C]
struct ncclProxyState {
  int refCount;
  pthread_t thread;
  ncclProxyProgressState progressState;
  // other fields
}

struct ncclProxyProgressState {
  struct ncclProxyOpsPool* opsPool;
  // other fields
}

struct ncclProxyOpsPool {
  struct ncclProxyOp ops[MAX_OPS_PER_PEER*NCCL_MAX_LOCAL_RANKS];
  // other fields
}
  
struct ncclProxyOps {
  // other fields
}
\end{lstlisting}

By default every NCCL communicator has its own shared resource. When the application calls \texttt{ncclCommSplit()} or \texttt{ncclCommShrink()} where the original communicator was initialized with a \texttt{ncclConfig\_t} with fields \texttt{splitShare} or \texttt{shrinkShare} set to \texttt{1}, the newly created communicator shares the shared resource (and the proxy threads) with the parent communicator.

\small
\begin{quote}
\texttt{/* proxyState is shared among parent comm and split comms}\\
\texttt{comm->proxyState->thread is pthread\_join()'d by commFree() in init.cc }\\
\texttt{when the refCount reduces down to 0. */}
\end{quote}
(Quoted from \textbf{/src/proxy.cc})
\normalsize


Later, whenever the application calls the NCCL User API, 
NCCL internally decides what network operations to perform and calls \texttt{ncclProxyPost()} to post them to a proxyOpsPool
(See Figure~\ref{fig:proxy-post}).

\iffalse
\begin{lstlisting}[style=ascii]
Flow from User API Calls to ncclProxyPost()
-------------------------------------------------------------------------------
User API
---------------------------------------------------------------------------------
ncclCommInitAll()          -+    ncclAllGather()      -+
ncclCommInitRankConfig()    |    ncclAlltoAll()        |
ncclCommInitRankScalable()  |    ncclAllReduce()       |
ncclCommFinalize()          |    ncclBroadcast()       |
ncclCommDestroy()           |    ncclGather()          |
ncclCommRevoke()            |    ncclReduce()          |
ncclCommAbort()             |    ncclReduceScatter()   |
ncclCommSplit()             |    ncclScatter()         |
ncclCommShrink()            |    ncclSend()            |
ncclCommGrow()              |    ncclRecv()           -+
ncclDevCommCreate()         |                          |
ncclCommWindowRegister()    |                          |
ncclGroupSimulateEnd()     -+                          |
-------------------------------------------------------------------------------
Internal Flow
-------------------------------------------------------------------------------
                            |                          |
                            |                          v
                            |                   ncclEnqueueCheck()
                            +--------------------------+
                            v
                            ncclGroupEndInternal()
                            +----+
                            |    v
                            |    groupLaunchNonBlocking()
                            +----+
                            v
                            groupLaunch()
                            |
                            v
                            doLaunches()
                            +-> ncclLaunchPrepare() ------------+
                            +-> ncclLaunchKernelAfter_NoCuda()  v
                                |                               ncclLaunchPrepare()
                                |                               |
                                |                               v
                                |                               cudaLaunchHostFunc()
                                |                               |
                                |                               v
                                |                               hostStreamPlanCallback()
                                +-------------------------------+
                                v
                                hostStreamPlanTask()
                                +-> uploadProxyOps() -+
                                +-> ncclProxyStart()  v
                                    |                 ncclProxySaveOp()
                                    |                 |
                                    |                 v
                                    |                 SaveProxy()
                                    |                 |
                                    |                 v
                                    |                 ncclLocalOpAppend()
                                    +-----------------+
                                    v
                                    ncclProxyPost() (proxy.cc)
                                    +-> [Posts Ops to pool]
                                    +-> [Signals Proxy Progress Thread]
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
Two families of user API calls lead to \texttt{ncclProxyPost()}: (1)~communicator lifecycle and group operations (\texttt{ncclCommInitAll}, \texttt{ncclCommSplit}, \texttt{ncclGroupSimulateEnd}, etc.) and (2)~collectives and P2P (\texttt{ncclAllReduce}, \texttt{ncclSend}, etc.). Both converge at \texttt{ncclGroupEndInternal()}, then \texttt{groupLaunch()} $\to$ \texttt{doLaunches()}. From there, kernel launch and host-callback paths lead to \texttt{hostStreamPlanTask()}, which calls \texttt{uploadProxyOps()} / \texttt{ncclProxyStart()} $\to$ \texttt{SaveProxy()} $\to$ \texttt{ncclProxyPost()}, where ops are posted to the pool and the ProxyProgress thread is signalled.

\paragraph{Representation as a sequence diagram.}
We show the \emph{User} invoking \emph{NCCL} with either a collective/P2P call or a comm/group call; NCCL's internal flow is collapsed into a chain of self-calls that merge at \texttt{ncclGroupEndInternal()} and end at \texttt{ncclProxyPost()}. The diagram emphasizes the single internal path after the merge and the role of \texttt{ncclProxyPost()} as the point where ops are posted and the proxy thread is woken.
\fi

\begin{figure}[htbp]
\centering
\begin{sequencediagram}
\newthread{user}{User}
\newinst[2]{nccl}{NCCL}
% \newinst[2]{cuda}{CUDA host stream}
\newinst[2]{proxy}{ProxyProgress}
\newinst[2]{opspool}{ncclProxyOpsPool}
\begin{call}{user}{User API (collective / comm / group)}{nccl}{}

  \begin{callself}{nccl}{ncclGroupEndInternal}{}
    \begin{callself}{nccl}{groupLaunch}{}
      \begin{callself}{nccl}{doLaunches}{}

        % \begin{callself}{nccl}{ncclLaunchPrepare}{}
        %   \begin{call}{nccl}{cudaLaunchHostFunc(hostStream, hostStreamPlanCallback)}{cuda}{}
        %   \end{call}
        % \end{callself}

        \begin{callself}{nccl}{hostStreamPlanTask}{}
          \begin{call}{nccl}{ncclProxyPost}{opspool}{}
          \end{call}
          \begin{call}{nccl}{signal}{proxy}{}
          \end{call}
        \end{callself}

      \end{callself}
    \end{callself}
  \end{callself}

\end{call}

% \begin{callself}{cuda}{hostStreamPlanCallback}{}
%   \begin{callself}{cuda}{hostStreamPlanTask}{}
%     \begin{call}{cuda}{ncclProxyPost}{opspool}{}
%     \end{call}
%     \begin{call}{cuda}{signal}{proxy}{}
%     \end{call}
%   \end{callself}
% \end{callself}

\end{sequencediagram}%
\caption{Flow from User API to \texttt{ncclProxyPost()}}
\label{fig:proxy-post}
\end{figure}

The ProxyProgress thread reads from this pool when calling \texttt{ncclProxyGetPostedOps()} and progresses the ops.
See Figure~\ref{fig:proxy-progress-loop}.

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
ncclProxyProgress() progressing loop
-------------------------------------------------------------------------------
ncclProxyProgress(proxyState)
|
+-> do {
        +-> progressOps(proxyState, ...)
        |   |
        |   +-> while (op) {
        |           op->progress(proxyState, op);
        |           op = op->next;
        |       }
        |
        +-> ncclProxyGetPostedOps()
            |
            +-> [reads Ops or thread will wait]
    } while (...)
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
The ProxyProgress thread runs a loop: in each iteration it first calls \texttt{progressOps()}, which walks the list of ops and invokes \texttt{op->progress(proxyState, op)} for each; then it calls \texttt{ncclProxyGetPostedOps()}, which either reads newly posted ops from the pool or blocks until the main path signals it. The loop thus alternates between making progress on existing ops and acquiring new work.

\paragraph{Representation as a sequence diagram.}
We use a single participant (the ProxyProgress thread) and a loop block. Inside the loop we show two self-calls: \texttt{progressOps()} (with an inner loop over ops) and \texttt{ncclProxyGetPostedOps()}. This makes the recurring cycle and the two phases (progress vs.\ get posted ops) explicit.
\fi

\begin{figure}[htbp]
\centering
\begin{sequencediagram}
\newthread{proxy}{ProxyProgress}
\newinst[5]{opspool}{ncclProxyOpsPool}
\begin{sdblock}{loop: do \ldots while}{}
  
  \begin{callself}{proxy}{progressOps(proxyState)}{}
  \end{callself}
  
  \begin{call}{proxy}{ncclProxyGetPostedOps [waits if no ops]}{opspool}{proxyState}
  \end{call}

\end{sdblock}
\end{sequencediagram}%
\caption{\textbf{/src/proxy.cc} \texttt{ncclProxyProgress()} progressing loop: progress ops, then get posted ops (or wait). }
\label{fig:proxy-progress-loop}
\end{figure}

In the next section, understanding the behaviour of NCCL for network-related activity is helpful in relation to the behavior of Profiler Plugin API.

% ---------------------------------------------------------------------------
\section{The Profiler API}
% ---------------------------------------------------------------------------

\subsection{How NCCL detects the profiler plugin}

When a NCCL communicator is created, NCCL looks for a shared library that represents the profiler plugin by checking an environment variable: \\
\texttt{profilerName = ncclGetEnv("NCCL\_PROFILER\_PLUGIN");} \\
It then calls \\
\texttt{handle* = dlopen(name, RTLD\_NOW | RTLD\_LOCAL);} \\
and \\
\texttt{ncclProfiler\_v5 = (ncclProfiler\_v5\_t*)dlsym(handle, "ncclProfiler\_v5");} \\
to load the library immediately with local symbol visibility. See Figure~\ref{fig:profiler-init}.

\begin{quote}
\begin{itemize}
  \item If \texttt{NCCL\_PROFILER\_PLUGIN} is set: attempt to load the library with the specified name; if that fails, attempt \texttt{libnccl-profiler-<NCCL\_PROFILER\_PLUGIN>.so}.
  \item If \texttt{NCCL\_PROFILER\_PLUGIN} is not set: attempt \texttt{libnccl-profiler.so}.
  \item If no plugin was found: profiling is disabled.
  \item If \texttt{NCCL\_PROFILER\_PLUGIN} is set to \texttt{STATIC\_PLUGIN}, the plugin symbols are searched in the program binary.
\end{itemize}
\end{quote}
\begin{flushleft}
\small
(Source: \url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html\#nccl-profiler-plugin})
\normalsize
\end{flushleft}


The profiler plugin is loaded when creating a communicator (before proxy thread creation). 
The plugin loading mechanism expects the struct variable name to follow the naming convention\\ 
\texttt{ncclProfiler\_v\{versionNum\}}, which also indicates the API version.

The profiler API has changed multiple times with newer NCCL releases; 
NCCL features a fallback mechanism to load older struct versions.
However from personal experience, backwards compatibility with plugins from API version 2 may be limited.
One instance is known, where a profiler plugin being developed against the NCCL release 2.25.1 with Profiler API version 2, was unable to run with the latest NCCL release.
NCCL developers may be required to actively maintain older API versions, to ensure they safely work when old behaviour is getting deprecated 
and do not unexpectedly get handed new features from new API versions, 
of which the Profiler Plugin developer is not aware of (when faithfully implementing old API versions).

The exact implementation is in \textbf{/src/plugin/plugin\_open.cc} and \textbf{/src/plugin/profiler.cc}.


\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
User API                          Internal Flow
-------------------------------------------------------------------------------
ncclCommInitRank()         --+
ncclCommInitAll()            |
ncclCommInitRankConfig()     +--> ncclCommInitRankDev()  --+
ncclCommInitRankScalable() --+                             |
                                                           |
ncclCommSplit()            --+                             |
ncclCommShrink()           --+--> ncclCommInitChildComm() -+
                                                           |
ncclCommGrow()             --+    +------------------------+
                             |    v
                             +--> ncclCommInitRankFunc()
                                  |
                                  v                        
                                  initTransportsRank()
                                  +-> ncclProfilerPluginInit(comm)-+
                                  +-> ncclProxyCreate(comm)        |
                                      |                            v
                                      v                            ncclProfilerPluginInit(comm)
                                      ...                          +-> ncclProfilerPluginLoad()
                                                                   |   +-> ncclGetEnv("NCCL_PROFILER_PLUGIN")
                                                                   |   +-> ncclOpenProfilerPluginLib()
                                                                   |   |   +-> dlopen(name, RTLD_NOW | RTLD_LOCAL)
                                                                   |   +-> getNcclProfiler_v5()
                                                                   |   |   +-> (ncclProfiler_v5_t*)dlsym(lib, "ncclProfiler_v5");
                                                                   |   +-> [try fallback to older api version]
                                                                   +-> profiler->init()

\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
User API calls for communicator creation (\texttt{ncclCommInitRank}, \texttt{ncclCommSplit}, etc.) enter NCCL via \texttt{ncclCommInitRankDev()} or \texttt{ncclCommInitChildComm()}, then \texttt{ncclCommInitRankFunc()} $\to$ \texttt{initTransportsRank()}. There, \texttt{ncclProfilerPluginInit(comm)} runs in parallel with \texttt{ncclProxyCreate(comm)}. The profiler path calls \texttt{ncclProfilerPluginLoad()} (get env, \texttt{dlopen}, \texttt{dlsym} for \texttt{ncclProfiler\_v5}), then \texttt{profiler->init()}.

\paragraph{Representation as a sequence diagram.}
We show three participants: \emph{User}, \emph{NCCL}, and \emph{Profiler plugin}. The User invokes NCCL; NCCL performs internal init and then loads and initializes the plugin (load library, get symbol, call \texttt{init}). The diagram makes it clear that the plugin is loaded and \texttt{init()} is called during communicator creation, before proxy threads are created.
\fi

\begin{figure}[H]
\centering
\begin{sequencediagram}
\newthread{user}{User}
\newinst[1.7]{nccl}{NCCL init.cc}
% \newinst[1]{profilercc}{NCCL profiler.cc}
\newinst[2.7]{proxy}{ProxyProgress}
\newinst[1]{plug}{Profiler Plugin}
\begin{call}{user}{initRank / split / \ldots}{nccl}{}
  \begin{callself}{nccl}{ncclCommInitRankFunc}{}
    \begin{callself}{nccl}{initTransportsRank}{}
      
      \begin{call}{nccl}{ncclProfilerPluginInit}{nccl}{}
     
        \begin{callself}{nccl}{ncclProfilerPluginLoad}{}
          \begin{callself}{nccl}{\texttt{ncclGetEnv}}{}
          \end{callself}
          \begin{callself}{nccl}{ncclOpenProfilerPluginLib}{}
            \begin{callself}{nccl}{\texttt{dlopen}}{}
            \end{callself}
          \end{callself}
          \begin{callself}{nccl}{getNcclProfiler\_v5}{}
            \begin{callself}{nccl}{\texttt{dlsym}}{}
            \end{callself}
          \end{callself}

        \end{callself}
        
        \begin{call}{nccl}{\texttt{ncclProfiler->init(commId, \ldots)}}{plug}{}
        \end{call}
     
      \end{call}
      
      \begin{call}{nccl}{ncclProxyCreate}{proxy}{}
      \end{call}
      
    \end{callself}
  \end{callself}
\end{call}
\end{sequencediagram}%
\caption{User API $\to$ NCCL init $\to$ load profiler plugin and call \texttt{profiler->init()}.}
\label{fig:profiler-init}
\end{figure}

\subsection{The profiler API definition}

The plugin must implement a profiler API specified by NCCL by exposing a struct. 
This struct should contain pointers to all functions required by the API. 
A plugin may expose multiple versioned structs for backwards compatibility with older NCCL versions.

\begin{lstlisting}[language=C]
ncclProfiler_v5_t ncclProfiler_v5 = {
  const char* name;
  ncclResult_t (*init)(...);              // NCCL calls this right after loading
  ncclResult_t (*startEvent)(...);        // at start of operations/activities
  ncclResult_t (*stopEvent)(...);         // at end of these operations/activities
  ncclResult_t (*recordEventState)(...);  // to record state of certain operations
  ncclResult_t (*finalize)(...);          // before unloading the plugin
};
\end{lstlisting}

The full profiler API is under \textbf{/src/include/plugin/profiler/profiler\_v\{versionNum\}.cc}. As of NCCL v2.29.1, version 6 is the latest; five functions must be implemented. 
Internally NCCL wraps calls to the profiler API in custom functions (found in \textbf{/src/include/profiler.h}).

NCCL invokes the profiler API at different levels to capture start/stop of NCCL groups, collectives, P2P, proxy, kernel and network activity. 
As the API function names suggest, this will allow the profiler to track these operations and activities as events.

The API functions and where NCCL invokes them are explained in the following sections.

\subsubsection{init}

\texttt{init} initializes the profiler plugin. NCCL passes follwing arguments:

\begin{lstlisting}[language=C]
ncclResult_t init(
  void** context,          // out param - opaque profiler context
  uint64_t commId,         // communicator id
  int* eActivationMask,    // out param - bitmask for which events are tracked
  const char* commName,    // user assigned communicator name
  int nNodes,              // number of nodes in communicator
  int nranks,              // number of ranks in communicator
  int rank,                // rank identifier in communicator
  ncclDebugLogger_t logfn  // logger function
);
\end{lstlisting}

Every time a communicator is created, \texttt{init()} is called immediately upon successful plugin load in \texttt{ncclProfilerPluginLoad()} 
(see Figure~\ref{fig:profiler-init}). If the profiler plugin \texttt{init} function does not return \texttt{ncclSuccess}, NCCL disables the plugin.


\begin{quote}
As soon as NCCL finds the plugin and the correct ncclProfiler symbol, it calls its init function. This allows the plugin to initialize its internal context used during profiling of NCCL events.
\end{quote}
(Source: \textbf{/ext-profiler/README.md})

\texttt{void** context} is an opaque handle that the plugin developer may point to any custom context object; this pointer is passed again in \texttt{startEvent} and \texttt{finalize}. This context object is separate per communicator.

The plugin developer should set \texttt{int* eActivationMask} to a bitmask indicating which event types the profiler wants to track. The mapping is in \textbf{/src/include/plugin/nccl\_profiler.h}; internally the mask defaults to 0 (no events). Setting it to 4095 will track all events.

\texttt{ncclDebugLogger\_t logfn} is a function pointer to NCCL's internal debug logger (\texttt{ncclDebugLog}). 
NCCL passes this so the plugin can emit log lines through the same channel and filtering as NCCL: 
the plugin may store the callback and call it with \texttt{(level, flags, file, line, fmt, ...)} when it wants to log. 
Messages then appear in NCCL's debug output (e.g.\ stderr or \texttt{NCCL\_DEBUG\_FILE}) and respect the user's \texttt{NCCL\_DEBUG} level and subsystem mask. 
Using \texttt{logfn} keeps profiler output consistent with NCCL's own logs.

\subsubsection{startEvent}

\texttt{startEvent} is called when NCCL begins certain operations:

\begin{lstlisting}[language=C]
ncclResult_t startEvent(
  void* context,                       // opaque profiler context object
  void** eHandle,                      // out param - event handle
  ncclProfilerEventDescr_v5_t* eDescr  // pointer to event descriptor
);
\end{lstlisting}

As of release v2.29.1 NCCL does not care about the return value. 
\texttt{void** eHandle} may point to a custom event object; this pointer is passed again in \texttt{stopEvent} and \texttt{recordEventState}. 
\texttt{eDescr} describes the started event. Its exact details can be found in \textbf{/src/include/plugin/profiler/}. 

The field \texttt{void* parentObj} in the event descriptor is the \texttt{eHandle} of a parent event (or null).
The use of this field can be explained as following:

All User API calls to Collective or P2P operations will start a Group API event.
When networking is required, ProxyCtrl Events may be emitted. 
Depending on the `eActivationMask` bitmask returned in the `init()` function, 
further (child) events will be emitted in deeper sections of the nccl code base. 
It can be thought of as an event hierarchy with several depth levels:


\begin{quote}
\begin{lstlisting}[style=ascii]
  Group API event
  |
  +- Collective API event
  |  |
  |  +- Collective event
  |     |
  |     +- ProxyOp event
  |     |  |
  |     |  +- ProxyStep event
  |     |     |
  |     |     +- NetPlugin event
  |     |
  |     +- KernelCh event
  |
  +- Point-to-point API event
  |  |
  |  +- Point-to-point event
  |     |
  |     +- ProxyOp event
  |     |  |
  |     |  +- ProxyStep event
  |     |     |
  |     |     +- NetPlugin event
  |     |
  |     +- KernelCh event
  |
  +- Kernel Launch event

ProxyCtrl event
\end{lstlisting}
\end{quote}
(Source: \textbf{/ext-profiler/README.md})

If the profiler enables tracking for event types lower in the hierarchy, NCCL also tracks their parent event types.
The \texttt{parentObj} inside \texttt{eDescr} will be a reference to the \texttt{eHandle} of the respective parent event for the current event according to this hierarchy. 

\subsubsection{stopEvent}

\begin{lstlisting}[language=C]
ncclResult_t stopEvent(void* eHandle);  // handle to event object
\end{lstlisting}
\texttt{stopEvent} tells the plugin that the event has stopped. \texttt{stopEvent} for collectives simply indicates to the profiler that the collective has been enqueued and not that the collective has been completed.

NCCL ignores the return value. \texttt{stopEvent} is called in the same functions that call \texttt{startEvent}, except for the GroupApi event (see diagram).

Figure~\ref{fig:profiler-events} shows when NCCL emits \texttt{startEvent} and \texttt{stopEvent} after a User API call.
The Proxy\-Progress thread also emits \texttt{startEvent} and \texttt{stopEvent} while progressing ops (see Figure~\ref{fig:proxy-event-emission}).

\iffalse
\begin{lstlisting}[style=ascii]
Flow from NCCL API Calls to profiler events
---------------------------------------------------------------------------------
User API
---------------------------------------------------------------------------------
ncclCommInitAll()          -+    ncclAllGather()      -+      
ncclCommInitRankConfig()    |    ncclAlltoAll()        |      
ncclCommInitRankScalable()  |    ncclAllReduce()       |      
ncclCommFinalize()          |    ncclBroadcast()       |      
ncclCommDestroy()           |    ncclGather()          |      
ncclCommRevoke()            |    ncclReduce()          |      
ncclCommAbort()             |    ncclReduceScatter()   |      
ncclCommSplit()             |    ncclScatter()         |      
ncclCommShrink()            |    ncclSend()            |      
ncclCommGrow()              |    ncclRecv()           -+      
ncclDevCommCreate()         |                          |      
ncclCommWindowRegister()    |                          |
ncclGroupSimulateEnd()     -+                          |
                            |                          | 
---------------------------------------------------------------------------------
Internal Flow
---------------------------------------------------------------------------------
                            |                          | 
                            |                          v      
                            |                          ncclEnqueueCheck()
                            |                          +-> taskAppend()
                            +--------------------------+   +-> collTaskAppend()
                            |                              |   +-> Emits: GroupApi Event (start), CollApi Event
                            |                              +-> p2pTaskAppend()
                            v                                  +-> Emits: GroupApi Event (start), P2pApi Event
                            ncclGroupEndInternal()
                            +-> Emits: GroupApi Event (stop, at end of function call if eHandle exists)
                            +-------+
                            |       v
                            |       groupLaunchNonBlocking()
                            +-------+
                            v
                            groupLaunch()
                            |
                            v
                            doLaunches()
                            +-> ncclLaunchPrepare() ------------+
                            +-> ncclLaunchKernel()              |
                            |   +-> Emits: KernelLaunch Event   |
                            +-> ncclLaunchKernelAfter_NoCuda()  v
                                |                               ncclLaunchPrepare()
                                |                               |
                                |                               v
                                |                               cudaLaunchHostFunc()
                                |                               |
                                |                               v
                                |                               hostStreamPlanCallback()
                                +-------------------------------+
                                v
                                hostStreamPlanTask()
                                +-> Emits: Group Event, Coll Event, P2p Event
                                +-> uploadProxyOps() -+
                                +-> ncclProxyStart()  v
                                    |                 ...
                                    v
                                    ...
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
The same two families of user API calls (comm/group vs.\ collectives/P2P) lead to profiler events. For collectives/P2P, \texttt{ncclEnqueueCheck()} $\to$ \texttt{taskAppend()} emits GroupApi (start) and CollApi or P2pApi (start). At \texttt{ncclGroupEndInternal()} NCCL emits GroupApi (stop). In \texttt{doLaunches()}, \texttt{ncclLaunchKernel()} emits KernelLaunch; \texttt{hostStreamPlanTask()} emits Group, Coll, P2p events; then \texttt{uploadProxyOps()} / \texttt{ncclProxyStart()} continue the path. The diagram shows where \texttt{startEvent} and \texttt{stopEvent} are invoked relative to the internal call flow.

\paragraph{Representation as a sequence diagram.}
We show \emph{User}, \emph{NCCL}, and \emph{Profiler plugin}. The User calls NCCL; NCCL performs internal steps and at specific points calls the plugin (\texttt{startEvent} / \texttt{stopEvent}). The sequence diagram makes the order of API calls and the injection points of profiler callbacks explicit.
\fi

\begin{figure}[H]
\centering
\resizebox{!}{20cm}{%
\begin{sequencediagram}
\newthread{user}{User}
\newinst[2]{nccl}{NCCL}
% \newinst[1.7]{cuda}{CUDA host stream}
\newinst[1]{proxy}{ProxyProgress}
\newinst[1]{opspool}{ncclProxyOpsPool}
\newinst[1.7]{plug}{Profiler plugin}

\begin{call}{user}{User collectives API}{nccl}{}
  
  \begin{callself}{nccl}{ncclEnqueueCheck}{}

    \begin{callself}{nccl}{taskAppend}{}
      \begin{call}{nccl}{startEvent (GroupApi)}{plug}{}
      \end{call}
      \begin{call}{nccl}{startEvent / stopEvent (CollApi / P2pApi)}{plug}{}
      \end{call}
    \end{callself}
    
    \begin{call}{nccl}{ncclGroupEndInternal}{nccl}{}
      
      \begin{callself}{nccl}{groupLaunch}{}
        \begin{callself}{nccl}{doLaunches}{}

          % \begin{callself}{nccl}{ncclLaunchPrepare}{}
          %   \begin{call}{nccl}{hostStreamPlanCallback}{nccl}{}
          %   \end{call}
          % \end{callself}

          \begin{callself}{nccl}{ncclLaunchKernel}{}
            \begin{call}{nccl}{startEvent / stopEvent (KernelLaunch)}{plug}{}
            \end{call}
          \end{callself}

          % \begin{callself}{nccl}{hostStreamPlanCallback}{}
          \begin{callself}{nccl}{hostStreamPlanTask}{}
            \begin{call}{nccl}{startEvent (Group)}{plug}{}
            \end{call}
            \begin{call}{nccl}{startEvent (Coll / P2p)}{plug}{}
            \end{call}
            
            \begin{call}{nccl}{ncclProxyPost}{opspool}{}
            \end{call}
            \begin{call}{nccl}{signal}{proxy}{}
            \end{call}
    
            \begin{call}{nccl}{StopEvent (Coll / P2p)}{plug}{}
            \end{call}
            \begin{call}{nccl}{stopEvent (Group)}{plug}{}
            \end{call}
          \end{callself}
          % \end{callself}
        
        \end{callself}
      \end{callself}

      \begin{call}{nccl}{stopEvent (GroupApi)}{plug}{}
      \end{call}    

    \end{call}
  
  \end{callself}
  
\end{call}
\end{sequencediagram}%
}
\caption{Flow from NCCL API calls to profiler events. In case of \texttt{ncclGroupStart / ncclGroupEnd}. multiple events of everything (except GroupApi) are called. internally, some Collectives (e.g. ncclAlltoAll) are implemented as many p2p ops, triggering many P2pApi and P2p events. Implementation: \textbf{/src/init.cc}, \textbf{/src/plugin/profiler.cc}.}
\label{fig:profiler-events}
\end{figure}


\iffalse
NCCL only uses the Cuda callback, if necessary for proxy args. 
Otherwise \texttt{hostStreamPlanTask} is called inside 
\texttt{doLaunches} wrapped inside \texttt{ncclLaunchKernelAfter\_NoCuda}:

\begin{quote}
\textbf{/src/enqueue.cc}
\begin{lstlisting}[language=C]
// We have to launch host tasks to push proxy args. We are careful to only
// do this if necessary since host tasks impose a high performance cost in CUDA.
if (plan->hasProxyOps) {
  // ...
  CUDACHECKGOTO(cudaLaunchHostFunc(hostStream, hostStreamPlanCallback, plan), result, failure);
}

// ...

ncclResult_t ncclLaunchKernelAfter_NoCuda(struct ncclComm* comm, struct ncclKernelPlan* plan) {
  if (!plan->isHostCbEnq) {
    // we are not using the host stream for proxy ops and reclaimation submission, call
    // hostStreamPlanTask directly
    NCCLCHECK(hostStreamPlanTask(comm, plan));
  }
  return ncclSuccess;
}
\end{lstlisting}
\end{quote}
\fi

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
ncclProxyProgress() Event Emission Flow
---------------------------------------------------------------------------------
ncclProxyProgress(proxyState) [Proxy Progress Thread Loop]
|
+-> do {
         |
         +-> progressOps(proxyState, opStart=state->active, ...)
         |   |
         |   +-> while (op) {
         |            op->progress(proxyState, op);
         |            |
         |            +-> [Transport-specific progress function]
         |                (net.cc, coll_net.cc, p2p.cc, shm.cc)
         |                | 
         |                +-> Emits: ProxyStep events
         |                +-> Emits: KernelCh events
         |                +-> Emits: Network plugin specific events
         |            op = op->next;
         |       }
         |
         +-> [Thread Idle/Active State Transitions]
         |   +-> Emits: ProxyCtrl events
         |
         +-> ncclProxyGetPostedOps(proxyState)
             +-> [Thread Sleep/Wakeup State Transitions]
             |     +-> [Thread sleeps, waits for signal]
             |     +-> Emits: ProxyCtrl events
             +-> [Update Op pool] 
             +-> Emits: ProxyCtrl events
             +-> ProxyAppend()
                 +-> ncclProxyOpToArgs()
                     +-> Emits: ProxyOp events
    } while (...)
\end{lstlisting}
\fi
\iffalse
\paragraph{What the diagram conveys.}
Inside the ProxyProgress thread loop, \texttt{progressOps()} invokes transport-specific progress (\texttt{op->progress}); those functions emit ProxyStep, KernelCh, and NetPlugin events. Thread idle/active and sleep/wakeup transitions emit ProxyCtrl events. When \texttt{ncclProxyGetPostedOps()} runs, it may emit ProxyCtrl events; \texttt{ProxyAppend()} / \texttt{ncclProxyOpToArgs()} emit ProxyOp events.

\paragraph{Representation as a sequence diagram.}
We use one participant (ProxyProgress thread) and a loop block. Inside the loop we show: \texttt{progressOps()} (with transport progress and event emission to the plugin), thread state transitions (ProxyCtrl), \texttt{ncclProxyGetPostedOps()} (ProxyCtrl), and \texttt{ProxyAppend()} (ProxyOp). The plugin is shown as a second participant that receives the emitted events.
\fi

\begin{figure}[H]
\centering
\resizebox{!}{13.5cm}{%
\begin{sequencediagram}
\newthread{proxy}{ProxyProgress}
\newinst[4]{opspool}{ncclProxyOpsPool}
\newinst[2]{plug}{Profiler plugin}
\begin{sdblock}{loop: do \ldots while}{}

  \begin{callself}{proxy}{progressOps}{}
    \begin{callself}{proxy}{\texttt{op->progress()} [transport specific progress]}{}
      \begin{call}{proxy}{startEvent / stopEvent (ProxyStep, KernelCh, NetPlugin)}{plug}{}
      \end{call}
    \end{callself}
  \end{callself}
  \begin{call}{proxy}{startEvent / stopEvent (ProxyCtrl)}{plug}{}
  \end{call}
  \begin{callself}{proxy}{ncclProxyGetPostedOps}{}
    \begin{call}{proxy}{startEvent / StopEvent (ProxyCtrl)}{plug}{}
    \end{call}

    \begin{call}{proxy}{get next ops / wait if no ops}{opspool}{}
    \end{call}

    \begin{callself}{proxy}{ProxyAppend}{}
      \begin{callself}{proxy}{ncclProxyOpToArgs}{}
        \begin{call}{proxy}{startEvent / stopEvent (ProxyOp)}{plug}{}
        \end{call}
      \end{callself}
    \end{callself}
  \end{callself}

\end{sdblock}
\end{sequencediagram}%
}
\caption{\texttt{ncclProxyProgress}: progressOps emits ProxyStep/KernelCh/NetPlugin events. getPostedOps emits ProxyOp events. Several events ProxyCtrl are also emitted}
\label{fig:proxy-event-emission}
\end{figure}

\texttt{op->progress()} progresses transport specific ops. 
this is implemented as a function pointer type (defined in \textbf{/src/include/proxy.h}). 
Confusingly the variable is called `op`, although its type is \texttt{ncclProxyArgs} and NOT \texttt{ncclProxyOp}.

\begin{lstlisting}[language=C]
typedef ncclResult_t (*proxyProgressFunc_t)(struct ncclProxyState*, struct ncclProxyArgs*);

struct ncclProxyArgs {
  proxyProgressFunc_t progress;
  struct ncclProxyArgs* next;
  /* other fields */
}
\end{lstlisting}

Which specific function this calls depends on the Op. 
This also decides which profiler events (ProxyStep, KernelCh or Network plugin specific) are started and stopped. 
The transport-specific progress functions are in \textbf{/src/transport/net.cc}, \textbf{coll\_net.cc}, \textbf{p2p.cc}, \textbf{shm.cc}.

\subsubsection{recordEventState}

\begin{lstlisting}[language=C]
  ncclResult_t recordEventState(
    void* eHandle,
    ncclProfilerEventState_v5_t eState,
    ncclProfilerEventStateArgs_v5_t* eStateArgs
  );
\end{lstlisting}
Some event types can be updated by NCCL through \texttt{recordEventState} (state and attributes). 
Supported states can be found under \textbf{/src/include/plugin/profiler/profiler\_v\{versionNum\}.h}.

Called at the same sites as \texttt{startEvent}.

\subsubsection{finalize}

\begin{lstlisting}[language=C]
  ncclResult_t finalize(void* context);
\end{lstlisting}
After a user API call to free resources associated with a communicator, \texttt{finalize()} is called.
Afterwards, a reference counter tracks how many communicators are still being tracked by the profiler plugin. 
If it reaches 0, the plugin will be closed via \texttt{dlclose(handle)}. Figure~\ref{fig:profiler-finalize} depicts the flow from user API call to \texttt{finalize()}.
See implementation at \textbf{/src/init.cc}, \textbf{/src/plugin/profiler.cc}, \textbf{/src/plugin/plugin\_open.cc}.

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
User API                          Internal Flow
-------------------------------------------------------------------------------
ncclCommAbort()    --+
ncclCommDestroy()  --+----------> commReclaim()
                                  |
                                  +-> ncclProfilerPluginFinalize()
                                      |
                                      +-> ncclProfiler->finalize()
                                      +-> ncclProfilerPluginUnload()
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
User API calls \texttt{ncclCommAbort()} or \texttt{ncclCommDestroy()} trigger \texttt{commReclaim()}, which calls \texttt{ncclProfilerPluginFinalize()} $\to$ \texttt{profiler->finalize()}, then \texttt{ncclProfilerPluginUnload()} (e.g.\ \texttt{dlclose}).

\paragraph{Representation as a sequence diagram.}
We show three participants: \emph{User}, \emph{NCCL}, and \emph{Profiler plugin}. The User calls NCCL to destroy/abort the communicator; NCCL runs \texttt{commReclaim()} and then calls the plugin's \texttt{finalize()} and unloads the plugin. The sequence diagram makes the teardown order explicit.
\fi

\begin{figure}[H]
\centering
\resizebox{!}{10cm}{%
\begin{sequencediagram}
\newthread{user}{User}
\newinst[4]{nccl}{NCCL}
\newinst[5]{plug}{Profiler plugin}
\begin{call}{user}{ncclCommAbort / ncclCommDestroy}{nccl}{}
  \begin{callself}{nccl}{commReclaim}{}
  \end{callself}
  \begin{callself}{nccl}{ncclProfilerPluginFinalize}{}
    \begin{call}{nccl}{finalize(context)}{plug}{}
    \end{call}
  \end{callself}
  \begin{callself}{nccl}{ncclProfilerPluginUnload}{}

    \begin{sdblock}{\texttt{if 0 == (---profilerPluginRefCount)}}{}
      \begin{callself}{nccl}{\texttt{dlclose}}{}
      \end{callself}
    \end{sdblock}
    
  \end{callself}
\end{call}
\end{sequencediagram}%
}
\caption{User API $\to$ \texttt{commReclaim()} $\to$ \texttt{finalize()} $\to$ plugin unload.}
\label{fig:profiler-finalize}
\end{figure}

\subsubsection{name}

The profiler plugin struct also has a \texttt{name} field.
The name field should point to a character string with the name of the profiler plugin. 
It will be used for all logging, especially when \texttt{NCCL\_DEBUG=INFO} is set.

\subsection{Code Examples}

The following examples illustrate the profiling behavior under different environment settings and user application.

\begin{itemize}
  \item One Device per Thread
  \item Multiple Devices per Thread utilizing \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}
  \item One Device per Thread and grouped collectives utilizting \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}
  \item Multiple Devices per Thread and grouped collectives utilizting \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}
\end{itemize}

TODO add exampless and pictures (profiler that just does logging + application examples)

A profiler plugin that logs all call information to a file is used in all examples.
An exemplary illustration is shown below:
\begin{lstlisting}[language=C]
struct MyContext { /* custom context struct */ };
struct MyEvent { /* custom event struct */ };

MyEvent* allocEvent(args) { /* handles event allocation */ }
uint64_t getTime() { /* gets time */ }
void writeJsonl() { /* writes call details to log file as structured jsonl */ }

ncclResult_t myInit( /* args - *context, *eActivationMask, ... */ ) {
  *context = malloc(sizeof(struct MyContext));
  *eActivationMask = 4095; /* enable ALL event types */
  
  writeJsonl(getTime(), args);
  return ncclSuccess;
}

ncclResult_t myStartEvent( /* args - *eHandle, ... */ ) {
  *event = allocEvent(args);
  *eHandle = event;

  writeJsonl(getTime(), args);
  return ncclSuccess;
}

ncclResult_t myStopEvent(void* eHandle) {
  writeJsonl(getTime(), eHandle);
  return ncclSuccess;
}

ncclResult_t myRecordEventState( /* args - ... */ ) {
  writeJsonl(getTime(), args);
  return ncclSuccess;
}

ncclResult_t myFinalize(void* context) {
  writeJsonl(getTime(), args);

  free(context);
  return ncclSuccess;
}

ncclProfiler_v5_t ncclProfiler_v5 = {
  "MyProfilerPlugin",
  myInit,
  myStartEvent,
  myStopEvent,
  myRecordEventState,
  myFinalize,
};
\end{lstlisting}

\subsubsection{One Device per Thread}

This example visualizes an AllReduce collective across multiple GPUs (see Fig~\ref{fig:multi-gpu-single-thread-allreduce} and Fig~\ref{fig:multi-gpu-single-thread-show-proxystep}).
Each NCCL thread manages a single GPU. 
This may be achieved by starting out with the same number of MPI tasks with each task running single threaded;
or by having less MPI tasks, but the tasks create multiple thread workers.
Custom initialization without MPI is also possible if desired.

\begin{lstlisting}[language=C]
// broadcast a commId 

// ...

ncclCommInitRank(&rootComm, nRanks, commId, myRank);

// ...

ncclAllReduce(sendBuff, recvBuff, BUFFER_SIZE, ncclFloat, ncclSum, rootComm, streams);

// ...

ncclCommFinalize(rootComm);
ncclCommDestroy(rootComm);
\end{lstlisting}

The profiler API calls are visualized in Fig~\ref{fig:multi-gpu-single-thread-allreduce} and Fig~\ref{fig:multi-gpu-single-thread-show-proxystep}

\begin{figure}[H]
  \centering
\includegraphics[width=0.87\textwidth]{"images/multi_gpu_single_thread/finalize alt (2).png"}
    \caption{One device per thread: 
      First, the profiler API \texttt{init} is called for each rank. 
      This occurs during NCCL's internal communicator creation, 
      roughly when the application calls \texttt{ncclCommInitRank}. 
      Then, the application calls \texttt{ncclAllReduce}. 
      This triggers many \texttt{stateEvent}, \texttt{stopEvent}, and \texttt{recordEventState} calls to the profiler API. 
      First, the \texttt{groupApi} event starts (green bar). 
      Below it, the startEvent and soon the stopEvent for the AllReduce \texttt{collApi} event are called. 
      The yellow bar shows when NCCL enqueues the GPU kernel launch (\texttt{KernelLaunch} event). 
      The two bars below represent the \texttt{group} and \texttt{coll} events. 
      NCCL also spawns a proxy progress thread per rank, which makes additional profiler API calls. 
      The first red \texttt{ProxyCtrl} event shows the proxy progress thread was asleep. 
      Next, a new \texttt{ProxyCtrl} event shows time for the proxy thread to append proxy ops. 
      Then, appended ops start progressing (\texttt{ProxyOps} events), which in \texttt{op->progress()} starts \texttt{ProxyStep} and \texttt{KernelCh} events that inform about low level network activity. 
      Network activity evntually completes and the AllReduce collective finishes. 
      The next \texttt{ProxyCtrl} event shows the proxy thread sleeping again. 
      Finally, profiler \texttt{finalize} is called, which happens when the application cleans up NCCL communicators and no further communicators are tracked in the profiler in each respective thread.
      Matching \texttt{coll} events across ranks can be identified for the same collective. 
      This is shown by red lines connecting events with equal \texttt{seqNum} values (from \texttt{eDescr} in the profiler API \texttt{init()} call) across ranks
    }
  \label{fig:multi-gpu-single-thread-allreduce}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/multi_gpu_single_thread/show proxystep alt.png"}
  \caption{One device per thread: In Fig~\ref{fig:multi-gpu-single-thread-allreduce} \texttt{ProxyStep} events have been omitted for visual clarity. 
  However, in multinode settings, many additional profiler API calls for proxyStep events are done, 
  informing about the low level network steps in their event details. 
  The blue dotted lines indicate the \texttt{parentObj} of each proxyStep event, 
  which are the above proxyOp events.}
  \label{fig:multi-gpu-single-thread-show-proxystep}
\end{figure}

\subsubsection{Multiple Devices per Thread (ncclGroup)}\label{sec:multiple-devices-per-thread-ncclgroup}

In this example, one NCCL thread manages all GPUs on the same node. 
This is achieved by wrapping communication initialization 
in \texttt{ncclGroupStart} and \texttt{ncclGroupEnd} for each managed GPU.
In this orchestration setting, \textbf{NVIDIA's documentation states that all collective API calls should also be wrapped in ncclGroup.}

Here, only one collective operation (per device) is inside the ncclGroup:

\begin{lstlisting}[language=C]
// broadcast a commId 

// ...

ncclGroupStart();
for (int i=0; i<num_gpus; i++) {
  int dev = devlist ? devlist[i] : i;
  cudaSetDevice(dev);
  ncclCommInitRankDev(comms+i, num_gpus,1, &uniqueId, i, dev, &config, __func__);
}
ncclGroupEndInternal();

// alternatively to above method, NCCL provides the convenience function 
// ncclCommInitAll();

// ...

ncclGroupStart();
for (int i = 0; i < num_gpus; i++) {
  ncclAllReduce( /* ... */ );
}
ncclGroupEnd();

// ...

for (int i = 0; i < num_gpus; i++) {
  ncclCommDestroy(comms[i]);
}
\end{lstlisting}

The full example is located in \textbf{/examples/03\_collectives/01\_allreduce/}.

In this example case, the profiler API behavior remains largely the same:
The one difference is that NCCL internally calls the profiler API groupApi event 
only one time in total for grouped collectives within a thread.
Otherwise all other events are processed as usual and are called their usual amount of times irrespective of \texttt{ncclGroup}.
This is visualized in Fig~\ref{fig:single-gpu-single-process-ncclgroup-finalize}.
This behaviour also holds true within a process. 
It also holds when grouping (single) collectives for different communicators.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclgroup 03_communicators_01_allreduce/init.png"}
    \caption{Multiple devices per thread: Events from the proxy thread are omitted. Grouped collectives within a thread only trigger a single \texttt{GroupApi} event}
  \label{fig:single-gpu-single-process-ncclgroup-finalize}
\end{figure}

\subsubsection{One Device per Thread \& Grouped Collectives}
In this example, the setting is such that only a single GPU is managed by a thread,
but multiple collective operations are grouped (i.e. to optimize communication efficiency):

\begin{lstlisting}[language=C]
// comm init
ncclUniqueId rootId;
if (myRank == 0) { ncclGetUniqueId(&rootId); }
MPI_Bcast(&rootId, sizeof(ncclUniqueId), MPI_BYTE, 0, MPI_COMM_WORLD);
ncclComm_t rootComm;
ncclCommInitRank(&rootComm, nRanks, rootId, myRank);

// collective ops
ncclGroupStart();
ncclAllReduce( /* ... */ );
ncclBroadcast( /* ... */ );
ncclReduce( /* ... */ );
ncclAllGather( /* ... */ );
ncclReduceScatter( /* ... */ );
ncclGroupEnd();
\end{lstlisting}

The behavior changes can be described as follow:
\begin{itemize}
  \item single groupApi event per thread
  \item single KernelLaunch event per thread
  \item single group event per thread
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclgroup stress test/ncclgroup commA 1 collX + commA 1 collY/phase 2.5, 1st iter.png"}
  \caption{one GPU per thread with grouped collectives: multiple collective calls are grouped together and nccl does only a single kernel launch per thread.}
  \label{fig:one-gpu-per-thread-grouped-collectives}
\end{figure}

\iffalse
\subsubsection{Multiple Devices per Thread and grouped collectives utilizting \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}}

In this setting, ncclGroup is used to manage multiple GPU by one thread
\textbf{and} multiple collective operations are within a group.
This is almost the same example as in the case for
\hyperref[sec:multiple-devices-per-thread-ncclgroup]{Multiple Devices per Thread (ncclGroup)},
but with additional collectives inside the ncclGroup:

\begin{lstlisting}[language=C]
  
  // ...
  
  ncclGroupStart();
  for (int i = 0; i < num_gpus; i++) {
    ncclAllReduce( /* ... */ );
    ncclAllReduce( /* ... */ );
    ncclBroadcast( /* ... */ );
  }
  ncclGroupEnd();
  
  // ...
  
\end{lstlisting}

\begin{itemize}
  \item single groupApi event
  \item single KernelLaunch event
  \item single group event
\end{itemize}

TODO run 03\_example\_01\_allreduce++ 
(same, except its group(allreduce allreduce broadcast))
slurm 2798216

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{"images/ncclgroup 03_communicators_01_allreduce++/init, 2 api calls alt.png"}
  \caption{Multiple GPUs per thread: When collectives are grouped, only a single GroupApi event for all ranks managed by the thread is emitted. For each rank a single Kernel for the grouped collective operations is launched. All individual \texttt{CollectiveApi} and \texttt{Collective} events are emitted. This also holds true within a process.}
  \label{fig:multi-gpu-per-thread-grouped-collectives}
\end{figure}


TODO this behaviour holds in thread/process/comm combinations how?
\fi

\subsection{Performance and scalability of the Profiler Plugin API}
 
Experiments were run to assess the performance and scalability of profiler plugins.

The profiler used for the experiments initializes a dummy context struct, returns NULL for event handles and tracks all events 4095 (including those for network activity).
\begin{lstlisting}[language=C]
// an 'empty' NCCL Profiler Plugin

struct MyContext {
  char dummy;
};

ncclResult_t myInit(void** context, uint64_t commId, int* eActivationMask, const char* commName, int nNodes, int nranks, int rank, ncclDebugLogger_t logfn) {
  *context = malloc(sizeof(struct MyContext));
  *eActivationMask = 4095; /* enable ALL event types */
  return ncclSuccess;
}

ncclResult_t myStartEvent(void* context, void** eHandle, ncclProfilerEventDescr_v5_t* eDescr) {
  *eHandle = NULL;
  return ncclSuccess;
}

ncclResult_t myStopEvent(void* eHandle) {
  return ncclSuccess;
}

ncclResult_t myRecordEventState(void* eHandle, ncclProfilerEventState_v5_t eState, ncclProfilerEventStateArgs_v5_t* eStateArgs) {
  return ncclSuccess;
}

ncclResult_t myFinalize(void* context) {
  free(context);
  return ncclSuccess;
}

ncclProfiler_v5_t ncclProfiler_v5 = {
  "EmptyProfiler",
  myInit,
  myStartEvent,
  myStopEvent,
  myRecordEventState,
  myFinalize,
};
\end{lstlisting}

For testing the performance overhead in collective and P2P operations, \textbf{nccl-tests} from NVIDIA was used\footnote{\url{https://github.com/NVIDIA/nccl-tests}}.

The applications were launched with follwing test parameters:
message size 64\,B, 1\,000\,000 iterations per size, 100 warmup iterations. 
Single-node jobs used one node and 4 GPUs; 
multi-node jobs used 2 nodes, 4 GPUs per node, 8 MPI ranks in total. 
For each experiment, the application was run once without the profiler and once with the empty profiler plugin.

The Table~\ref{tab:profiler-overhead} shows the average latency per operation (time in $\mu$s) across iterations.
The empty profiler adds roughly 8 to 9\,$\mu$s overhead per operation in single-node runs (4 GPUs), but introduces negligible overhead in multi-node runs (8 GPUs across 2 nodes).

\begin{table}[H]
\centering
\caption{Profiler overhead: nccl-tests \texttt{sendrecv\_perf} (P2P) and \texttt{all\_reduce\_perf} (collectives). Latency averaged over 1M iterations.}
\label{tab:profiler-overhead}
\begin{tabular}{llrr}
\hline
Test & Environment & Without profiler ($\mu$s) & With profiler ($\mu$s) \\
\hline
P2P (\texttt{sendrecv\_perf}) & Single-node (4 GPUs) & 14.3 & 23.88 \\
 & Multi-node (2$\times$4 GPUs) & 13.05 & 12.95 \\
\hline
Collectives (\texttt{all\_reduce\_perf}) & Single-node (4 GPUs) & 14.96 & 23.29 \\
 & Multi-node (2$\times$4 GPUs) & 17.99 & 18.34 \\
\hline
\end{tabular}
\end{table}

\iffalse
multinode
\begin{itemize}
  \item commops multinode (100 iters, 10 warmup iters):
  \begin{itemize}
    \item sbatch run 1: "init, destroy" experiment
    \begin{itemize}
      \item 1790.05 s overhead. averaged 577455.37 s per iter w/o. 579245.42 s w/ profiler
    \end{itemize}
    \item sbatch run 1: "init once, split, destroy" experiment
    \begin{itemize}
      \item 1546.16 s overhead. averaged 1079679.93 s per iter w/o. 1078133.77 s w/ profiler
    \end{itemize}
  \end{itemize}
\end{itemize}

TODO describe sbatch run flags for nccl-tests

There are two experiments for communication operations. 
First, the time of \texttt{ncclCommInit} and \texttt{ncclCommDestroy} are tested.
Second, initialization happens once and \texttt{ncclCommSplit} and \texttt{ncclCommDestroy} are tested:

synthetic - TODO add application code in an appendix section or something


\begin{lstlisting}[language=C]
uint64_t start_time = get_time_ns();
for (int iter = 0; iter < num_iterations; iter++) {
  ncclUniqueId id;
  if (mpi_rank == 0) { ncclGetUniqueId(&id); }
  MPI_Bcast((void *)&id, sizeof(id), MPI_BYTE, 0, MPI_COMM_WORLD);

  ncclComm_t comm;
  ncclCommInitRank(&comm, mpi_size, id, mpi_rank);
  ncclCommDestroy(comm);
}
uint64_t end_time = get_time_ns();
\end{lstlisting}

\begin{lstlisting}[language=C]
  ncclUniqueId id;
  if (mpi_rank == 0) { ncclGetUniqueId(&id); }
  MPI_Bcast((void *)&id, sizeof(id), MPI_BYTE, 0, MPI_COMM_WORLD);
  
  ncclComm_t parent_comm;
  ncclCommInitRank(&parent_comm, mpi_size, id, mpi_rank);

  uint64_t start_time = get_time_ns();
  for (int iter = 0; iter < num_iterations; iter++) {
    ncclComm_t comm;

    int color = mpi_rank % 2;
    int key = mpi_rank;

    ncclCommSplit(parent_comm, color, key, &child_comm, NULL)
    ncclCommDestroy(comm);
  }
  uint64_t end_time = get_time_ns();
\end{lstlisting}
\fi

Using the profiler plugin when scaled to many gpus across multiple nodes is effortless 
and did not require any changes in the profiler plugin for the used code examples and experiments.

% ---------------------------------------------------------------------------
\section{Discussion}
% ---------------------------------------------------------------------------

\subsection{Considerations for developers of a Profiler Plugin}

\paragraph{Logging.} TODO
is this even worth talking about? Code snippet: custom logging infrastructure, timestamping

example records may look like this 
\begin{lstlisting}
{"type":"ProfilerLifecycle","func":"ProfilerInit","gpuUuid":"785dffffffb4ffffffad-0fffffff87-ffff","commId":16819983883469885264,"rank":0,"start":{"ts":9222912570430.104,"pid":990077,"tid":990096},"stop":{"ts":9222912570430.104,"pid":990077,"tid":990096},"duration":0.0,"details":{"nranks":4,"commName":"","eventMask":4095}}
{"recordType":"state","eventAddr":"0x7f6813f084d8","ts":9222912622786.445,"name":"EndGroupApiStart","id":23,"pid":990077,"tid":990077}
{"recordType":"event","type":"ncclProfileCollApi","gpuUuid":"785dffffffb4ffffffad-0fffffff87-ffff","commId":16819983883469885264,"rank":0,"start":{"ts":9222912622801.756,"pid":990077,"tid":990077},"stop":{"ts":9222912622811.686,"pid":990077,"tid":990077},"duration":9.930,"parentObj":"0x7f6813f084d8","eventAddr":"0x7f6813f08578","ctx":"0x7f6813eccfa0","details":{"func":"AllReduce","count":33554432,"dtype":"ncclFloat32","root":0,"stream":"0x4bdd010","graphCaptured":false}}
\end{lstlisting}


\paragraph{Profiler Visualization.} TODO
Building a visualization tool on top of a nccl profiler plugin is simple.
For the purpose providing helpful visualizations in this report, such a visualizer has been developed
The profiler plugin logs API calls and events as jsonl, which feed the visualization tool.

\paragraph{Tracking \& running metrics.} TODO
is this even worth talking about?
\begin{quote}
  Due to the asynchronous nature of NCCL operations, events associated with collectives and point-to-point are not easy to delimit precisely. 
  \texttt{stopEvent} for collectives simply indicates to the profiler that the collective has been enqueued. 
  Without proxy and/or kernel activity the plugin cannot determine when a collective ends. 
  With proxy/kernel events enabled, the plugin can estimate when it ends.
\end{quote}
(slightly rephrased from \textbf{/ext-profiler/README.md})


\paragraph{Kernel tracing with CUPTI.}
The CUPTI API provides functions to correlate application code regions with CUPTI activity
via \texttt{cuptiActivityPushExternalCorrelationId} and \texttt{cuptiActivityPopExternalCorrelationId}.
This can be handily integrated with the profiler API, whenever \texttt{KernelLaunch} events are started and stopped, 
while continuously incrementing the correlation id in a thread-safe manner.
Fig~\ref{fig:cupti-example} provides an example visualization of this method.
Cupti can be initialized and cleaned up within the profiler plugin's own \texttt{init} and \texttt{finalize} functions.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclgroup stress test/cupti example.png"}
  \caption{Illustration of CUPTI activity correlation with \texttt{KernelLaunch} events. Kernel activity is visualized as orange event bars.}
  \label{fig:cupti-example}
\end{figure}

\paragraph{Correlating Collective Events with \texttt{seqNumber}.}
When profiling is enabled, NCCL counts the number of calls for each type of collective function per communicator.

/src/include/comm.h
\begin{lstlisting}[language=C]
struct ncclComm {
  uint64_t seqNumber[NCCL_NUM_FUNCTIONS];
  /* other fields */
}
\end{lstlisting}

/src/plugin/profiler.cc
\begin{lstlisting}[language=C]
ncclResult_t ncclProfilerStartTaskEvents(struct ncclKernelPlan* plan) {
  /* other code */
  __atomic_fetch_add(&plan->comm->seqNumber[ct->func], 1, __ATOMIC_RELAXED);
  /* other code */
}
\end{lstlisting}

This value is present in the \texttt{eDescr} for collective events 
and can be used to identify which collectives operations belong together across processes (see Fig~\ref{fig:phase-4-3rd-iter}).

\paragraph{Tracing low level activity back to NCCL API calls with \texttt{parentObj}.}
If the plugin developer wants utilize this field, 
they should ensure that potential address reuse does not create ambiguity to what the parentObj was originally pointing to. 
Custom memory management is advised.
This field is useful when trying to understand which user API call triggered which events of lower level operations or activity such as network activity (see Fig~\ref{fig:phase-4-3rd-iter}).

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclgroup stress test/ncclgroup commA 1 collX + commB 1 collY/phase 4, 3rd iter.png"}
  \caption{An example illustrating how \texttt{parentObj} and \texttt{seqNumber} can aid in understanding nccl collective operations.}
  \label{fig:phase-4-3rd-iter}
\end{figure}

\paragraph{Process origin for profiler callbacks with PXN enabled.}
Unless Setting the environment variable \texttt{NCCL\_PXN\_DISABLE}=0 (default 1), due to PXN (PCIe x NVLink) some proxy ops may be progressed in a proxy thread from another process, different to the one that originally generated the operation. 
Then \texttt{parentObj} in \texttt{eDescr} is not safe to dereference; the \texttt{eDescr} for \texttt{ProxyOp} events includes the originator's PID, which the profiler can match against the local PID. 
The \texttt{eDescr} for \texttt{ProxyStep} does not provide this field. However a workaround is possible:

The passed \texttt{context} object in \texttt{startEvent} is also unsafe to dereference due to PXN. 
the profiler plugin developer may internally track initialized contexts 
and whether the passed \texttt{context} belongs to the local process. 
This is also indicative of PXN.

\paragraph{Tracking communicator parent--child relationships.}
With the current Profiler plugin API, it is not possible to detect whether a communicator originates from another one 
(e.g., via \texttt{ncclCommSplit} or \texttt{ncclCommShrink}).
The plugin's \texttt{init} callback only receives a single communicator ID (\texttt{commId},
which corresponds to \texttt{comm->commHash}), as well as \texttt{commName}, \texttt{nNodes}, \texttt{nRanks}, and \texttt{rank};
there is no \texttt{parentCommId} or similar argument.
In split/shrink, the \texttt{commHash} of the child node is calculated internally as a one-way digest of the \texttt{commHash} of the parent node and the split parameters (\texttt{splitCount}, \texttt{color}).
Therefore, the relationship cannot be restored based on the ID alone.

\subsection{Known limitations}

Kernel event instrumentation uses counters exposed by the kernel to the host and the proxy progress thread. 
Thus the proxy progress thread infrastructure is shared between network and profiler. 
If the proxy is serving network requests, reading kernel profiling data can be delayed, causing loss of accuracy. 
Similarly, under heavy CPU load and delayed scheduling of the proxy progress thread, accuracy can be lost.

From profiler version 4, NCCL uses a per-channel ring buffer of 64 elements. 
Each counter is complemented by two timestamps (ptimers) supplied by the NCCL kernel (start and stop of the operation in the kernel). 
NCCL propagates these timestamps to the profiler plugin so it can convert them to the CPU time domain.

(Source: \url{https://github.com/NVIDIA/nccl/tree/master/ext-profiler/README.md}

\subsection{Potential Integration with Score-P}

Score-P is a scalable measurement infrastructure for profiling,
event tracing, and online analysis of parallel HPC applications\footnote{\url{https://scorepci.pages.jsc.fz-juelich.de/scorep-pipelines/docs/scorep-6.0/html/}}.
It supports MPI, OpenMP, CUDA, and other paradigms, and produces OTF2 traces and CUBE4 profiles for tools such as Vampir and Scalasca.
Developers can extend Score-P via plugins.

\textbf{Substrate plugins} are loaded as \texttt{libscorep\_substrate\_\{name\}.so}
and register callbacks for Score-P's internal runtime events (region entries, MPI calls, etc.).
Substrate plugins are \emph{consumers} of events: they observe Score-P's event stream but do not inject new events.

In contrast, the NCCL profiler plugin is callback-driven \emph{by NCCL}.
NCCL loads it via \texttt{dlopen} and invokes \texttt{startEvent}, \texttt{stopEvent}, and \texttt{recordEventState}
during collective and P2P operations.

A potential integration strategy would involve writing a \textbf{dual-interface plugin:}
A developer writes a shared library that simultaneously implements the NCCL profiler API and integrates with Score-P as a substrate plugin.
Score-P loads the plugin as \texttt{libscorep\_substrate\_{\it nccl}.so}, 
while NCCL loads it via the environment variable \texttt{NCCL\_PROFILER\_PLUGIN}.
Although substrate plugins consume but do not produce events, the plugin could use Score-P's user instrumentation API
(e.g., \texttt{SCOREP\_USER\_REGION\_BY\_NAME\_BEGIN}/\texttt{END})
to inject Profiler Events (and by extension NCCL operations) as regions into Score-P's.
When NCCL calls \texttt{startEvent} or \texttt{stopEvent}, the plugin converts those callbacks into Score-P events.
The region name can be derived from the event descriptor (e.g., \texttt{collApi.func} for \texttt{ncclAllReduce}).
The application must be compiled with Score-P instrumentation and linked against the Score-P measurement library;
\texttt{NCCL\_PROFILER\_PLUGIN} points to the dual-interface library.

In this design, NCCL drives the profiler and the profiler forwards events into Score-P. 
NCCL collective operations then appear as regions in Score-P profiles and traces.

% ---------------------------------------------------------------------------
\section{Conclusion}
% ---------------------------------------------------------------------------

This feasibility study examined the NCCL Profiler Plugin API 
and its suitability for integration with Score-P.
The report provided background on NCCL and its design, 
explained how the profiler plugin is detected and loaded,
and described the API definition with its five core callbacks 
\texttt{init}, \texttt{startEvent}, \texttt{stopEvent}, \texttt{recordEventState}, \texttt{finalize}.
Code examples and visualizations illustrate the event flow from API calls to NCCL's internal profiler callbacks.
Performance experiments showed that an empty profiler adds roughly 8--9\,$\mu$s overhead per operation in single-node runs 
but introduces negligible overhead in multi-node runs, and scaling to many GPUs across nodes required no changes to the profiler plugin. 
The discussion covered developer considerations, known limitations, and a potential integration strategy with Score-P.

The NCCL Profiler API allows for highly customized plugins tailored to the analysis needs, 
whether for simple timing, kernel tracing via CUPTI, or integration with external tools such as Score-P. 
A notable advantage is its low overhead: NVIDIA advertises their \texttt{inspector} implementation (\textbf{/ext-profiler/inspector}) as efficient enough for ``always-on'' profiling in production. 
On the downside, profiler plugins may require maintenance and active development, since NCCL is actively developed. 
API versions evolve and new features are being introduced.

\iffalse
\subsection{NCCL\_DEBUG}

\url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-debug}

NCCL already comes with debug logging at various levels of granularity:
\begin{itemize}
  \item INFO -- debug information
  \item TRACE -- replayable trace information on every call
  \item Further options (v2.2.12 \texttt{NCCL\_DEBUG\_FILE}, v2.3.4 \texttt{NCCL\_DEBUG\_SUBSYS}, v2.26 timestamp format/levels)
  \item other profiling and tracing tools exists that are maintained by NVIDIA: nsight systems, nsight compute
\end{itemize}
\fi

\end{document}
