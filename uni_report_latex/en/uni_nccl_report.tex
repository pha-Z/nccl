% \documentclass[11pt]{article}
\documentclass[english,hauptseminar,hyperref]{zihpub}


\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% \usepackage[english,ngerman]{babel}
\usepackage{setspace}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{url}
\usepackage{color}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{grffile}
\usepackage{float}
\usepackage{lineno}

% Sequence diagrams
\usepackage{tikz}
\usepackage{pgf-umlsd}
\usepgflibrary{arrows.meta}

\lstset{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  breaklines=true,
  frame=single,
  framerule=0.3pt,
  rulecolor=\color{black!30},
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{black!60},
  stringstyle=\color{red!60!black}
}

% For ASCII/plaintext diagrams
\lstdefinestyle{ascii}{
  basicstyle=\ttfamily\footnotesize,
  columns=fixed,
  breaklines=true,
  frame=single,
  framerule=0.3pt
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.75\baselineskip}

\title{NCCL Profiler Plugin API -- A Feasibility Study}
\author{Alexander Moritz Van Le}
\matno{4607469}
\betreuer{Bert Wesarg}

\begin{document}
\linenumbers

\maketitle
\tableofcontents

% ---------------------------------------------------------------------------
\section{Abstract}
% ---------------------------------------------------------------------------

Artificial intelligence (AI) has established itself as a primary use case in high-performance computing (HPC) environments due to its compute-intensive and resource-intensive workloads. 
Analyzing and optimizing application performance is therefore essential to maximize efficiency and reduce costs. 
Many AI workloads involve communication between GPUs, often distributed across numerous GPUs in multi-node systems. 
The NVIDIA Collective Communication Library (NCCL) serves as the core library for implementing optimized communication primitives on NVIDIA GPUs. 
To provide detailed performance insights, NCCL offers a flexible profiler plugin API. 
This allows developers to directly integrate custom profiling tools into the library to extract detailed performance data on communication operations. 
This feasibility study explores the capabilities and integration mechanisms of the API.

First, this study provides background information on NCCL, 
followed by an explanation of the Profiler API accompanied with code examples and visualizations.
Next, considerations for developers of the Profiler API and its potential integration with Score-P is discussed. 
Finally, the study concludes with a summary of the findings.

% ---------------------------------------------------------------------------
\section{Introduction to NCCL}
% ---------------------------------------------------------------------------
NCCL was first introduced by NVIDIA in 2015 at the Supercomputing Conference\footnote{\url{https://images.nvidia.com/events/sc15/pdfs/NCCL-Woolley.pdf}}
with code being made available on GitHub\footnote{\url{https://github.com/NVIDIA/nccl}}.
The release of NCCL 2.0 in 2017 brought support for NVLink, however this was initially only available as pre-built binaries.
With the release of NCCL 2.3 in 2018, it returned to being fully open source.
The NCCL Profiler Plugin API was even later introduced with NCCL 2.23 in early 2025.

Before taking a closer look at the Profiler Plugin API, it is helpful to have some rudimentary understanding on certain designs in NCCL.

\subsection{Comparison to MPI}
Although NCCL is inspired by the Message Passing Interface (MPI) in terms of API design and usage patterns, there are notable differences due to their respective focuses:
\begin{itemize}
\item \textbf{MPI}: Communication is CPU-based. A rank corresponds to a single CPU process within a communicator.
\item \textbf{NCCL}: Communication is GPU-based, with CPU threads handling orchestration. A rank corresponds to a GPU device within a communicator; the mapping from ranks to devices is surjective. A single CPU thread can manage multiple ranks (i.e., multiple devices) in a communicator using the functions \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}. A CPU thread can also manage multiple ranks from different communicators (i.e same device alloted by multiple ranks from different communicators) through communicator creation with \texttt{ncclCommSplit} or \texttt{ncclCommShrink}. This means the mapping from ranks to threads is also surjective.
\end{itemize}

\subsection{Relevant NCCL internals}
It helps to understand what NCCL does internally when an application calls the NCCL User API.

A typical NCCL application follows this basic structure:

\begin{itemize}
  \item create nccl communicators
  \item allocate memory for computation and communication
  \item do computation and communication
  \item clean up nccl communicators
\end{itemize}

During NCCL communicator creation, NCCL internally spawns a thread called \texttt{ProxyService}. 
This thread lazily starts another thread called \texttt{ProxyProgress}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/proxy.cc}}, 
which handles network requests for GPU communication during collective and P2P operations.
See Fig.~\ref{fig:thread-creation}.
\iffalse
\begin{lstlisting}[style=ascii]
Thread Creation Flow
-------------------------------------------------------------------------------
User API                          Internal Flow
-------------------------------------------------------------------------------
ncclCommInitRank()         -+
ncclCommInitAll()           |
ncclCommInitRankConfig()    +---> ncclCommInitRankDev()  --+
ncclCommInitRankScalable() -+                              |
ncclCommSplit()            -+                              |
ncclCommShrink()            +---> ncclCommInitChildComm() -+
ncclCommGrow()             -+     +------------------------+
                            |     v
                            +---> ncclCommInitRankFunc()
                                  |
                                  v
                                  initTransportsRank()
                                  |
                                  +-> ncclProxyCreate(comm)
                                      |
                                      v
                                      if (proxyState->refCount == 1)
                                          pthread_create(&comm->proxyState->thread, NULL, ncclProxyService, ...)
                                          |
                                          v
                                          [New Thread] ncclProxyService()
                                          |
                                          +-> proxyProgressAsync(...)
                                          |   |
                                          |   +-> proxyConnInit(...)
                                          |       |
                                          |       +-> proxyProgressInit(proxyState)
                                          |           |
                                          |           +-> ncclProxyProgressCreate(proxyState)
                                          |               |
                                          |               +-> if (!state->thread)
                                          |                       pthread_create(&state->thread, NULL, ncclProxyProgress, ...)
                                          |                       |
                                          |                       v
                                          |                       [New Thread] ncclProxyProgress()
                                          |
                                          +---> proxyServiceInitOp(...)
                                                |
                                                +---> proxyProgressAsync(...)  (same path as above)
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
The ASCII diagram shows how user API calls (e.g.\ \texttt{ncclCommInitRank}, \texttt{ncclCommSplit}) funnel into two internal entry points (\texttt{ncclCommInitRankDev} or \texttt{ncclCommInitChildComm}), then into \texttt{ncclCommInitRankFunc()}, \texttt{initTransportsRank()}, and \texttt{ncclProxyCreate(comm)}. From there, NCCL conditionally spawns the \texttt{ProxyService} thread; that thread later lazily starts the \texttt{ProxyProgress} thread via \texttt{proxyProgressAsync} $\to$ \texttt{ncclProxyProgressCreate} $\to$ \texttt{pthread\_create(ncclProxyProgress)}.

\paragraph{Representation as a sequence diagram.}
We model four participants: the \emph{User} (application), \emph{NCCL} (internal call chain), and the two spawned threads \emph{ProxyService} and \emph{ProxyProgress}. The User invokes NCCL; NCCL performs internal calls (self-calls) and then creates the ProxyService thread; ProxyService creates the ProxyProgress thread. This makes the creation order and the two-level thread spawn explicit.
\fi

\begin{figure}[htbp]
\centering
\begin{sequencediagram}
\newthread{user}{User}
\newinst[2]{nccl}{NCCL init.cc}
\newinst[2]{proxySvc}{ProxyService}
\newinst[2]{proxyProg}{ProxyProgress}
\begin{call}{user}{initRank / split / \ldots}{nccl}{}
  \begin{callself}{nccl}{ncclCommInitRankFunc}{}
    \begin{callself}{nccl}{initTransportsRank}{}
      
      \begin{callself}{nccl}{ncclProfilerPluginInit}{}
      \end{callself}

      \begin{callself}{nccl}{ncclProxyCreate}{}
        
        \begin{sdblock}{\texttt{if (proxyState->refCount == 1)}}{}
          \begin{call}{nccl}{create}{proxySvc}{}
            
            % \begin{callself}{proxySvc}{proxyProgressAsync}{}
            % \begin{callself}{proxySvc}{ncclProxyProgressCreate}{}
            \begin{sdblock}{\texttt{if (!state->thread)}}{}
              \begin{call}{proxySvc}{create}{proxyProg}{}
              \end{call}
            \end{sdblock}
            % \end{callself}
            % \end{callself}
            
          \end{call}
        \end{sdblock}
      \end{callself}
    \end{callself}
  \end{callself}
\end{call}
\end{sequencediagram}
\caption{Thread creation: User API $\to$ NCCL internal init $\to$ create ProxyService $\to$ create ProxyProgress.}
\label{fig:thread-creation}
\end{figure}

\texttt{if}-guards ensure that these threads are created once per \texttt{ncclSharedResources}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/include/comm.h}}.
\iffalse
\textbf{/src/include/comm.h}
\begin{lstlisting}[language=C]
struct ncclSharedResources {
  struct ncclComm* owner; /* communicator which creates this shared res. */
  struct ncclProxyState* proxyState;
  // other fields
}
\end{lstlisting}

\textbf{/src/include/proxy.h}
\begin{lstlisting}[language=C]
struct ncclProxyState {
  int refCount;
  pthread_t thread;
  ncclProxyProgressState progressState;
  // other fields
}

struct ncclProxyProgressState {
  struct ncclProxyOpsPool* opsPool;
  // other fields
}

struct ncclProxyOpsPool {
  struct ncclProxyOp ops[MAX_OPS_PER_PEER*NCCL_MAX_LOCAL_RANKS];
  // other fields
}
  
struct ncclProxyOps {
  // other fields
}
\end{lstlisting}
\fi
By default every NCCL communicator has its own shared resource. 
When the application calls \texttt{ncclCommSplit} or \texttt{ncclCommShrink}, 
where the original communicator was initialized with a \\\texttt{ncclConfig\_t} with fields \texttt{splitShare} or \texttt{shrinkShare} set to \texttt{1}, 
the newly created communicator uses the same shared resource (and the proxy threads) as the parent communicator.

\iffalse
\small
\begin{quote}
\texttt{/* proxyState is shared among parent comm and split comms}\\
\texttt{comm->proxyState->thread is pthread\_join()'d by commFree() in init.cc }\\
\texttt{when the refCount reduces down to 0. */}
\end{quote}
(Quoted from \textbf{/src/proxy.cc})
\normalsize
\fi

Later, whenever the application calls the NCCL User API, 
NCCL internally decides what network operations to perform and calls \texttt{ncclProxyPost} to post them to a proxyOpsPool
(See Fig.~\ref{fig:proxy-post}).

\iffalse
\begin{lstlisting}[style=ascii]
Flow from User API Calls to ncclProxyPost()
-------------------------------------------------------------------------------
User API
---------------------------------------------------------------------------------
ncclCommInitAll()          -+    ncclAllGather()      -+
ncclCommInitRankConfig()    |    ncclAlltoAll()        |
ncclCommInitRankScalable()  |    ncclAllReduce()       |
ncclCommFinalize()          |    ncclBroadcast()       |
ncclCommDestroy()           |    ncclGather()          |
ncclCommRevoke()            |    ncclReduce()          |
ncclCommAbort()             |    ncclReduceScatter()   |
ncclCommSplit()             |    ncclScatter()         |
ncclCommShrink()            |    ncclSend()            |
ncclCommGrow()              |    ncclRecv()           -+
ncclDevCommCreate()         |                          |
ncclCommWindowRegister()    |                          |
ncclGroupSimulateEnd()     -+                          |
-------------------------------------------------------------------------------
Internal Flow
-------------------------------------------------------------------------------
                            |                          |
                            |                          v
                            |                   ncclEnqueueCheck()
                            +--------------------------+
                            v
                            ncclGroupEndInternal()
                            +----+
                            |    v
                            |    groupLaunchNonBlocking()
                            +----+
                            v
                            groupLaunch()
                            |
                            v
                            doLaunches()
                            +-> ncclLaunchPrepare() ------------+
                            +-> ncclLaunchKernelAfter_NoCuda()  v
                                |                               ncclLaunchPrepare()
                                |                               |
                                |                               v
                                |                               cudaLaunchHostFunc()
                                |                               |
                                |                               v
                                |                               hostStreamPlanCallback()
                                +-------------------------------+
                                v
                                hostStreamPlanTask()
                                +-> uploadProxyOps() -+
                                +-> ncclProxyStart()  v
                                    |                 ncclProxySaveOp()
                                    |                 |
                                    |                 v
                                    |                 SaveProxy()
                                    |                 |
                                    |                 v
                                    |                 ncclLocalOpAppend()
                                    +-----------------+
                                    v
                                    ncclProxyPost() (proxy.cc)
                                    +-> [Posts Ops to pool]
                                    +-> [Signals Proxy Progress Thread]
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
Two families of user API calls lead to \texttt{ncclProxyPost()}: (1)~communicator lifecycle and group operations (\texttt{ncclCommInitAll}, \texttt{ncclCommSplit}, \texttt{ncclGroupSimulateEnd}, etc.) and (2)~collectives and P2P (\texttt{ncclAllReduce}, \texttt{ncclSend}, etc.). Both converge at \texttt{ncclGroupEndInternal()}, then \texttt{groupLaunch()} $\to$ \texttt{doLaunches()}. From there, kernel launch and host-callback paths lead to \texttt{hostStreamPlanTask()}, which calls \texttt{uploadProxyOps()} / \texttt{ncclProxyStart()} $\to$ \texttt{SaveProxy()} $\to$ \texttt{ncclProxyPost()}, where ops are posted to the pool and the ProxyProgress thread is signalled.

\paragraph{Representation as a sequence diagram.}
We show the \emph{User} invoking \emph{NCCL} with either a collective/P2P call or a comm/group call; NCCL's internal flow is collapsed into a chain of self-calls that merge at \texttt{ncclGroupEndInternal()} and end at \texttt{ncclProxyPost()}. The diagram emphasizes the single internal path after the merge and the role of \texttt{ncclProxyPost()} as the point where ops are posted and the proxy thread is woken.
\fi

\begin{figure}[htbp]
\centering
\begin{sequencediagram}
\newthread{user}{User}
\newinst[2]{nccl}{NCCL}
% \newinst[2]{cuda}{CUDA host stream}
\newinst[2]{proxy}{ProxyProgress}
\newinst[2]{opspool}{ncclProxyOpsPool}
\begin{call}{user}{User API (collective / comm / group)}{nccl}{}

  \begin{callself}{nccl}{ncclGroupEndInternal}{}
    \begin{callself}{nccl}{groupLaunch}{}
      \begin{callself}{nccl}{doLaunches}{}

        % \begin{callself}{nccl}{ncclLaunchPrepare}{}
        %   \begin{call}{nccl}{cudaLaunchHostFunc(hostStream, hostStreamPlanCallback)}{cuda}{}
        %   \end{call}
        % \end{callself}

        \begin{callself}{nccl}{hostStreamPlanTask}{}
          \begin{call}{nccl}{ncclProxyPost}{opspool}{}
          \end{call}
          \begin{call}{nccl}{signal}{proxy}{}
          \end{call}
        \end{callself}

      \end{callself}
    \end{callself}
  \end{callself}

\end{call}

% \begin{callself}{cuda}{hostStreamPlanCallback}{}
%   \begin{callself}{cuda}{hostStreamPlanTask}{}
%     \begin{call}{cuda}{ncclProxyPost}{opspool}{}
%     \end{call}
%     \begin{call}{cuda}{signal}{proxy}{}
%     \end{call}
%   \end{callself}
% \end{callself}

\end{sequencediagram}%
\caption{Flow from User API to \texttt{ncclProxyPost}}
\label{fig:proxy-post}
\end{figure}

The ProxyProgress thread reads from this pool when calling \texttt{ncclProxyGetPostedOps} and progresses the ops.
See Fig.~\ref{fig:proxy-progress-loop}.

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
ncclProxyProgress() progressing loop
-------------------------------------------------------------------------------
ncclProxyProgress(proxyState)
|
+-> do {
        +-> progressOps(proxyState, ...)
        |   |
        |   +-> while (op) {
        |           op->progress(proxyState, op);
        |           op = op->next;
        |       }
        |
        +-> ncclProxyGetPostedOps()
            |
            +-> [reads Ops or thread will wait]
    } while (...)
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
The ProxyProgress thread runs a loop: in each iteration it first calls \texttt{progressOps()}, which walks the list of ops and invokes \texttt{op->progress(proxyState, op)} for each; then it calls \texttt{ncclProxyGetPostedOps()}, which either reads newly posted ops from the pool or blocks until the main path signals it. The loop thus alternates between making progress on existing ops and acquiring new work.

\paragraph{Representation as a sequence diagram.}
We use a single participant (the ProxyProgress thread) and a loop block. Inside the loop we show two self-calls: \texttt{progressOps()} (with an inner loop over ops) and \texttt{ncclProxyGetPostedOps()}. This makes the recurring cycle and the two phases (progress vs.\ get posted ops) explicit.
\fi

\begin{figure}[htbp]
\centering
\begin{sequencediagram}
\newthread{proxy}{ProxyProgress}
\newinst[5]{opspool}{ncclProxyOpsPool}
\begin{sdblock}{loop: do \ldots while}{}
  
  \begin{callself}{proxy}{progressOps(proxyState)}{}
  \end{callself}
  
  \begin{call}{proxy}{ncclProxyGetPostedOps [waits if no ops]}{opspool}{proxyState}
  \end{call}

\end{sdblock}
\end{sequencediagram}%
\caption{/src/proxy.cc \texttt{ncclProxyProgress} progressing loop: progress ops, then get posted ops (or wait). }
\label{fig:proxy-progress-loop}
\end{figure}

Familiarity with this network activity pattern will aid in understanding
the Profiler Plugin API's behavior discussed in the following section.

% ---------------------------------------------------------------------------
\section{Profiler Plugin}
% ---------------------------------------------------------------------------

Whenever a communicator is created, NCCL looks for the existence of a profiler plugin
and loads it if it has not already been loaded on the process.
NCCL then initializes the plugin with the created communicator.
Whenever the application makes calls to the Collectives or P2p API (e.g. \texttt{ncclAllReduce}) with that communicator,
NCCL calls the profiler API in different regions of the internal code.
When the communicator is destroyed, the profiler plugin is unloaded if this was the only communicator on the process.


\subsection{Profiler plugin loading mechanism}

Each time a NCCL communicator is created, \texttt{ncclProfilerPluginLoad}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/plugin/profiler.cc}} is called, 
where NCCL looks for a shared library that represents the profiler plugin by checking an environment variable. 
It then calls \texttt{dlopen}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/plugin/plugin_open.cc}} and \texttt{dlsym} to load the library immediately with local symbol visibility:
\begin{lstlisting}[language=C]
profilerName = ncclGetEnv("NCCL_PROFILER_PLUGIN");
// ...
handle* = dlopen(name, RTLD_NOW | RTLD_LOCAL);
// ...
ncclProfiler_v5 = (ncclProfiler_v5_t*)dlsym(handle, "ncclProfiler_v5");
\end{lstlisting}
If the library has already been loaded on the process, this procedure is skipped. \\
A \texttt{profilerPluginRefCount} keeps track of the number of calls to this procedure to ensure correct unloading during finalization.
See Fig.~\ref{fig:profiler-init}. The NCCL documentation\footnote{\url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html\#nccl-profiler-plugin}} also describes some further loading logic:

\begin{quote}
\begin{itemize}
  \item If \texttt{NCCL\_PROFILER\_PLUGIN} is set: attempt to load the library with the specified name; \\if that fails, attempt \texttt{libnccl-profiler-<NCCL\_PROFILER\_PLUGIN>.so}.
  \item If \texttt{NCCL\_PROFILER\_PLUGIN} is not set: attempt \texttt{libnccl-profiler.so}.
  \item If no plugin was found: profiling is disabled.
  \item If \texttt{NCCL\_PROFILER\_PLUGIN} is set to \texttt{STATIC\_PLUGIN}, the plugin symbols are searched in the program binary.
\end{itemize}
\end{quote}


The plugin loading mechanism expects the struct variable name to follow the naming convention\\ 
\texttt{ncclProfiler\_v\{versionNum\}}, which also indicates the API version.

The profiler API has changed multiple times with newer NCCL releases. 
NCCL features a fallback mechanism to load older struct versions.
However one instance is known, where a profiler plugin being developed against the NCCL release 2.25.1 with Profiler API version 2, was unable to run with the latest NCCL release\footnote{\url{https://github.com/variemai/ncclsee}}.
Around this time, the NCCL repository has undergone a refactor related to the profiler plugin.
% NCCL developers may be required to actively maintain older API versions, to ensure they safely work when old behaviour is getting deprecated 
% and do not unexpectedly get handed new features from new API versions, 
% of which the an older Profiler Plugin is not aware of (when faithfully implementing the old API version).

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
User API                          Internal Flow
-------------------------------------------------------------------------------
ncclCommInitRank()         --+
ncclCommInitAll()            |
ncclCommInitRankConfig()     +--> ncclCommInitRankDev()  --+
ncclCommInitRankScalable() --+                             |
                                                           |
ncclCommSplit()            --+                             |
ncclCommShrink()           --+--> ncclCommInitChildComm() -+
                                                           |
ncclCommGrow()             --+    +------------------------+
                             |    v
                             +--> ncclCommInitRankFunc()
                                  |
                                  v                        
                                  initTransportsRank()
                                  +-> ncclProfilerPluginInit(comm)-+
                                  +-> ncclProxyCreate(comm)        |
                                      |                            v
                                      v                            ncclProfilerPluginInit(comm)
                                      ...                          +-> ncclProfilerPluginLoad()
                                                                   |   +-> ncclGetEnv("NCCL_PROFILER_PLUGIN")
                                                                   |   +-> ncclOpenProfilerPluginLib()
                                                                   |   |   +-> dlopen(name, RTLD_NOW | RTLD_LOCAL)
                                                                   |   +-> getNcclProfiler_v5()
                                                                   |   |   +-> (ncclProfiler_v5_t*)dlsym(lib, "ncclProfiler_v5");
                                                                   |   +-> [try fallback to older api version]
                                                                   +-> profiler->init()

\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
User API calls for communicator creation (\texttt{ncclCommInitRank}, \texttt{ncclCommSplit}, etc.) enter NCCL via \texttt{ncclCommInitRankDev()} or \texttt{ncclCommInitChildComm()}, then \texttt{ncclCommInitRankFunc()} $\to$ \texttt{initTransportsRank()}. There, \texttt{ncclProfilerPluginInit(comm)} runs in parallel with \texttt{ncclProxyCreate(comm)}. The profiler path calls \texttt{ncclProfilerPluginLoad()} (get env, \texttt{dlopen}, \texttt{dlsym} for \texttt{ncclProfiler\_v5}), then \texttt{profiler->init()}.

\paragraph{Representation as a sequence diagram.}
We show three participants: \emph{User}, \emph{NCCL}, and \emph{Profiler plugin}. The User invokes NCCL; NCCL performs internal init and then loads and initializes the plugin (load library, get symbol, call \texttt{init}). The diagram makes it clear that the plugin is loaded and \texttt{init()} is called during communicator creation, before proxy threads are created.
\fi

\begin{figure}[H]
\centering
\scalebox{0.95}{%
\begin{sequencediagram}
\newthread{user}{User}
\newinst[1.7]{nccl}{NCCL init.cc}
% \newinst[1]{profilercc}{NCCL profiler.cc}
\newinst[2.7]{proxy}{ProxyProgress}
\newinst[1]{plug}{Profiler Plugin}
\begin{call}{user}{initRank / split / \ldots}{nccl}{}
  \begin{callself}{nccl}{ncclCommInitRankFunc}{}
    \begin{callself}{nccl}{initTransportsRank}{}
      
      \begin{call}{nccl}{ncclProfilerPluginInit}{nccl}{}
     
        \begin{callself}{nccl}{ncclProfilerPluginLoad}{}

          \begin{sdblock}{\footnotesize\texttt{if (profilerPluginStatus != profilerPluginLoadSuccess)}}{}
            \begin{callself}{nccl}{\texttt{ncclGetEnv}}{}
            \end{callself}
            \begin{callself}{nccl}{ncclOpenProfilerPluginLib}{}
              \begin{callself}{nccl}{\texttt{dlopen}}{}
              \end{callself}
            \end{callself}
            \begin{callself}{nccl}{getNcclProfiler\_v5}{}
              \begin{callself}{nccl}{\texttt{dlsym}}{}
              \end{callself}
            \end{callself}
          \end{sdblock}

          \begin{callself}{nccl}{\texttt{++profilerPluginRefCount;}}{}
          \end{callself}
          
        \end{callself}
        
        \begin{call}{nccl}{\texttt{ncclProfiler->init(commId, \ldots)}}{plug}{}
        \end{call}
     
      \end{call}
      
      \begin{call}{nccl}{ncclProxyCreate}{proxy}{}
      \end{call}
      
    \end{callself}
  \end{callself}
\end{call}
\end{sequencediagram}%
}
\caption{User API $\to$ NCCL communicator init $\to$ load profiler plugin and call \texttt{profiler->init}.}
\label{fig:profiler-init}
\end{figure}

\subsection{Profiler API}

The plugin must implement a profiler API specified by NCCL by exposing a struct\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/include/plugin/profiler/profiler_v5.h}}. 
This struct should contain pointers to all functions required by the API. 
A plugin may expose multiple versioned structs for backwards compatibility with older NCCL versions.

\begin{lstlisting}[language=C]
ncclProfiler_v5_t ncclProfiler_v5 = {
  const char* name;
  ncclResult_t (*init)(...);              // called when a communicator is created
  ncclResult_t (*startEvent)(...);        // at start of operations/activities
  ncclResult_t (*stopEvent)(...);         // at end of these operations/activities
  ncclResult_t (*recordEventState)(...);  // to record state of certain operations
  ncclResult_t (*finalize)(...);          // called when a communicator is destroyed
};
\end{lstlisting}

As of NCCL v2.29.2, version 6 is the latest, which was released on on Dec 24, 2025. 
This release happened well after the begin of the study, so the focus will be on version 5.
Version 6 introduced additional profiler API callbacks for Copy-Engine based collective operations, 
otherwise version 6 and version 5 remain the same.

Five functions must be implemented for the API. 
Internally NCCL wraps calls to the profiler API in custom functions which are all declared in a single file\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/include/profiler.h}}.

NCCL invokes the profiler API at different levels to capture start/stop of NCCL groups, collectives, P2P, proxy, kernel and network activity. 
As the API function names suggest, this will allow the profiler to track these operations and activities as events.

The API functions and where NCCL invokes them are explained in the following sections.

\subsubsection{init}

\texttt{init} initializes the profiler plugin with a communicator. 
\texttt{init} is called immediately after \\\texttt{ncclProfilerPluginLoad}, 
which happens every time a communicator is created 
(see Fig.~\ref{fig:profiler-init}). 
This may happen multiple times for the same profiler instance, if further communicators are created on that process. 
NCCL passes follwing arguments:

\begin{lstlisting}[language=C]
ncclResult_t init(
  void** context,          // out param - opaque profiler context
  uint64_t commId,         // communicator id
  int* eActivationMask,    // out param - bitmask for which events are tracked
  const char* commName,    // user assigned communicator name
  int nNodes,              // number of nodes in communicator
  int nranks,              // number of ranks in communicator
  int rank,                // rank identifier in communicator
  ncclDebugLogger_t logfn  // logger function
);
\end{lstlisting}

If the profiler plugin \texttt{init} function does not return \texttt{ncclSuccess}, NCCL disables the plugin.

\texttt{void** context} is an opaque handle that the plugin developer may point to any custom context object; this pointer is passed again in \texttt{startEvent} and \texttt{finalize}. 
This context object is separate per communicator.

The plugin developer should set \texttt{int* eActivationMask} to a bitmask\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/include/plugin/nccl_profiler.h}}, 
indicating which event types the profiler wants to track:
\begin{lstlisting}[language=C]
enum {
  ncclProfileGroup          = (1 << 0),  // group event type
  ncclProfileColl           = (1 << 1),  // host collective call event type
  ncclProfileP2p            = (1 << 2),  // host point-to-point call event type
  ncclProfileProxyOp        = (1 << 3),  // proxy operation event type
  ncclProfileProxyStep      = (1 << 4),  // proxy step event type
  ncclProfileProxyCtrl      = (1 << 5),  // proxy control event type
  ncclProfileKernelCh       = (1 << 6),  // kernel channel event type
  ncclProfileNetPlugin      = (1 << 7),  // network plugin-defined, events
  ncclProfileGroupApi       = (1 << 8),  // Group API events
  ncclProfileCollApi        = (1 << 9),  // Collective API events
  ncclProfileP2pApi         = (1 << 10), // Point-to-Point API events
  ncclProfileKernelLaunch   = (1 << 11), // Kernel launch events
};
\end{lstlisting}
The default value is to 0, which means no events are tracked by the profiler. 
Setting it to 4095 will track all events.

\texttt{ncclDebugLogger\_t logfn} is a function pointer to NCCL's internal debug logger \\(\texttt{ncclDebugLog}). 
NCCL passes this so the plugin can emit log lines through the same channel and filtering as NCCL: 
the plugin may store the callback and call it with \texttt{(level, flags, file, line, fmt, ...)} when it wants to log. 
Messages then appear in NCCL's debug output (e.g.\ stderr or \texttt{NCCL\_DEBUG\_FILE}) and respect the user's \texttt{NCCL\_DEBUG} level and subsystem mask. 
Using \texttt{logfn} keeps profiler output consistent with NCCL's own logs.

\subsubsection{startEvent}

\texttt{startEvent} is called when NCCL begins certain operations:

\begin{lstlisting}[language=C]
ncclResult_t startEvent(
  void* context,                       // opaque profiler context object
  void** eHandle,                      // out param - event handle
  ncclProfilerEventDescr_v5_t* eDescr  // pointer to event descriptor
);
\end{lstlisting}

As of release v2.29.2 NCCL does not use the return value. 
\texttt{void** eHandle} may point to a custom event object; this pointer is passed again in \texttt{stopEvent} and \texttt{recordEventState}. 
\texttt{eDescr}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/include/plugin/profiler/profiler_v5.h}} describes the started event. 

The field \texttt{void* parentObj} in the event descriptor is the \texttt{eHandle} of a parent event (or null).
The use of this field can be explained as following:

All User API calls to Collective or P2P operations will start a Group API event.
When networking is required, ProxyCtrl Events may be emitted. 
Depending on the \texttt{eActivationMask} bitmask returned in the \texttt{init} function, 
further (child) events will be emitted in deeper regions of the nccl code base. 
It can be thought of as an event hierarchy\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/ext-profiler/README.md}} with several depth levels:
\begin{quote}
\begin{lstlisting}[style=ascii]
Group API event
    |
    +- Collective API event
    |  |
    |  +- Collective event
    |     |
    |     +- ProxyOp event
    |     |  |
    |     |  +- ProxyStep event
    |     |     |
    |     |     +- NetPlugin event
    |     |
    |     +- KernelCh event
    |
    +- Point-to-point API event
    |  |
    |  +- Point-to-point event
    |     |
    |     +- ProxyOp event
    |     |  |
    |     |  +- ProxyStep event
    |     |     |
    |     |     +- NetPlugin event
    |     |
    |     +- KernelCh event
    |
    +- Kernel Launch event

ProxyCtrl event
\end{lstlisting}
\end{quote}

The \texttt{parentObj} inside \texttt{eDescr} will be a reference to the \texttt{eHandle} of the respective parent event for the current event according to this hierarchy. 
Thus, if the \texttt{eActivationMask} set during \texttt{init} enables tracking for event types lower in the hierarchy, NCCL always also tracks their parent event types.

\subsubsection{stopEvent}

\begin{lstlisting}[language=C]
ncclResult_t stopEvent(void* eHandle);  // handle to event object
\end{lstlisting}
\texttt{stopEvent} tells the plugin that the event has stopped. 
\texttt{stopEvent} for collectives simply indicates to the profiler that the collective has been enqueued 
and not that the collective has been completed.

As of NCCL v2.29.2 NCCL does not use the return value. 

\texttt{stopEvent} is called in the same functions that call \texttt{startEvent}, except for the GroupApi event.
Fig.~\ref{fig:profiler-events} shows when NCCL emits \texttt{startEvent} and \texttt{stopEvent} after a user API call.
The Proxy\-Progress thread also emits \texttt{startEvent} and \texttt{stopEvent} while progressing ops (see Fig.~\ref{fig:proxy-event-emission}).

\iffalse
\begin{lstlisting}[style=ascii]
Flow from NCCL API Calls to profiler events
---------------------------------------------------------------------------------
User API
---------------------------------------------------------------------------------
ncclCommInitAll()          -+    ncclAllGather()      -+      
ncclCommInitRankConfig()    |    ncclAlltoAll()        |      
ncclCommInitRankScalable()  |    ncclAllReduce()       |      
ncclCommFinalize()          |    ncclBroadcast()       |      
ncclCommDestroy()           |    ncclGather()          |      
ncclCommRevoke()            |    ncclReduce()          |      
ncclCommAbort()             |    ncclReduceScatter()   |      
ncclCommSplit()             |    ncclScatter()         |      
ncclCommShrink()            |    ncclSend()            |      
ncclCommGrow()              |    ncclRecv()           -+      
ncclDevCommCreate()         |                          |      
ncclCommWindowRegister()    |                          |
ncclGroupSimulateEnd()     -+                          |
                            |                          | 
---------------------------------------------------------------------------------
Internal Flow
---------------------------------------------------------------------------------
                            |                          | 
                            |                          v      
                            |                          ncclEnqueueCheck()
                            |                          +-> taskAppend()
                            +--------------------------+   +-> collTaskAppend()
                            |                              |   +-> Emits: GroupApi Event (start), CollApi Event
                            |                              +-> p2pTaskAppend()
                            v                                  +-> Emits: GroupApi Event (start), P2pApi Event
                            ncclGroupEndInternal()
                            +-> Emits: GroupApi Event (stop, at end of function call if eHandle exists)
                            +-------+
                            |       v
                            |       groupLaunchNonBlocking()
                            +-------+
                            v
                            groupLaunch()
                            |
                            v
                            doLaunches()
                            +-> ncclLaunchPrepare() ------------+
                            +-> ncclLaunchKernel()              |
                            |   +-> Emits: KernelLaunch Event   |
                            +-> ncclLaunchKernelAfter_NoCuda()  v
                                |                               ncclLaunchPrepare()
                                |                               |
                                |                               v
                                |                               cudaLaunchHostFunc()
                                |                               |
                                |                               v
                                |                               hostStreamPlanCallback()
                                +-------------------------------+
                                v
                                hostStreamPlanTask()
                                +-> Emits: Group Event, Coll Event, P2p Event
                                +-> uploadProxyOps() -+
                                +-> ncclProxyStart()  v
                                    |                 ...
                                    v
                                    ...
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
The same two families of user API calls (comm/group vs.\ collectives/P2P) lead to profiler events. For collectives/P2P, \texttt{ncclEnqueueCheck()} $\to$ \texttt{taskAppend()} emits GroupApi (start) and CollApi or P2pApi (start). At \texttt{ncclGroupEndInternal()} NCCL emits GroupApi (stop). In \texttt{doLaunches()}, \texttt{ncclLaunchKernel()} emits KernelLaunch; \texttt{hostStreamPlanTask()} emits Group, Coll, P2p events; then \texttt{uploadProxyOps()} / \texttt{ncclProxyStart()} continue the path. The diagram shows where \texttt{startEvent} and \texttt{stopEvent} are invoked relative to the internal call flow.

\paragraph{Representation as a sequence diagram.}
We show \emph{User}, \emph{NCCL}, and \emph{Profiler plugin}. The User calls NCCL; NCCL performs internal steps and at specific points calls the plugin (\texttt{startEvent} / \texttt{stopEvent}). The sequence diagram makes the order of API calls and the injection points of profiler callbacks explicit.
\fi

\begin{figure}[H]
\centering
\resizebox{!}{18.5cm}{%
\begin{sequencediagram}
\newthread{user}{User}
\newinst[3]{nccl}{NCCL}
% \newinst[1.7]{cuda}{CUDA host stream}
\newinst[1]{proxy}{ProxyProgress}
\newinst[1]{opspool}{ncclProxyOpsPool}
\newinst[1]{plug}{Profiler plugin}

\begin{call}{user}{Collective / P2P API call}{nccl}{}
  
  \begin{callself}{nccl}{ncclEnqueueCheck}{}

    \begin{callself}{nccl}{taskAppend}{}
      \begin{call}{nccl}{startEvent (GroupApi)}{plug}{}
      \end{call}
      \begin{call}{nccl}{startEvent \& stopEvent (CollApi / P2pApi)}{plug}{}
      \end{call}
    \end{callself}
    
    \begin{call}{nccl}{ncclGroupEndInternal}{nccl}{}
      
      \begin{callself}{nccl}{groupLaunch}{}
        \begin{callself}{nccl}{doLaunches}{}

          % \begin{callself}{nccl}{ncclLaunchPrepare}{}
          %   \begin{call}{nccl}{hostStreamPlanCallback}{nccl}{}
          %   \end{call}
          % \end{callself}

          \begin{callself}{nccl}{ncclLaunchKernel}{}
            \begin{call}{nccl}{startEvent \& stopEvent (KernelLaunch)}{plug}{}
            \end{call}
          \end{callself}

          % \begin{callself}{nccl}{hostStreamPlanCallback}{}
          \begin{callself}{nccl}{hostStreamPlanTask}{}
            \begin{call}{nccl}{startEvent (Group)}{plug}{}
            \end{call}
            \begin{call}{nccl}{startEvent (Coll / P2p)}{plug}{}
            \end{call}
            
            \begin{call}{nccl}{ncclProxyPost}{opspool}{}
            \end{call}
            \begin{call}{nccl}{signal}{proxy}{}
            \end{call}
    
            \begin{call}{nccl}{StopEvent (Coll / P2p)}{plug}{}
            \end{call}
            \begin{call}{nccl}{stopEvent (Group)}{plug}{}
            \end{call}
          \end{callself}
          % \end{callself}
        
        \end{callself}
      \end{callself}

      \begin{call}{nccl}{stopEvent (GroupApi)}{plug}{}
      \end{call}    

    \end{call}
  
  \end{callself}
  
\end{call}
\end{sequencediagram}%
}
\caption{Flow from NCCL API calls to profiler events. 
In case of \texttt{ncclGroupStart / ncclGroupEnd}. 
multiple events of everything (except GroupApi) are called. 
Internally, some Collectives (e.g. ncclAlltoAll) are implemented as multiple p2p ops, triggering many P2pApi and P2p events.}
\label{fig:profiler-events}
\end{figure}


\iffalse
NCCL only uses the Cuda callback, if necessary for proxy args. 
Otherwise \texttt{hostStreamPlanTask} is called inside 
\texttt{doLaunches} wrapped inside \texttt{ncclLaunchKernelAfter\_NoCuda}:

\begin{quote}
\textbf{/src/enqueue.cc}
\begin{lstlisting}[language=C]
// We have to launch host tasks to push proxy args. We are careful to only
// do this if necessary since host tasks impose a high performance cost in CUDA.
if (plan->hasProxyOps) {
  // ...
  CUDACHECKGOTO(cudaLaunchHostFunc(hostStream, hostStreamPlanCallback, plan), result, failure);
}

// ...

ncclResult_t ncclLaunchKernelAfter_NoCuda(struct ncclComm* comm, struct ncclKernelPlan* plan) {
  if (!plan->isHostCbEnq) {
    // we are not using the host stream for proxy ops and reclaimation submission, call
    // hostStreamPlanTask directly
    NCCLCHECK(hostStreamPlanTask(comm, plan));
  }
  return ncclSuccess;
}
\end{lstlisting}
\end{quote}
\fi

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
ncclProxyProgress() Event Emission Flow
---------------------------------------------------------------------------------
ncclProxyProgress(proxyState) [Proxy Progress Thread Loop]
|
+-> do {
         |
         +-> progressOps(proxyState, opStart=state->active, ...)
         |   |
         |   +-> while (op) {
         |            op->progress(proxyState, op);
         |            |
         |            +-> [Transport-specific progress function]
         |                (net.cc, coll_net.cc, p2p.cc, shm.cc)
         |                | 
         |                +-> Emits: ProxyStep events
         |                +-> Emits: KernelCh events
         |                +-> Emits: Network plugin specific events
         |            op = op->next;
         |       }
         |
         +-> [Thread Idle/Active State Transitions]
         |   +-> Emits: ProxyCtrl events
         |
         +-> ncclProxyGetPostedOps(proxyState)
             +-> [Thread Sleep/Wakeup State Transitions]
             |     +-> [Thread sleeps, waits for signal]
             |     +-> Emits: ProxyCtrl events
             +-> [Update Op pool] 
             +-> Emits: ProxyCtrl events
             +-> ProxyAppend()
                 +-> ncclProxyOpToArgs()
                     +-> Emits: ProxyOp events
    } while (...)
\end{lstlisting}
\fi
\iffalse
\paragraph{What the diagram conveys.}
Inside the ProxyProgress thread loop, \texttt{progressOps()} invokes transport-specific progress (\texttt{op->progress}); those functions emit ProxyStep, KernelCh, and NetPlugin events. Thread idle/active and sleep/wakeup transitions emit ProxyCtrl events. When \texttt{ncclProxyGetPostedOps()} runs, it may emit ProxyCtrl events; \texttt{ProxyAppend()} / \texttt{ncclProxyOpToArgs()} emit ProxyOp events.

\paragraph{Representation as a sequence diagram.}
We use one participant (ProxyProgress thread) and a loop block. Inside the loop we show: \texttt{progressOps()} (with transport progress and event emission to the plugin), thread state transitions (ProxyCtrl), \texttt{ncclProxyGetPostedOps()} (ProxyCtrl), and \texttt{ProxyAppend()} (ProxyOp). The plugin is shown as a second participant that receives the emitted events.
\fi

\begin{figure}[H]
\centering
\resizebox{!}{13.5cm}{%
\begin{sequencediagram}
\newthread{proxy}{ProxyProgress}
\newinst[4]{opspool}{ncclProxyOpsPool}
\newinst[2]{plug}{Profiler plugin}
\begin{sdblock}{loop: do \ldots while}{}

  \begin{callself}{proxy}{progressOps}{}
    \begin{callself}{proxy}{\texttt{op->progress()} [transport specific progress]}{}
      \begin{call}{proxy}{startEvent \& stopEvent (ProxyStep, KernelCh, NetPlugin)}{plug}{}
      \end{call}
    \end{callself}
  \end{callself}
  \begin{call}{proxy}{startEvent \& stopEvent (ProxyCtrl)}{plug}{}
  \end{call}
  \begin{callself}{proxy}{ncclProxyGetPostedOps}{}
    \begin{call}{proxy}{startEvent \& StopEvent (ProxyCtrl)}{plug}{}
    \end{call}

    \begin{call}{proxy}{get next ops / wait if no ops}{opspool}{}
    \end{call}

    \begin{callself}{proxy}{ProxyAppend}{}
      \begin{callself}{proxy}{ncclProxyOpToArgs}{}
        \begin{call}{proxy}{startEvent \& stopEvent (ProxyOp)}{plug}{}
        \end{call}
      \end{callself}
    \end{callself}
  \end{callself}

\end{sdblock}
\end{sequencediagram}%
}
\caption{\texttt{ncclProxyProgress}: progressOps emits ProxyStep/KernelCh/NetPlugin events. getPostedOps emits ProxyOp events. Several events ProxyCtrl are also emitted}
\label{fig:proxy-event-emission}
\end{figure}

\texttt{op->progress()} progresses transport specific ops. 
This is implemented as a function pointer type\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/include/proxy.h}}.
Confusingly the variable is called `op`, although its type is \texttt{ncclProxyArgs} and \emph{not} \texttt{ncclProxyOp}.

\begin{lstlisting}[language=C]
typedef ncclResult_t (*proxyProgressFunc_t)(struct ncclProxyState*, struct ncclProxyArgs*);

struct ncclProxyArgs {
  proxyProgressFunc_t progress;
  struct ncclProxyArgs* next;
  /* other fields */
}
\end{lstlisting}

This allows calls to different the implementations of the \texttt{progress} function 
for different transport methods\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/transport/net.cc}}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/transport/coll_net.cc}}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/transport/p2p.cc}}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/transport/shm.cc}}.
Each implementations calls the profiler API to inform about a different event type (ProxyStep, KernelCh or Network plugin specific). 

\subsubsection{recordEventState}

\begin{lstlisting}[language=C]
  ncclResult_t recordEventState(
    void* eHandle,
    ncclProfilerEventState_v5_t eState,
    ncclProfilerEventStateArgs_v5_t* eStateArgs
  );
\end{lstlisting}
Some event types can be updated by NCCL through \texttt{recordEventState} (state and attributes)\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/include/plugin/profiler/profiler_v5.h}}.
\texttt{recordEventState} is called in the same functions that call \texttt{startEvent} and are happening after \texttt{startEvent}.

\subsubsection{finalize}

\begin{lstlisting}[language=C]
  ncclResult_t finalize(void* context);
\end{lstlisting}
After a user API call to free resources associated with a communicator, \texttt{finalize} is called.
Afterwards, a reference counter tracks how many communicators are still being tracked by the profiler plugin. 
If it reaches 0, the plugin will be closed via \texttt{dlclose(handle)}. Fig.~\ref{fig:profiler-finalize} depicts the flow from user API call to \texttt{finalize}.

\iffalse
\begin{lstlisting}[style=ascii]
-------------------------------------------------------------------------------
User API                          Internal Flow
-------------------------------------------------------------------------------
ncclCommAbort()    --+
ncclCommDestroy()  --+----------> commReclaim()
                                  |
                                  +-> ncclProfilerPluginFinalize()
                                      |
                                      +-> ncclProfiler->finalize()
                                      +-> ncclProfilerPluginUnload()
\end{lstlisting}
\fi

\iffalse
\paragraph{What the diagram conveys.}
User API calls \texttt{ncclCommAbort()} or \texttt{ncclCommDestroy()} trigger \texttt{commReclaim()}, which calls \texttt{ncclProfilerPluginFinalize()} $\to$ \texttt{profiler->finalize()}, then \texttt{ncclProfilerPluginUnload()} (e.g.\ \texttt{dlclose}).

\paragraph{Representation as a sequence diagram.}
We show three participants: \emph{User}, \emph{NCCL}, and \emph{Profiler plugin}. The User calls NCCL to destroy/abort the communicator; NCCL runs \texttt{commReclaim()} and then calls the plugin's \texttt{finalize()} and unloads the plugin. The sequence diagram makes the teardown order explicit.
\fi

\begin{figure}[H]
\centering
\resizebox{!}{10cm}{%
\begin{sequencediagram}
\newthread{user}{User}
\newinst[4]{nccl}{NCCL}
\newinst[5]{plug}{Profiler plugin}
\begin{call}{user}{ncclCommAbort / ncclCommDestroy}{nccl}{}
  \begin{callself}{nccl}{commReclaim}{}
  \end{callself}
  \begin{callself}{nccl}{ncclProfilerPluginFinalize}{}
    \begin{call}{nccl}{finalize(context)}{plug}{}
    \end{call}
  \end{callself}
  \begin{callself}{nccl}{ncclProfilerPluginUnload}{}

    \begin{sdblock}{\texttt{if 0 == (---profilerPluginRefCount)}}{}
      \begin{callself}{nccl}{\texttt{dlclose}}{}
      \end{callself}
    \end{sdblock}
    
  \end{callself}
\end{call}
\end{sequencediagram}%
}
\caption{User API $\to$ \texttt{commReclaim} $\to$ \texttt{finalize} $\to$ plugin unload.}
\label{fig:profiler-finalize}
\end{figure}

\subsubsection{name}

The profiler plugin struct also has a \texttt{name} field.
The name field should point to a character string with the name of the profiler plugin. 
It will be used for all logging, especially when \texttt{NCCL\_DEBUG=INFO} is set.

\section{Code examples and visualizations}

The following examples illustrate the profiling behavior for different user application settings:

\begin{itemize}
  \item One Device per Thread
  \item Multiple Devices per Thread via \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}
  \item One Device per Thread and aggregated operations via \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}
  % \item Multiple Devices per Thread and aggregated operations utilizting \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}
\end{itemize}

A profiler plugin that logs all call information to a file has been developed and is used in all examples.
An exemplary illustration is shown below:
\begin{lstlisting}[language=C]
struct MyContext { /* custom context struct */ };
struct MyEvent { /* custom event struct */ };

MyEvent* allocEvent(args) { /* handles event allocation */ }
uint64_t getTime() { /* gets time */ }
void writeJsonl() { /* writes call details to process specific log file as structured jsonl */ }

ncclResult_t myInit( /* args - **context, *eActivationMask, ... */ ) {
  *context = malloc(sizeof(struct MyContext));
  *eActivationMask = 4095; /* enable ALL event types */
  
  writeJsonl(getTime(), "Init", args);
  return ncclSuccess;
}

ncclResult_t myStartEvent( /* args - **eHandle, ... */ ) {
  *eHandle = allocEvent(args);

  writeJsonl(getTime(), "StartEvent", args);
  return ncclSuccess;
}

ncclResult_t myStopEvent(void* eHandle) {
  writeJsonl(getTime(), "StopEvent", eHandle);

  free(eHandle)
  return ncclSuccess;
}

ncclResult_t myRecordEventState( /* args - ... */ ) {
  writeJsonl(getTime(), "RecordEventState", args);
  return ncclSuccess;
}

ncclResult_t myFinalize(void* context) {
  writeJsonl(getTime(), "Finalize", args);

  free(context);
  return ncclSuccess;
}

ncclProfiler_v5_t ncclProfiler_v5 = {
  "MyProfilerPlugin",
  myInit,
  myStartEvent,
  myStopEvent,
  myRecordEventState,
  myFinalize,
};
\end{lstlisting}

Alongside the logging profiler plugin, a visualization tool as been built, 
that ingests the profiler logs to inspect the exact behavior of internal calls from NCCL to the Profiler API.
It displays the events as colored bars on a timeline and separates them on different lanes.
Each lane also displays some information about the communicator, rank and thread corresponding
to the event.
Additionally, blue dotted lines indicate the relationship between events according to the \texttt{parentObj} field
and red lines indicate which collective events belong to the same collective operation.

Further, a hover feature was added to inspect all details of an event, however this feature is not used in the following illustrative examples.

\subsection{One Device per Thread}

This example visualizes an AllReduce collective across multiple GPUs (see Fig.~\ref{fig:multi-gpu-single-thread-allreduce} and Fig.~\ref{fig:multi-gpu-single-thread-show-proxystep}).
Each NCCL thread manages a single GPU. 
This may be achieved by starting out with the same number of MPI tasks with each task running single threaded;
or by having less MPI tasks, but the tasks create multiple thread workers.
Custom initialization without MPI is also possible if desired.

\begin{lstlisting}[language=C]
// broadcast a commId 

// ...

ncclCommInitRank(&rootComm, nRanks, commId, myRank);

// ...

ncclAllReduce(sendBuff, recvBuff, BUFFER_SIZE, ncclFloat, ncclSum, rootComm, streams);

// ...

ncclCommDestroy(rootComm);
\end{lstlisting}

The profiler API calls are visualized in Fig.~\ref{fig:multi-gpu-single-thread-allreduce} and Fig.~\ref{fig:multi-gpu-single-thread-show-proxystep}.
Below follows a full description of the calls to the profiler API induced by the example program:

First, the profiler API \texttt{init} is called for each rank. 
This occurs during NCCL's internal communicator creation, 
when the application calls \texttt{ncclCommInitRank}. 
After the application calls \texttt{ncclAllReduce},
many Profiler API calls to \texttt{stateEvent}, \texttt{stopEvent}, and \texttt{recordEventState} are triggered:
Intially, startEvent for the \texttt{groupApi} (green bar) is called.
Below it, the startEvent and soon the stopEvent for the AllReduce \texttt{collApi} event are called. 
The yellow bar shows when NCCL enqueues the GPU kernel launch (\texttt{KernelLaunch} event). 
The two bars below represent the \texttt{group} and \texttt{coll} events. 
NCCL also spawns a proxy progress thread per rank, which does additional profiler API calls. 
The first red \texttt{ProxyCtrl} event shows the proxy progress thread was asleep. 
Next, a new \texttt{ProxyCtrl} event shows time for the proxy thread to append proxy ops. 
Then, appended ops start progressing (\texttt{ProxyOps} events), 
which in \texttt{op->progress()} starts \texttt{ProxyStep} and \texttt{KernelCh} events 
that inform about low level network activity in updates via \texttt{recordEventState} like \texttt{ProxyStepRecvGPUWait} (see Fig.~\ref{fig:multi-gpu-single-thread-show-proxystep}).
Network activity eventually completes and the AllReduce collective finishes. 
The next \texttt{ProxyCtrl} event only shows the proxy thread sleeping again. 
Finally, profiler \texttt{finalize} is called, which happens when the application cleans up NCCL communicators and no further communicators are tracked in the profiler in each respective thread.

\texttt{ProxyStep} events are emitted in cross node communication environments. 
If this type of communication is not required, then \texttt{ProxyStep} events will not happen either.

\begin{figure}[H]
  \centering
\includegraphics[width=\textwidth]{"images/multi_gpu_single_thread/finalize alt (2).png"}
    \caption{One device per thread: A visualization of the calls generated to the Profiler API, starting from communicator creation, 
    followed by a collective operation and communicator destruction. 
    \texttt{ProxyStep} events have been omitted for visual clarity, see Fig.~\ref{fig:multi-gpu-single-thread-show-proxystep} for a depiction.}
  \label{fig:multi-gpu-single-thread-allreduce}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/multi_gpu_single_thread/show proxystep alt (2).png"}
  \caption{One device per thread: In Fig.~\ref{fig:multi-gpu-single-thread-allreduce} \texttt{ProxyStep} events have been omitted for visual clarity. 
  However, in multinode settings, many additional profiler API calls for proxyStep events happen, 
  informing about the low level network steps in their event details via \texttt{recordEventState} (indicated as red circles above each of the event bars).
  The blue dotted lines indicate the \texttt{parentObj} of each proxyStep event, 
  which are the above proxyOp events.}
  \label{fig:multi-gpu-single-thread-show-proxystep}
\end{figure}

\subsubsection{Multiple Devices per Thread (ncclGroup)}\label{sec:multiple-devices-per-thread-ncclgroup}

In this example\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/examples/03_collectives/01_allreduce/}}, one NCCL thread manages all GPUs on the same node. 
This is achieved by wrapping communication initialization 
in \texttt{ncclGroupStart} and \texttt{ncclGroupEnd} for each managed GPU.
In this orchestration setting, \textbf{NVIDIA's documentation states that collective API calls should also be wrapped in ncclGroup.}
Here, only one collective operation (per device) is inside the ncclGroup:

\begin{lstlisting}[language=C]
// broadcast a commId 

// ...

ncclGroupStart();
for (int i=0; i<ngpus; i++) {
  cudaSetDevice(dev);
  ncclCommInitRank(comms+i, ngpus*nRanks, id, myRank*ngpus+i);
}
ncclGroupEnd();

// alternatively to above method, NCCL provides the convenience function 
// ncclCommInitAll();

// ...

ncclGroupStart();
for (int i = 0; i < num_gpus; i++) {
  ncclAllReduce( /* ... */ );
}
ncclGroupEnd();

// ...

for (int i = 0; i < num_gpus; i++) {
  ncclCommDestroy(comms[i]);
}
\end{lstlisting}

In this example case, the profiler API behavior remains largely the same:
The one difference is that NCCL internally calls the profiler API groupApi event 
only one time in total for aggregated operations within a thread.
Otherwise all other events are processed as usual and are called their usual amount of times irrespective of \texttt{ncclGroup}.
This is visualized in Fig.~\ref{fig:single-gpu-single-process-ncclgroup-finalize}.
This behaviour also holds true within a process. 
It also holds when grouping (single) collectives for different communicators.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclgroup 03_communicators_01_allreduce/init (2).png"}
    \caption{Multiple devices per thread: Events from the proxy thread as well as init and finalize calls are omitted. 
    Collective API calls from multiple GPUs managed by a single thread only trigger a single \texttt{GroupApi} event.}
  \label{fig:single-gpu-single-process-ncclgroup-finalize}
\end{figure}

\subsubsection{Aggregated operations}
In this example, the setting is such that only a single GPU is managed by a thread,
but multiple collective operations are grouped (i.e. to optimize communication efficiency):

\begin{lstlisting}[language=C]
// broadcast a commId

// ...

ncclCommInitRank(&rootComm, nRanks, rootId, myRank);

// ...

ncclGroupStart();
ncclAllReduce( /* ... */ );
ncclBroadcast( /* ... */ );
ncclReduce( /* ... */ );
ncclAllGather( /* ... */ );
ncclReduceScatter( /* ... */ );
ncclGroupEnd();

// ...
\end{lstlisting}

The behavior changes can be described as follow:
\begin{itemize}
  \item single GroupApi event per thread
  \item single KernelLaunch event per thread
  \item single Group event per thread
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclgroup stress test/ncclgroup commA 1 collX + commA 1 collY/phase 2.5, 1st iter (3).png"}
  \caption{one GPU per thread with aggregated operations: multiple collective calls are grouped together and nccl does only a single kernel launch per thread.}
  \label{fig:one-gpu-per-thread-grouped-collectives}
\end{figure}

\iffalse
\subsubsection{Multiple Devices per Thread and aggregated operations utilizting \texttt{ncclGroupStart} and \texttt{ncclGroupEnd}}

In this setting, ncclGroup is used to manage multiple GPU by one thread
\textbf{and} multiple collective operations are within a group.
This is almost the same example as in the case for
\hyperref[sec:multiple-devices-per-thread-ncclgroup]{Multiple Devices per Thread (ncclGroup)},
but with additional collectives inside the ncclGroup:

\begin{lstlisting}[language=C]
  
  // ...
  
  ncclGroupStart();
  for (int i = 0; i < num_gpus; i++) {
    ncclAllReduce( /* ... */ );
    ncclAllReduce( /* ... */ );
    ncclBroadcast( /* ... */ );
  }
  ncclGroupEnd();
  
  // ...
  
\end{lstlisting}

\begin{itemize}
  \item single groupApi event
  \item single KernelLaunch event
  \item single group event
\end{itemize}

TODO run 03\_example\_01\_allreduce++ 
(same, except its group(allreduce allreduce broadcast))
slurm 2798216

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{"images/ncclgroup 03_communicators_01_allreduce++/init, 2 api calls alt.png"}
  \caption{Multiple GPUs per thread: When collectives are grouped, only a single GroupApi event for all ranks managed by the thread is emitted. For each rank a single Kernel for the grouped collective operations is launched. All individual \texttt{CollectiveApi} and \texttt{Collective} events are emitted. This also holds true within a process.}
  \label{fig:multi-gpu-per-thread-grouped-collectives}
\end{figure}


TODO this behaviour holds in thread/process/comm combinations how?
\fi

\section{Performance and scalability of the Profiler Plugin API}
 
Experiments were run to assess the performance and scalability of profiler plugins.
These experiments measure the overhead induced internally by NCCL to serve the profiler plugin, 
but do not intend to measure the performance of a profiler plugin itself 
as the plugin is fully customizable to the needs of the developer.

Thus, the profiler developed for the experiments only initializes a dummy context struct, 
returns NULL for event handles 
and tracks all events (\texttt{eActivationMask} set to 4095).
\begin{lstlisting}[language=C]
// an 'empty' NCCL Profiler Plugin

struct MyContext {
  char dummy;
};

ncclResult_t myInit(void** context, uint64_t commId, int* eActivationMask, const char* commName, int nNodes, int nranks, int rank, ncclDebugLogger_t logfn) {
  *context = malloc(sizeof(struct MyContext));
  *eActivationMask = 4095; /* enable ALL event types */
  return ncclSuccess;
}

ncclResult_t myStartEvent(void* context, void** eHandle, ncclProfilerEventDescr_v5_t* eDescr) {
  *eHandle = NULL;
  return ncclSuccess;
}

ncclResult_t myStopEvent(void* eHandle) {
  return ncclSuccess;
}

ncclResult_t myRecordEventState(void* eHandle, ncclProfilerEventState_v5_t eState, ncclProfilerEventStateArgs_v5_t* eStateArgs) {
  return ncclSuccess;
}

ncclResult_t myFinalize(void* context) {
  free(context);
  return ncclSuccess;
}

ncclProfiler_v5_t ncclProfiler_v5 = {
  "EmptyProfiler",
  myInit,
  myStartEvent,
  myStopEvent,
  myRecordEventState,
  myFinalize,
};
\end{lstlisting}

For testing the performance overhead in collective and P2P operations, \textbf{nccl-tests} from NVIDIA was used\footnote{\url{https://github.com/NVIDIA/nccl-tests}}.

The applications \texttt{sendrecv\_perf} and \texttt{all\_reduce\_perf} were launched with follwing test parameters:
message size 64\,B, 1\,000\,000 iterations per size, 100 warmup iterations. 
Single-node jobs used one node and 4 GPUs; 
multi-node jobs used 2 nodes, 4 GPUs per node, 8 MPI ranks in total. 
For each experiment, the application was run once without the profiler and once with the empty profiler plugin.

The Table~\ref{tab:profiler-overhead} shows the average latency per operation (time in $\mu$s) across iterations.
The empty profiler adds roughly 8 to 9\,$\mu$s overhead per operation in single-node runs (4 GPUs), but introduces negligible overhead in multi-node runs (8 GPUs across 2 nodes).

\begin{table}[H]
\centering
\caption{Profiler overhead: nccl-tests \texttt{sendrecv\_perf} (P2P) and \texttt{all\_reduce\_perf} (collectives). Latency averaged over 1M iterations.}
\label{tab:profiler-overhead}
\begin{tabular}{llrr}
\hline
Test & Environment & Without profiler ($\mu$s) & With profiler ($\mu$s) \\
\hline
P2P (\texttt{sendrecv\_perf}) & Single-node (4 GPUs) & 14.3 & 23.88 \\
 & Multi-node (2$\times$4 GPUs) & 13.05 & 12.95 \\
\hline
Collectives (\texttt{all\_reduce\_perf}) & Single-node (4 GPUs) & 14.96 & 23.29 \\
 & Multi-node (2$\times$4 GPUs) & 17.99 & 18.34 \\
\hline
\end{tabular}
\end{table}

\iffalse
multinode
\begin{itemize}
  \item commops multinode (100 iters, 10 warmup iters):
  \begin{itemize}
    \item sbatch run 1: "init, destroy" experiment
    \begin{itemize}
      \item 1790.05 s overhead. averaged 577455.37 s per iter w/o. 579245.42 s w/ profiler
    \end{itemize}
    \item sbatch run 1: "init once, split, destroy" experiment
    \begin{itemize}
      \item 1546.16 s overhead. averaged 1079679.93 s per iter w/o. 1078133.77 s w/ profiler
    \end{itemize}
  \end{itemize}
\end{itemize}

TODO describe sbatch run flags for nccl-tests

There are two experiments for communication operations. 
First, the time of \texttt{ncclCommInit} and \texttt{ncclCommDestroy} are tested.
Second, initialization happens once and \texttt{ncclCommSplit} and \texttt{ncclCommDestroy} are tested:

synthetic - TODO add application code in an appendix section or something


\begin{lstlisting}[language=C]
uint64_t start_time = get_time_ns();
for (int iter = 0; iter < num_iterations; iter++) {
  ncclUniqueId id;
  if (mpi_rank == 0) { ncclGetUniqueId(&id); }
  MPI_Bcast((void *)&id, sizeof(id), MPI_BYTE, 0, MPI_COMM_WORLD);

  ncclComm_t comm;
  ncclCommInitRank(&comm, mpi_size, id, mpi_rank);
  ncclCommDestroy(comm);
}
uint64_t end_time = get_time_ns();
\end{lstlisting}

\begin{lstlisting}[language=C]
  ncclUniqueId id;
  if (mpi_rank == 0) { ncclGetUniqueId(&id); }
  MPI_Bcast((void *)&id, sizeof(id), MPI_BYTE, 0, MPI_COMM_WORLD);
  
  ncclComm_t parent_comm;
  ncclCommInitRank(&parent_comm, mpi_size, id, mpi_rank);

  uint64_t start_time = get_time_ns();
  for (int iter = 0; iter < num_iterations; iter++) {
    ncclComm_t comm;

    int color = mpi_rank % 2;
    int key = mpi_rank;

    ncclCommSplit(parent_comm, color, key, &child_comm, NULL)
    ncclCommDestroy(comm);
  }
  uint64_t end_time = get_time_ns();
\end{lstlisting}
\fi

Using the profiler plugin when scaled to many gpus across multiple nodes is effortless 
and did not require any changes in the profiler plugin for the used code examples and experiments.

% ---------------------------------------------------------------------------
\section{Discussion}
% ---------------------------------------------------------------------------

This section first discusses practical considerations for developers who implement or extend an NCCL profiler plugin, 
as well as known limitations of the current profiling infrastructure, 
and then shows how the plugin could be integrated with the Score-P measurement infrastructure for HPC-wide tracing and analysis.

\subsection{Considerations for developers of a Profiler Plugin}

\paragraph{Profiler Visualization.}
The visualization tool used in the code examples is helpful for understanding the internal call behavior to the Profiler API by NCCL
and will be made available along with this report.
It may serve as a reference to compare against for other developers that build a profiler plugin or visualizer

\paragraph{Correlating Collective Events with \texttt{seqNumber}.}
When profiling is enabled, NCCL counts the number of calls for each type of collective function per communicator.

/src/include/comm.h
\begin{lstlisting}[language=C]
struct ncclComm {
  uint64_t seqNumber[NCCL_NUM_FUNCTIONS];
  /* other fields */
}
\end{lstlisting}

/src/plugin/profiler.cc
\begin{lstlisting}[language=C]
ncclResult_t ncclProfilerStartTaskEvents(struct ncclKernelPlan* plan) {
  /* other code */
  __atomic_fetch_add(&plan->comm->seqNumber[ct->func], 1, __ATOMIC_RELAXED);
  /* other code */
}
\end{lstlisting}

This value is present in the \texttt{eDescr} for collective events 
and can be used to identify which collectives operations belong together across processes (see Fig.~\ref{fig:phase-4-3rd-iter}).

\paragraph{Tracing low level activity back to NCCL API calls with \texttt{parentObj}.}
If a plugin developer wants utilize this field, 
they should ensure that potential address reuse does not create ambiguity to what the parentObj was originally pointing to. 
\emph{Custom memory management is advised}.
This field is useful when trying to understand which user API call triggered which events of lower level operations or activity such as network activity (see Fig.~\ref{fig:phase-4-3rd-iter}).

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclgroup stress test/ncclgroup commA 1 collX + commB 1 collY/phase 4, 3rd iter.png"}
  \caption{An example illustrating how \texttt{parentObj} and \texttt{seqNumber} can be used to better understand the timing of concurrent collective operations.}
  \label{fig:phase-4-3rd-iter}
\end{figure}

\paragraph{Process origin for profiler callbacks with PXN enabled.}
Unless Setting the environment variable \texttt{NCCL\_PXN\_DISABLE}=0 (default 1), due to PXN (PCIe x NVLink) some proxy ops may be progressed in a proxy thread from another process, different to the one that originally generated the operation. 
Then \texttt{parentObj} in \texttt{eDescr} is not safe to dereference; the \texttt{eDescr} for \texttt{ProxyOp} events includes the originator's PID, which the profiler can match against the local PID. 
The \texttt{eDescr} for \texttt{ProxyStep} does not provide this field. However a workaround is possible:

The passed \texttt{context} object in \texttt{startEvent} is also unsafe to dereference due to PXN. 
the profiler plugin developer may internally track initialized contexts 
and whether the passed \texttt{context} belongs to the local process. 
This is also indicative of PXN.

\paragraph{Tracking communicator parent--child relationships.}
With the current Profiler plugin API, it is not possible to detect whether a communicator originates from another one 
(e.g., via \\\texttt{ncclCommSplit} or \texttt{ncclCommShrink}).
The plugin's \texttt{init} callback only receives a single communicator ID (\texttt{commId},
which corresponds to \texttt{comm->commHash}), as well as \texttt{commName}, \texttt{nNodes}, \texttt{nRanks}, and \texttt{rank};
there is no \texttt{parentCommId} or similar argument.
In split/shrink, the \texttt{commHash} of the child node is calculated internally as a one-way digest of the \texttt{commHash} of the parent node and the split parameters (\texttt{splitCount}, \texttt{color}).
Therefore, the relationship cannot be restored based on the ID alone.

\subsection{Known limitations}

Kernel event instrumentation uses counters exposed by the kernel to the host and the proxy progress thread. 
Thus the proxy progress thread infrastructure is shared between network and profiler. 
If the proxy is serving network requests, reading kernel profiling data can be delayed, causing loss of accuracy. 
Similarly, under heavy CPU load and delayed scheduling of the proxy progress thread, accuracy can be lost.

From profiler version 4, NCCL uses a per-channel ring buffer of 64 elements. 
Each counter is complemented by two timestamps (ptimers) supplied by the NCCL kernel (start and stop of the operation in the kernel). 
NCCL propagates these timestamps to the profiler plugin so it can convert them to the CPU time domain.

(Source: \textbf{/ext-profiler/README.md})

\subsection{Potential Integration with Score-P}

The Score-P measurement infrastructure\footnote{\url{https://www.vi-hps.org/projects/score-p/overview}} is a highly scalable and easy-to-use tool suite 
for profiling and event tracing of HPC applications.
It supports a number of analysis tools. 
Currently, it works with Scalasca, Vampir, and Tau and is open for other tools 
and produces OTF2 traces and CUBE4 profiles.
Integrating NCCL into Score-P allows developers 
to see communication collectives alongside the application logic. 

A prerequisite for distributed tracing is the unique identification of process groups. 
NCCL achieves this via \texttt{ncclGetUniqueId}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/src/init.cc}} without a central coordinator. 
To establish a communicator, one process generates a handle containing
a random 64-bit \texttt{magic} value from \texttt{/dev/urandom} 
and the socket \texttt{address} of a new listening socket (IP, port), 
whose port is chosen by the operating system. 
This combination avoids collisions across a cluster, 
ensuring the ID is globally unique in practice.
A Score-P integration can use these to accurately define Process Groups. 

The integration could be achieved in two ways, 
either using a direct Profiler API mapping 
or via an indirect NVTX/CUPTI annotation:

A direct integration would potentially involve implementing a NCCL profiler plugin 
that translates the \texttt{startEvent} and \texttt{stopEvent} callbacks into Score-P regions:
The plugin maps NCCL event descriptors (e.g., ncclAllReduce) to Score-P regions using the instrumentation macros
(e.g., \\\texttt{SCOREP\_USER\_REGION\_BY\_NAME\_BEGIN}/\texttt{END}).

Alternatively, the NCCL profiler plugin can act as a bridge to NVIDIA's Tools Extension (NVTX). 
If Score-P has been built with CUDA support it can intercept NVTX ranges.
The NCCL profiler plugin would emit \texttt{nvtxRangePush}\footnote{\url{https://nvidia.github.io/NVTX/doxygen/group___m_a_r_k_e_r_s___a_n_d___r_a_n_g_e_s.html}} and \texttt{nvtxRangePop} around NCCL operations. 
Score-P records these as labeled regions without requiring the plugin to link directly against Score-P libraries.
This approach decouples the NCCL plugin from the Score-P build environment
and instead relies on Score-P's internal NVTX-to-OTF2 mapping logic.

The plugin can utilize \texttt{cuptiActivityPush/PopExternalCorrelationId} to capture GPU activity 
during the \texttt{startEvent} and \texttt{stopEvent} of \texttt{KernelLaunch} events, 
while incrementing a thread-safe correlation ID (see Fig.~\ref{fig:cupti-example}).
CUPTI can be initialized and cleaned up within the profiler plugin's \texttt{init} and \texttt{finalize} functions.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{"images/ncclgroup stress test/cupti example.png"}
  \caption{CUPTI activity is visualized as orange event bars. With a unique correlation Id, it is possible to trace the activity back to \texttt{KernelLaunch} events}
  \label{fig:cupti-example}
\end{figure}

% ---------------------------------------------------------------------------
\section{Conclusion}
% ---------------------------------------------------------------------------

This study examined the NCCL Profiler Plugin API 
and its suitability for integration with Score-P.
It provided background on NCCL and its design, 
explained how the profiler plugin is loaded 
and described the API definition with its five core callbacks 
\texttt{init}, \texttt{startEvent}, \texttt{stopEvent}, \\\texttt{recordEventState} and \texttt{finalize}.
Code examples and visualizations illustrate the event flow from API calls to NCCL's internal profiler callbacks.
Performance experiments showed that an empty profiler adds roughly 8--9\,$\mu$s overhead per operation in single-node runs 
but introduces negligible overhead in multi-node runs, and scaling to many GPUs across nodes required no changes to the profiler plugin. 
The discussion covered developer considerations, known limitations, and a potential integration strategy with Score-P.

The NCCL Profiler API allows for highly customized plugins tailored to the analysis needs, 
whether for simple timing, kernel tracing via CUPTI, or integration with external tools such as Score-P. 
A notable advantage is its low overhead: 
NVIDIA advertises their \texttt{inspector}\footnote{\url{https://github.com/NVIDIA/nccl/tree/master/ext-profiler/inspector}} implementation as efficient enough for ``always-on'' profiling in production. 
On the downside, profiler plugins may require maintenance and active development, since NCCL is actively developed. 
API versions evolve and new features are being introduced.

\iffalse
\subsection{NCCL\_DEBUG}

\url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-debug}

NCCL already comes with debug logging at various levels of granularity:
\begin{itemize}
  \item INFO -- debug information
  \item TRACE -- replayable trace information on every call
  \item Further options (v2.2.12 \texttt{NCCL\_DEBUG\_FILE}, v2.3.4 \texttt{NCCL\_DEBUG\_SUBSYS}, v2.26 timestamp format/levels)
  \item other profiling and tracing tools exists that are maintained by NVIDIA: nsight systems, nsight compute
\end{itemize}
\fi

\end{document}
